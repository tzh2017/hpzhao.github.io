---
title: 【译】神经网络与深度学习
date: 2016-07-10 15:34:36
tags: [Deep Learning,Translation]
categories: Deep Learning
descriptions: 

---

[*NEURAL NETWORKS AND DEEP LEARNING*](http://neuralnetworksanddeeplearning.com)是*Michael Nielsen*在2015年初推出的书。他本身是个量子物理学家，因此数学功底深厚，能把数学道理讲的深入浅出。而这本书就把神经网络背后的数学讲的浅显易懂，非常适合初学者。仅供参考，不得作商业用途，水平有限，如有错误，请指正。

# 用神经网络识别手写数字
## 前言
人类的视觉系统是是大自然的奇迹。考虑下面手写数字序列：  

<img src="/images/c1s0-1.png" width="25%" height="20%"/>

大多数人能够轻易地是识别出是504192。在我们大脑的每个半球都有一个基础的皮质，这就是我们熟知的V1区，它包含了14亿个神经元，并有着数十亿级的神经元连接。然而人类地视觉不仅包含V1区，同时还有一系列地视觉皮质——V2，V3，V4，V5——它们进行着复杂地图像处理。我们头脑携带着一个超级计算机，经过数百万年的进化，能够很好地适应视觉的世界。识别手写数字并不容易。但是，我们人类惊人地擅长将我们所见之物转化成脑中有意义地东西。当然这些工作都是在无意识地情况下完成的。因此，通常我们不会感受到我们地视觉系统在处理着怎样的难题。  

当你尝试写程序去解决上述手写数字识别时，你就会发现这个问题有多难。我们大脑轻易在瞬间完成地事情变得相当困难了。一个简单地例子能描述我们识别形状为何变得不容易用算法表达——“一个9上面时一个环形，下面是一个钩”。当你想表达更加精细时，你会很快陷入特例的沼泽。这个问题看起来相当令人绝望。  

神经网络用另一种方式来解决这个问题。思想就是利用大量的手写数字，即我们熟知的训练数据，然后能通过学习这些训练数据来发展一个系统。  

<img src="/images/c1s0-2.png" width="45%" height="40%"/> 

换句话说，神经网络能够利用这些例子自动地学习识别手写数字地内在规则。另外，如果增大训练样本，那么神经网络能够学到更多手写数字的信息，因此能够提高它的准确率。尽管我们上面地图只有100个训练样本，但是在实际中可能会用到数以亿计的训练样本。  

在这章我将写一个程序来实现一个神经网络能够学习识别这些手写数字。这段代码只有仅仅74行，没有用到任何神经网络的库。但是这段代码在没有人工的干预下达到超过96%的准确率。另外，在稍后地章节我们会用其他的一些想法来把我们的准确率提高到99%以上。实际上，最好的商用神经网络能够很好地用于银行识别支票，邮局识别地址等等。  

我们关注手写数字识别是因为它是个很好的一个原型能够帮助我们学习神经网络地通用原理。作为一个模型，它有两点好处：首先它是个有挑战性的项目——识别手写数字是个不小地壮举——但是它也不是需要一个及其复杂地模型或者强大的计算能力。另外，它也能帮助我们学习更高级的技术，例如深度学习，打下良好的基础。因此，在这本书我一直会贯穿手写数字识别问题。在这本书的稍后章节，我将会讨论如何将这些思想应用到计算机视觉，语音识别，自然语言处理等领域。  

当然，如果这章地内容仅仅关于如何写一段程序识别手写数字，那么篇幅会大大缩短！但我们将会学习神经网络地一些关键思想，包括两个重要的人工神经元（分类器和sigmoid神经元)，神经网络地标准学习算法即我们熟知的梯度下降法(stochastic gradient descent)。我将会关注为何它们能起到这样的作用，用来建立你对神经网络地一些直觉。我不仅会给出基本数学原理，还会花费更长的篇幅来讨论，但我觉得如果你能收获更深层次的理解，这也是值得的。我们将会在本章的最后理解什么是深度学习以及为什么它们如此重要。

## 感知机
什么是神经网络？作为开始，我将会解释一种被称为感知器（perceptron）的人工神经元。感知器是20世纪50年代和60年代期间由科学家[Frank Rosenblatt](http://en.wikipedia.org/wiki/Frank_Rosenblatt)提出，他受到[Warren McCulloch](http://en.wikipedia.org/wiki/Warren_McCulloch)和[Walter Pitts](http://en.wikipedia.org/wiki/Walter_Pitts)早期工作的影响。当今，人们通常使用其他的人工神经元——在这本书和大量的现在神经网络的工作都使用了一种称之为**sigmoid**神经元。但是为了理解为什么sigmoid神经元那么定义，我们需要首先花些时间理解感知器。  

那么感知器是如何工作的呢？一个感知器有若干个二进制输入，$x_1, x_2, \ldots$并且产生一个二进制输出：  

<img src="/images/c1s1-1.png">  

在这个例子中展示的感知器有三个输入，$x_1, x_2, x_3$。通常来讲它可能会有更多或更少的输入。Rosenblatt提出一种简单的规则来计算输出。他引入权重(*weight*)，$w_1,w_2,\ldots$，这些数字是用来表达各个输入对输出结果的重要程度。神经元的输出$0$或$1$是由加权和$\sum_j w_j x_j$是否大于或小于某个阈值(*threshold value*)来确定的。就像权重一样，阈值也是作为神经元参数的一部分。下面的公式精确地表达了这种思想：  

{% raw %}
\begin{eqnarray}
\mbox{output} & = & \left\{ \begin{array}{ll}
0 & \mbox{if } \sum_j w_j x_j \leq \mbox{ threshold} \\
1 & \mbox{if } \sum_j w_j x_j > \mbox{ threshold}
\end{array} \right.
\tag{1}\end{eqnarray}
{% endraw %}
这就是一个感知器的工作方式！  

这是一个基础的数学模型。你可以把感知器看作通过权重实现的决策器。我将给出一个例子。这不是一个真实的例子，但它易于理解，稍后我们将接触更真实的例子。假设周末来了，你听说城市里将会举办起司节。你喜欢起司，所以你尽力去决定要不要参加这个节日。你的决定可能由衡量以下三个因素确定：  

1. 天气好么？  
2. 有朋友陪你么？
3. 节日旁边是否有公交站？（假设你没有车）。   

我们用$x_1, x_2$和$x_3$来表示这三个因素。举个例子来说，$x_1 = 1$代表天气不错，$x_1 = 0$代表天气很糟糕。同理，$x_2 = 1$代表你有朋友陪，而$x_2 = 0$则代表没人陪。$x_3$也类似。  

现在假设你真心的喜欢起司，以至于即使没有朋友陪伴或者很难去起司节你也会乐意前往。但是你及其厌恶糟糕的天气，如果天气不好你就坚决不去起司节。你能用感知器来作为决策器的模型。那么做的一种方式就是选择权重$w_1=6$作为天气的权重，$w_2=2$和$w_3=2$作为其他的条件。$w_1$的权重很大暗示着你很在在意天气，这可能比朋友陪伴和交通状况更重要。最后，你选择了5作为感知器的阈值。选择之后感知器能够生成这样的决策器模型：当天气是好的时候输出为$1$，当天气差的时候输出为$0$。  

通过改变权重和阈值，我们能够获得不同的决策器模型。举个例子来说，假设我们设定阈值为$3$。此时，你要去起司节的情况变为：天气很好或者天气糟糕但是交通便利并有朋友陪伴。换句话说，这是一个不同的决策器。降低阈值意味着你更想去起司节。  

很显然，感知机和人类做决策时并不完全一样！但是这个例子很好地解释了感知器如何通过改变权重来影响决策。这应该是合理的，一个复杂的感知机网络能够做更精细的决定：  

<img src="/images/c1s1-2.png" width="60%" height="35%" />  

在这个网络中，第一列感知器——就是我们俗称的第一层感知器——通过改变输入的权重来做三个简单的决策。那么第二层呢？每个感知器也通过改变第一层输出的权重来做决策。 这样感知器的第二层比第一层能够在更复杂更抽象的水平上做出决策。而且，第三层能够做出更加复杂的决策。通过这种方式，一个多层感知器能够参与复杂的决策。  

顺便提一句，当我定义感知器时我说一个感知器仅仅有一个输出。在上述的网络中似乎有多个输出。事实上，它们仍然只是有一个输出。多个输出的箭头仅仅是一种有用的方式来解释一个感知器的输出能够作为其他感知器的输入。它比起绘制一条输出线然后分裂更方便。  

让我们来简化我们描述感知器的方式。条件$\sum_j w_j x_j > \mbox{threshold}$有些麻烦，我们改变两种符号来简化它。第一个改变就是将$\sum_j w_j x_j$写作点积，$w \cdot x \equiv \sum_j w_j x_j$，这里的$w$和$x$分别是权重和输入组成的向量。第二个改变就是将阈值改变成不等式的形式，我们用熟知的感知器的*偏置*(bias)代替阈值，$b \equiv
-\mbox{threshold}$。那么感知器的规则就能重写为以下形式：  

{% raw %}
\begin{eqnarray}
  \mbox{output} = \left\{ 
    \begin{array}{ll} 
      0 & \mbox{if } w\cdot x + b \leq 0 \\
      1 & \mbox{if } w\cdot x + b > 0
    \end{array}
  \right.
\tag{2}\end{eqnarray}
{% endraw %}


你可以把偏置（bias）看作衡量感知器输出$1$的难易程度的度量。或者我们用一个更生物的术语，偏置是感知器激活（fire）难易程度的度量。对于一个有着相当大偏置的感知器，它非常容易输出$1$.但如果偏置是一个很大的负数，那么它很难输出$1$。显然引入偏置只是我们对感知器描述的微小改变，但是稍后我们会看到它将带来更多的符号简化。正因如此，在这本书的剩余部分，我不会提阈值（threshold），我们将一直用偏置（bias）这个概念。  

我已经把感知器作为一种通过改变权重而做决策的方法。感知器的另外一种用途就是能够用于计算比较底层的逻辑计算，例如**AND,OR**和**NAND**。举个例子来说，我们有一个2个输入的感知器，每个权重都是$-2$，偏置为$3$。下图就是我们的感知器： 

<img src="/images/c1s1-3.png" width="40%" height="20%" />  

我们会看到输入$00$将会产生$1$，因为$\(-2\)\*0\+\(-2\)\*0+3 = 3$是正数。这里我们引入符号$*$为乘法符号。类似的，当我们输入$01$和$10$会产生输出$1$。但我们输入$11$则会产生$0$，因为{% raw %}$ (-2)*1+(-2)*1+3 = -1 $ {% endraw %}是负数。因此我们的感知器是一个[与非门](https://en.wikipedia.org/wiki/NAND_gate)!  

从与非门的例子中我们可以看出可以用感知器计算逻辑运算。事实上，我们能用感知器计算所有的逻辑运算。理由就是任何逻辑运算都能用与非门来构建。举个例子，我们用与非门来构建一个两个bits相加的电路：$x_1$和$x_2$。这需要计算按位求和，$x_1 \oplus x_2$，同时也需要尽算进位位。下图是一个示例：  

<img src="/images/c1s1-4.png" width="65%" height="40%" />  

为了得到等价的感知器网络，我们需要用上面的与非门感知器替换上图的与非门。下图是结果。我稍微调整了一下位置，因为这样比较容易画图：  

<img src="/images/c1s1-5.png" width="65%" height="40%" />  

上面这个网络的显著特征就是最左边的感知器的结果被最底面的感知器用作两次输入。当我定义感知器时并没有说这种情况是允许的。事实上，这并不会影响什么。如果我们不想允许这种事发生，那么我们能把这两条线简化成一条，但下面感知器的权重就变为$-4$。（如果你觉得看着不那么显而易见，你可以停下来去证明它的等价性。）改变之后的感知器如下图：  

<img src="/images/c1s1-6.png" width="65%" height="40%" />  

到目前为止，我画了$x_1$和$x_2$作为输入变量。事实上，如果我们把用输入层（*the input layer*）来编码输入那会更加方便画图：  

<img src="/images/c1s1-7.png" width="65%" height="40%" />  

输入感知器的明显特征就是我们有一个输出，但是我们没有输入。  

<img src="/images/c1s1-8.png" width="15%" height="10%" />  

这并不真实意味着这是一个没有输入的感知器。为了理解这个，我们假设的确有一个没有输入的感知器。那么加和$\sum_j w_j x_j$将会是$0$。那就是说这个感知器仅仅会输出一个定值。但我们最好不要把输入层看作感知器，而是把它仅仅看作是一个能输出定值$x_1, x_2,\ldots$的特定模块。  

加法器的例子告诉我们如何用感知器网络模拟包含众多与非门的电路。因为与非门可以作为通用的计算，因此感知器也能作为通用的计算。  

感知器具有通用计算的能力既让我们得到鼓舞也让我们失望。让我们鼓舞的是感知器能够作为一种强大的计算设备。但它也是令人失望的，因为它仅仅是一种新型的与非门。这很难说是一个大新闻！  

尽管如此，真实的形式比这一观点更好。现在已被证明我们能用学习算法自动地学习人工神经网络权重和偏置。这全是由于外部刺激所引起的，并没有编程人员的直接参与。这些学习算法让人工神经元的工作方法和传统的逻辑门截然不同。相比仅仅解决与非门和其它门的电路，我们的神经网络能够通过简单学习来解决问题，有时这些问题可能用传统的电路极难设计。

## Sigmoid神经元

学习算法听起来很美妙。但是我们怎么样为神经网络设计如此的算法呢？假设我们要用感知器网络学习解决一些问题。举例来说，假设输入是手写数字图片转化成的像素点。我们想让这个网络学习权重和偏置来对手写数字进行分类。为了理解学习算法是如何工作的，假设我们对权重（或偏置）做出小的改变。我们所希望的是这个小的改变只对输出相应部分造成影响。我们稍后将会看到，这种性质能够让学习算法成为一种可能。正如下图所示，这正是我们想要的。（显然这种网络太简单，难以胜任手写数字识别的问题！）：  

<img src="/images/c1s2-1.png" width="60%" height="40%" />  

如果权重（或偏置）的变化仅仅对输出造成影响，那么我们能利用这点修改权重和偏置来得到我们想要的神经网络。举个例子，假设网络错误把“9”识别成“8”。我们搞清楚了如何改变权重和偏置以至于让我们的网络能够更接近把图片识别为“9”。如果我们一直如此重复改变权重和偏置，那么我们的网络能够产生越来越好的结果。那么网络就能够学习了。  

问题是这并不是包含感知器神经元网络所发生的事情。事实上，我们对任何一个感知器神经元权重和偏置做出的小变化能够造成结果的完全翻转，比如从$0$变为$1$。这种翻转可能造成网络剩余部分行为的改变。因此当“9”被正确分类了，但对于网络其他图片的分类可能以某种不可控的方式造成分类结果完全改变的情况。那么逐渐调整权重和偏置来逼近我们想要的网络行为这种方法变得非常苦难。可能有某种聪明的方法能解决这种问题。但是我们如何学习感知器神经元网络并不是那么显而易见的。  

我们通过引进一种新的人工神经元——*sigmoid*神经元来客服这个问题。Sigmoid神经元和感知器神经元非常相似，但是对它权重和偏置造成微小改变仅仅对输出造成微小改变。这正是允许神经网络学习的关键点。  

现在我们来描述一下sigmoid神经元。我们将以描绘感知器神经元的方式来描绘sigmoid神经元：  

<img src="/images/c1s2-2.png" width="40%" height="20%"/>  

<span id="（3）">就像感知器一样，sigmoid神经元有输入，$x_1, x_2,\ldots$但是不仅仅是$0$和$1$，这些输入可以是$0$到$1$之间的任何值。举个例子，$0.638\ldots$也是一个有效的输入。和感知器一样，sigmoid神经元也有对应每个输入的权重，$w_1, w_2,\ldots$和偏置$b$。但是输出不是$0$和$1$。具体而言，输出是$\sigma(w \cdot x+b)$,这里$sigma$被称为**Sigmoid函数**(*sigmoid function*),它的定义如下： </span> 

{% raw %}
\begin{eqnarray} 
  \sigma(z) \equiv \frac{1}{1+e^{-z}}.
  \tag{3}\end{eqnarray}
{% endraw %}

更精确的说，一个输入为$x_1,x_2,\ldots$,权重为$w_1,w_2,\ldots$，偏置为$b$的sigmoid神经元的输出为：  

{% raw %}
\begin{eqnarray} 
  \frac{1}{1+\exp(-\sum_j w_j x_j-b)}.\tag{4}\end{eqnarray}
{% endraw %}

在第一印象中，sigmoid神经元看起来和感知器完全不同。如果你对sigmoid函数形式不熟悉的话，它看起来是不透明的。事实上，sigmoid神经元和感知器有很多相似之处，代数形式只是一种技术细节，而不应该成为理解上的障碍。  

为了理解它和感知器模型的相似性，假设$z\equiv w \cdot x + b$是一个很大的正数，sigmoid神经元的输出接近$1$，这就像感知器一般。我们再假设$z\equiv w \cdot x + b$是一个很小的负数。那么$e^{-z} \rightarrow \infty$，因此$\sigma(z) \approx 0$。所以当$z\equiv w \cdot x + b$是一个很小的负数时，它的行为也和感知器类似。仅仅当$w \cdot x+b$是一个适中的数时sigmoid神经元和感知器才有所差别。  

那么如何理解$\sigma$呢？事实上，$\sigma$的确切形式并不是那么重要——真正有用的是它的函数图形形状。正如下图：  
<span id="sigmoid">
<img src="/images/c1s2-3.png" width="50%" height="40%" />  
</span>
它的形状比阶跃函数更加平滑：  

<img src="/images/c1s2-4.png" width="50%" height="40%" />  

如果$\sigma$函数是阶跃函数，那么sigmoid神经元将会变为感知器，因为输出结果$0$或$1$将取决于$w\cdot x+b$是正数还是负数。如果函数是sigmoid的真正形式，那么我们可以把它看作是一种更加平滑的感知器。事实上，sigmoid函数图形的平滑性才是真正的关键点，而不是在于它的具体形式是如何。$\sigma$函数的平滑性意味着如果我们对权重做出小的改变$\Delta w_j$，对偏置做出微小改变$\Delta b$那么输出将会产生改变$\Delta \mbox{output}$。事实上，通过计算我们可以得到输出变化可以近似为如下形式：  
<span id="（5）"></span>
{% raw %}
\begin{eqnarray} 
  \Delta \mbox{output} \approx \sum_j \frac{\partial \, \mbox{output}}{\partial w_j}
  \Delta w_j + \frac{\partial \, \mbox{output}}{\partial b} \Delta b,
\tag{5}\end{eqnarray}
{% endraw %} 

不要被上面复杂的偏导形式吓到，它其实在说一件很简单的事：$\Delta\mbox{output}$是关于$\Delta w_j$和$\Delta b$的线性函数。线性使得我们选择权重和偏置的微小改变来得到期望输出的微小改变这件事变得更加容易。所以虽然sigmoid神经元和感知器有很多相似之处，但sigmoid神经元能够更容易指出我们应该如何选择权重和偏置来得到我们想要的输出。  

如果我们真的仅仅关注函数图形的形状而不是具体的形式，那么为什么我们选用[(3)](#（3）)作为具体的形式呢？事实上，我们在这本书中偶尔会用到$f(\cdot)$作为激活函数。不同函数形式的主要不同在于公式[(5)](#（5）)的偏导形式有所区别。稍后我们将看到当计算偏导时，用$\sigma$能够简化代数形式，因为e指数函数求偏导时形式比较优美。无论如何，$\sigma$都是神经网络中比较常用的一种激活函数，也是本书中用的最多的激活函数。  

那么如何解释sigmoid神经元的输出呢？显然，感知器和sigmoid神经元的一个重大不同就是sigmoid神经元不仅仅输出$0$或者$1$。它们可能是$0$到$1$之间的任何实数。，因此值可能像$0.173\ldots$和$0.689\ldots$都是有效的输出。这可能是有用的，举个例子来说，当我们想用输出的结果值代表输入图像的像素点平局密度时。但是有时可能是令人厌恶的。假设我们想要判断输出的结果是$9$或者不是$9$时。显然，用感知器输出的$0$或$1$来解释这件事更容易。但我们能够约定一些惯例来解释这种事，例如，当输出结果不小于$0.5$时就是$9$，小于$0.5$时不是$9$.当我们使用这种约定时，我会明确强调，因此它不应该成为一个问题。  

**Exercises**  

+ **sigmoid神经元模拟感知器，第一步** 

假设我们把感知器的所有权重和偏置都乘上一个正常数，$c > 0$。证明感知器的行为不会改变。  

+ **sigmiod神经元模拟感知器，第二步**

假设我们和上题的设定一样。同时也假设感知器神经网络的所有输入也都被选择。我们不需要实际的输入，我们只需要令输入值固定。假设对于输入$x$，对于任何感知器神经元选择的权重和偏置都使得$w \cdot x + b \neq 0$。现在把所有的感知器神经元换为sigmoid神经元，并把权重和偏置乘上常数$c>0$。证明，当$c \rightarrow \infty$时sigmoid神经元和感知器神经元的行为一致。为什么当$w \cdot x + b = 0$时这种结论就不成立了呢？

## 神经网络的结构

在下一节中，我会介绍一个神经网络能够很好地解决手写数字分类问题。作为准备，解释一些神经网络的术语有利于我们更好地理解。假设我们有如下的网络：  

<img src="/images/c1s3-1.png" width="50%" height="40%" />  

正如上面提到过的一样，最左边的一层被称为输入层，这一层的神经元被称为*输入神经元（input neurons）*.最右边或者输出层包含*输出神经元（output layer）*。中间的层被称为*隐藏层（hidden layer）*，因为它们既不是输入层也不是输出层。“隐藏”这个术语可能听起来有点神秘——我初次听到时以为它一定包含着某些深度的哲学或者数学含义——但是它真的只是意味着“既不是输入也不是输出”。这个网络包含了一个隐藏层，但是一些网络有多层的隐藏层。举例来说，下面的四层网络就包含两个隐藏层：  

<img src="/images/c1s3-2.png" width="70%" height="50%" />  

令人感到困惑的是，因为一些历史原因，想这种多层的神经网络有时被称为*多层感知器（multilayer perceptrons）*或者**MLPs**，尽管它是由sigmoid神经元而不是感知器神经元构成。在这本书中我不打算用MLP这个术语，因为它让人感到困惑，但是只是为了提醒你的确有这种称呼。  

输入层和输入层的设计通常是很简单的。举例来说，假设我们正努力识别一个手写数字图片是“9”或者其他数字。一个自然的方式就是设计一个神经网络去编码图片像素作为输入。如果图像是64乘64的灰度图像，那么我们一共会有$4,096 = 64 \times 64$个输入神经元，其中每个神经元的值都是$0$到$1$之间的灰度图像。那么输出层只会包含一个神经元，当输出值小于$0.5$时意味着“输入图像不是9”，当大于$0.5$时暗示着“输入图像是9”。  

输入层和输出层的设计通常比较直白，但是隐藏层的设计是一门艺术。当然我们也不可能把所有的设计过程总结为几条科学经验。神经网络研究员们设计了很多启发式的算法来构造隐藏层，用于完成不同的任务。举个例子来说，这样的启发式算法能用来帮助我们决定隐藏层神经元的数目。我们在这本书的后面会遇到几种启发式算法。  

到目前为止，我们已经讨论的神经网络都是上一层的输出作为下一层的输入，这些网络被称为前馈（*feedforward*）网络。这意味着网络结构没有环。因为有环的话停止条件将变得没有意义，因为我们不允许带环。  

当然也有一些人工神经网络也使得前馈环变为可能。这些模型被称为*[recurrent neural networks](http://en.wikipedia.org/wiki/Recurrent_neural_network)*。这种环不会造成问题是因为一个神经元的输出仅仅会在稍后的时间影响输入，而不是立即影响。（译者注：其实这里的解释并不是太好，循环神经网络并没有所谓的环状结构，环状的示意图只是为了便于理解。）

## 用简单的神经网络识别手写数字

定义了神经网络之后，让我们回到手写数字识别的问题上来。我们可以把手写数字识别问题拆分为两个子问题。首先，我们要找到一种方法能够把一张包含若干数字的图像分割为若干小图片，其中每个小图像只包含一个数字。举个例子，我们想将下面的图像

<img src="/images/c1s4-1.png" width="20%" height="10%" />

分割为6张小图像


<img src="/images/c1s4-2.png" width="40%" height="10%" />  


我们人类能够很容易地解决这个分割问题，但是让计算机去正确地分割图像是一件极具挑战的任务。当图像被分割之后，接下来的任务就是如何识别每个独立的手写数字。举个例子来说，我们想要程序去识别上述图像中的第一个数字，

<img src="/images/c1s4-3.png" width="10%" height="10%" />

结果是5。

我们将把精力集中在实现程序去解决第二个问题，即如何正确分类每个单独的手写数字。因为事实证明，只要你解决了数字分类的问题，分割问题相对来说不是那么困难。分割问题的解决方法有很多。一种方法是尝试不同的分割方式，用数字分类器对每一个切分片段打分。如果数字分类器对每一个片段的置信度都比较高，那么这个分割方式就能得到较高的分数；如果数字分类器在一或多个片段中出现问题，那么这种分割方式就会得到较低的分数。这种方法的思想是，如果分类器有问题，那么很可能是由于图像分割出错导致的。这种思想以及它的变种能够比较好地解决分割问题。因此，与其关心分割问题，我们不如把精力集中在设计一个神经网络来解决更有趣、更困难的问题，即手写数字的识别。

为了识别数字，我们将会使用一个三层神经网络：

<img src="/images/c1s4-4.png" width="60%" height="50%" />

这个网络的输入层是对输入像素编码的神经元。正如我们在下一节将要讨论的，我们的训练数据是一堆$28$乘$28$手写数字的位图，因此我们的输入层包含了$784 = 28 \times 28$个神经元。为了方便起见，我在上图中没有完全画出$784$个输入神经元。输入的像素点是其灰度值，$0.0$代表白色，$1.0$代表黑色，中间值表示不同程度的灰度值。

网络的第二层是隐层。我们为隐层设置了$n$个神经元，我们会实验$n$的不同取值。在这个例子中我们只展现了一个规模较小的隐层，它仅包含了$n = 15$个神经元。

网络的输出层包含了10个神经元。如果第一个神经元被激活，例如输出$\approx 1$，然后我们可以推断出这个网络认为这个数字是$0$。如果第二层被激活那么我们可以推断出这个网络认为这个数字是$1$。以此类推。更精确一些的表述是，我们把输出层神经元依次标记为$0$到$9$，我们要找到哪一个神经元拥有最高的激活值。如果$6$号神经元有最高值，那么我们的神经网络预测输入数字是$6$。对其它的神经元也如此。

你可能会好奇为什么我们用$10$个输出神经元。毕竟我们的任务是能让神经网络告诉我们哪个数字（$0, 1, 2, \ldots, 9$）能和输入图片匹配。一个看起来更自然的方式就是使用$4$个输出神经元，把每一个当做一个二进制值，结果取决于它的输出更靠近$0$还是$1$。四个神经元足够编码这个问题了，因为$2^4 = 16$大于$10$种可能的输入。为什么我们反而要用$10$个神经元呢？这样做难道效率不低吗？最终的判断是基于经验主义的：我们可以实验两种不同的网络设计，结果证明对于这个特定的问题而言，$10$个输出神经元的神经网络比$4$个的识别效果更好。但是令我们好奇的是为什么使用$10$个输出神经元的神经网络更有效呢。有没有什么启发性的思考能提前告诉我们用$10$个输出编码比使用$4$个输出编码更有好呢？

为了理解为什么我们这么做，我们需要从根本原理上理解神经网络究竟在做些什么。首先考虑有$10$个神经元的情况。我们首先考虑第一个输出神经元，它告诉我们一个数字是不是$0$。它能那么做是因为可以权衡从隐藏层来的信息。隐藏层的神经元在做什么呢？假设隐藏层的第一个神经元只是用于检测如下的图像是否存在：

<img src="/images/c1s4-5.png" width="10%" height="10%" />

为了达到这个目的，它通过对此图像对应部分的像素赋予较大权重，对其它部分赋予较小的权重。同理，我们可以假设第二，第三，第四个隐藏层的神经元是为检测下列图片是否存在：

<img src="/images/c1s4-6.png" width="20%" height="10%" />

正如你所猜到的那样，这四个图像拼到一起就组成了数字$0$ ：

<img src="/images/c1s4-7.png" width="10%" height="10%" />

如果所有这四个隐藏层的神经元被激活那么我们就可以推断出这个数字是$0$。当然，这不是我们推断出$0$的**唯一**方式——我们能通过很多其他合理的方式得到$0$ （举个例子来说，通过上述图像的转换，或者稍微变形）。但至少在这个例子中我们可以推断出输入的数字是$0$ 。

假设神经网络以上述方式运行，我们可以给出一个貌似合理的理由去解释为什么用$10$个输出而不是$4$个。如果我们有$4$个输出，那么第一个输出神经元将会尽力去判断数字的最高有效位是什么。把数字的最高有效位和数字的形状联系起来并不是一个简单的问题。很难想象出有什么恰当的历史原因，一个数字的形状要素会和一个数字的最高有效位有什么紧密联系。

上面我们说的只是一个启发性的思考。没有什么理由表明这个三层的神经网络必须按照我所描述的方式运行，即隐藏层是用来探测数字的组成形状。可能一个聪明的学习算法将会找到一些合适的权重能让我们仅仅用$4$个输出神经元就行。但是这个启发式的思考通常很有效，它会节省你大量时间去设计一个好的神经网络结构。

**练习**

* 通过在上述的三层神经网络加一个额外的一层就可以实现按位表示数字。额外的一层把原来的输出层转化为一个二进制表示，如下图所示。为新的输出层寻找一些合适的权重和偏移。假定原先的$3$层神经网络在第三层得到正确输出（即原来的输出层）的激活值至少是$0.99$，得到错误的输出的激活值至多是$0.01$。

<img src="/images/c1s4-8.png" width="60%" height="50%" />

## 通过梯度下降法学习参数

我们已经设计出了我们的神经网络结构，现在的问题是，它要如何学习来识别数字呢？首先，我们需要一个待学习的数据集——即所谓的训练数据集。我们将会使用[MNIST数据集](http://yann.lecun.com/exdb/mnist/)，其中包含了成千上万个扫描手写数字图像以及它们正确的分类。MNIST数据集是由[NIST](http://en.wikipedia.org/wiki/National_Institute_of_Standards_and_Technology)（the United States' National Institute of Standards and Technology，美国国家标准与技术研究院）收集了两个数据集后修改得到的子数据集构成，所以取名叫MNIST。下面是MNIST中的一些图像：

<img src="/images/c1s5-1.png" width="60%" height="10%" />

正如你所见，这些数字实际上就是我们在本章开始所提到的图像。当然，当我们测试我们的神经网络时，我们需要让它去识别训练集之外的数据。

MNIST数据集可以分为两个部分。第一个部分包含了60,000个用于训练的图像。这些图像是扫描了250个人的手写数字，他们之中一半是来自美国人口普查局的员工，另一半是高中生。这些图像是28乘28个像素点灰度图像。MNIST的第二个部分是10,000个用于测试的图像。同样，它们也是28乘28的灰度图像。我们将会用这些测试数据去评估我们的神经网络的识别效果。为了保证测试结果的准确性，这些测试数据是采样于另一个*不同的*250人的团体（尽管这些人也是美国人口普查局的员工和高中生）。这保证了我们的系统能够识别训练集之外的手写数字图像。

我们用$x$去定义训练输入。为了方便起见，我们将每一个训练输入$x$视为一个$28 \times 28 = 784$维的向量。向量中的一个元素代表图像的一个像素点的灰度值。我们定义输出为$y = y(x)$，每一个$y$是一个$10$维的向量。举例来说，对于一个数字$6$的训练图像$x$，我们期待的输出是$y(x) = (0, 0, 0, 0, 0, 0, 1, 0, 0, 0)^T$。这里的$T$是转置符号，它能将一个行向量变为列向量。

现在我们需要一个算法能让我们找到合适的权重和偏置，从而对所有的训练输入为$x$的输出都近似于$y(x)$。为了衡量我们当前取得的结果距离目标结果的好坏程度，我们定义一个*代价函数*：

{% raw %}
\begin{eqnarray} C(w,b) \equiv \frac{1}{2n} \sum_x \| y(x) - a\|^2. \tag{6}\end{eqnarray} 
{% endraw %}

公式里面的$w$表示所有的权重的集合，$b$表示所有的偏置，$n$是训练数据的个数，$a$代表$x$作为输入时输出层的向量，求和针对所有的训练输入$x$。显而易见，输出$a$取决于$x$, $w$和$b$，但是为了保持公式的简洁性，我们没有明确指出这种依赖性。符号$\| v \|$是指求向量$v$的范数。这时，我们称$C$为二次代价函数；有时我们也称它为*平方误差*或者*MSE*。通过代价函数的形式我们可以得知$C(w,b)$是非负的，因为加和的每一项都是非负的。另外，当所有的训练输入$x$的输出$a$基本都等于$y(x)$时，代价函数$C(w,b)$的值会变小，可能有$C(w,b) \approx 0$。因此如果我们的学习算法可以找到合适的权重和偏置使得$C(w,b) \approx 0$，那么这就是一个好的学习算法。反过来来说，如果$C(w,b)$很大，那么就说明学习算法的效果很差——这意味着对于大量的输入，我们的结果$a$与正确结果$y(x)$相差很大。因此，我们的训练算法的目标就是通过调整函数的权重和偏置来最小化代价函数$C(w,b)$。换句话说，我们想寻找合适的权重和偏置让代价函数尽可能地小。我们将使用一个叫做*梯度下降法（gradient descent）*的算法来达到这个目的。

为什么要介绍平方代价（quadratic cost）呢？毕竟我们最初所感兴趣的内容不是对图像正确地分类么？为什么不增大正确输出的得分，而是去最小化一个像平方代价类似的间接评估呢？这么做是因为在神经网络中，被正确分类的图像的数量关于权重、偏置的函数并不是一个平滑的函数。大多数情况下，对权重和偏置做出的微小变动并不会影响被正确分类的图像的数量。这会导致我们很难去刻画如何去优化权重和偏置才能得到更好的结果。一个类似平方代价的平滑代价函数能够更好地指导我们如何去改变权重和偏置来达到更好的效果。这就是为何我们集中精力去最小化平方代价，只有通过这种方式我们才能让分类器更精确。

即使知道了我们需要选择一个平滑的代价函数，你可能仍然会好奇为什么我们选择方程（6）中的二次函数。这是一个拍脑袋的决定么？是不是我们选择另一个不同的代价函数将会得到完全不同的权重和偏置呢？这是一个合理的问题，稍后我们会再次提到这个代价函数并做一些修改。尽管如此，方程（6）中的平方代价函数能让我们更好地理解神经网络的基本原理，因此我们目前先用它。

再次回顾一下，我们训练神经网络的目的是寻找合适的权重和偏置来最小化平方代价函数$C(w, b)$。这是一个明确定义了的问题，但是现在还存在很多让我们分散精力的部分——对权重$w$和偏置$b$合理的解释，隐藏在背后的$\sigma$函数，神经网络结构的选择，*MNIST*等等。实际上我们可以先忽略这些问题，把精力集中在最小化的问题上。现在让我们忘掉代价函数的具体形式，忘掉神经网络的结构，忘掉其它一切。现在想象我们仅仅是要去最小化一个给定的有很多变量的函数。我们即将要介绍一种可以解决最小化问题的技术，称作*梯度下降法*。然后我们再回到要在神经网络中最小化的函数上来。

假定我们要最小化某些函数，$C(v)$。它可能是任意的多元实值函数，$v = v_1, v_2, \ldots$。注意我们将会用$v$去代表$w$和$b$以强调它可能是任意的函数——我们不会把问题局限在神经网络中。假定$C(v)$有两个变量$v_1$ 和$v_2$ ：

<img src="/images/c1s5-2.png" width="50%" height="45%" />

我们的目标就是找到$C$在何处取得全局最小值。当然，对于上图的函数，我们能通过肉眼就可以找到最小值。这是因为我所展示的函数*太*简单了！一个一般的函数$C$，可能是一个复杂的多元函数，通过肉眼通常找不到它的最小值。

一种解决方法就是用数值计算的方法去计算出它的最小值。我们可以计算出偏导，利用偏导去寻找函数$C$的极值点。运气好的话我们的函数$C$只有一个或者少数的几个变量。但是变量过多的话将是一个噩梦。在神经网络中我们通常会需要*多得多得多*的变量——最大的神经网络的代价函数包含了数亿个权重和偏置。根本没法使用数值计算的方法！

（我们只讨论了$C$只有两个变量的情况，如果我说「嗨，如果函数有多于两个变量怎么办？」。对于这种情况我只能说很抱歉。请相信我把$C$看成一个二元函数是有助于我们的理解的。善于思考数学通常也包含善于利用多种直观的图片，学习什么时候用什么图片合适。）

所以，数值计算的方法没法完成这个任务了。幸运的是，有一个漂亮的类比预示着有一种算法能得到很好的效果。首先把我们的函数想象成一个山谷。我们想象有一个小球从山谷的斜坡滚落下来。我们的日常经验告诉我们这个球终会达到谷底。也许我们可以用这种思想来解决函数最小值的问题？我们随机地为小球选取一个起点，然后开始模拟小球滚落到谷底的运动过程。我们可以简单的通过计算$C$的导数（或者二阶导数）来模拟这个过程——这些导数将会告诉我们关于山谷「地形」的一切，从而告诉我们的小球该如何下落。

基于上述的想法，你可能会认为我们会写下牛顿运动定理，考虑摩擦力、重力等等。其实我们不打算真的去实现这个小球的类比——我们只想要一个算法去最小化$C$，而不是真的去模拟它真实的物理定律。小球的视角能激发我们的想象而不是束缚我们的思维。因此与其去考虑麻烦的物理定律，不如我们这样问自己：如果我们扮演一天的上帝，能够构造自己的物理定律，能够支配小球的下落方式，那么我们将会采取什么样的物理定律来让小球能够始终滑落到谷底呢？

为了更精确地描述这个问题，让我们想象一下如果我们在$v_1$方向移动一个很小的量$\Delta v_1$并在$v_2$方向移动一个很小的量$\Delta v_2$将会发生什么呢。通过计算可以告诉我们$C$将会产生如下改变：

{% raw %}
\begin{eqnarray} \Delta C \approx \frac{\partial C}{\partial v_1} \Delta v_1 + \frac{\partial C}{\partial v_2} \Delta v_2. \tag{7}\end{eqnarray}
{% endraw %}

我们将要寻找一种方式去选择$\Delta v_1$和$\Delta v_2$使得$\Delta C$为负，因为负值能够让小球下落。为了搞清楚如何选择，我们有必要定义$\Delta v$，它是描述$v$变化的向量，$\Delta v \equiv (\Delta v_1, \Delta v_2)^T$，$T$是转置符号。我们也定义$C$用来表示偏导的向量，$\left(\frac{\partial C}{\partial v_1}, \frac{\partial C}{\partial v_2}\right)^T$。我们用$\nabla C$来表示梯度向量：

{% raw %}
\begin{eqnarray} \nabla C \equiv \left( \frac{\partial C}{\partial v_1}, \frac{\partial C}{\partial v_2} \right)^T. \tag{8}\end{eqnarray}
{% endraw %}

待会我们将用$\Delta v$和$\nabla C$来重写$\Delta C$。在这之前我想先说明一些令人困惑的关于梯度的事情。当第一次碰到梯度的符号$\nabla C$时，人们常会好奇为什么$\nabla C$符号是这样。$\nabla$究竟代表什么？事实上你可以把梯度仅仅看做一个简单的数学记号——一个方便用来表示偏导的向量——这样我们就不必写两个符号了。这样来看，$\nabla$仅仅是一面旗帜，它告诉你：「嗨，$\nabla C$是一个梯度向量」。也有很多其他的关于$\nabla$的解释（比如，作为一种微分符号），但我们不需要这种观点。

定义好上述符号之后，关于$\Delta C$的表达式（7）能被重写成如下形式

{% raw %}
\begin{eqnarray} \Delta C \approx \nabla C \cdot \Delta v. \tag{9}\end{eqnarray}
{% endraw %}

这个表达式能很好地解释为什么$\nabla C$被称作梯度向量：$\nabla C$和$C$中$v$的变化密切相关，只是我们把它称为梯度罢了。但是这个式子真正让我们兴奋的是，它让我们看到了如何选取$\Delta v$才能让$\Delta C$为负数。假设我们选取

{% raw %}
\begin{eqnarray} \Delta v = -\eta \nabla C, \tag{10}\end{eqnarray}
{% endraw %}

这里的$\eta$是个很小的正数（就是我们熟知的*学习速率（learning rate）*）。等式（9）告诉我们$\Delta C \approx -\eta \nabla C \cdot \nabla C = -\eta \|\nabla C\|^2$。由于$\| \nabla C \|^2 \geq 0$，这保证了$\Delta C \leq 0$，例如，如果我们以等式（10）的方式去改变$v$，那么$C$将一直会降低，不会增加。（当然，要在（9）式的近似约束下）。这就是我们想要的特性！因此我们把（10）式看做梯度下降算法的「运动定律」。也就是说，我们用（10）式计算$\Delta v$，把小球位置$v$移动：

{% raw %}
\begin{eqnarray} v \rightarrow v' = v -\eta \nabla C. \tag{11}\end{eqnarray}
{% endraw %}

然后我们可以迭代地去更新。如果我们反复那么做，那么$C$会一直降低到我们想要寻找的全局最小值。

总结一下，梯度下降算法工作的方式就是重复计算梯度$\nabla C$，然后沿着梯度的*反*方向运动，即沿着山谷的斜坡下降。就像下图一样：

<img src="/images/c1s5-3.png" width="50%" height="45%" />

需要注意这种梯度下降的规则不代表真实的物理运动。在真实世界里，小球有势能，势能让小球可以在山谷斜坡上滚动，甚至顷刻间滚向山顶。只有加上摩擦力的影响才能保证小球会下落到谷底。我们选择$\Delta v$的规则不一样，我们就好像在命令「立马下降！」。这仍然是一个寻找最小值的好办法！

为了使我们的梯度下降法能够正确地工作，我们需要选择足够小的学习速率$\eta$使得等式（9）能得到很好的近似。如果不那么做，我们将会以$\Delta C > 0$结束，这显然不是一个很好的结果。与此同时，我们也不能把$\eta$设得过小，因为如果$\eta$过小，那么梯度下降算法就会变得异常缓慢。在真正的实现中，$\eta$通常是变化的，从而方程（9）能保持很好地近似度同时保证算法不会太慢。我们稍后会看这是如何工作的。

我已经解释了具有两个变量的函数$C$的梯度下降法。但事实上当$C$是其他多元函数时也能很好地运行。我们假设$C$是一个有$m$个变量$v_1,\ldots,v_m$的多元函数。我们对自变量做如下改变$\Delta v = (\Delta v_1, \ldots, \Delta v_m)^T$，那么$\Delta C$将会变为

{% raw %}
\begin{eqnarray} \Delta C \approx \nabla C \cdot \Delta v, \tag{12}\end{eqnarray}
{% endraw %}

这里的$\nabla C$是
{% raw %}
\begin{eqnarray} \nabla C \equiv \left(\frac{\partial C}{\partial v_1}, \ldots, \frac{\partial C}{\partial v_m}\right)^T. \tag{13}\end{eqnarray}
{% endraw %}

正如两个变量的例子一样，我们可以选取

{% raw %}
\begin{eqnarray} \Delta v = -\eta \nabla C, \tag{14}\end{eqnarray}
{% endraw %}

并且我们也会保证公式（12）中$\Delta C$是负数。这使得我们能够随着梯度得到函数的最小值，即使$C$是任意的多元函数，我们也能重复运用下面的规则

{% raw %}
\begin{eqnarray} v \rightarrow v' = v-\eta \nabla C. \tag{15}\end{eqnarray}
{% endraw %}

你可以把这个更新规则看作梯度下降算法的定义。这给我们提供了一种方式去通过重复改变$v$来找到函数$C$的最小值。然而这种方式并不总是有效的——有几种情况能导致错误的发生，使得我们无法从梯度得到函数$C$的全局最小值，这种情况我们会在后面的章节中讨论。但在实践中，梯度下降法通常效果非常好，在神经网络中这是一种非常有效的方式去求代价函数的最小值，从而帮助神经网络学习。

事实上，有一种观念认为梯度下降法是求最小值的最优策略。我们假设努力去改变$\Delta v$来让$C$尽可能地减小，减小量为$\Delta C \approx \nabla C \cdot \Delta v$。我们首先限制步长为固定值，即$\| \Delta v \| = \epsilon$ ，$\epsilon > 0$。当步长固定时，我们要找到使得$C$减小最大的下降方向。可以证明，使得$\nabla C \cdot \Delta v$取得最小值的$\Delta v$为$\Delta v = - \eta \nabla C$，这里$\eta = \epsilon / \|\nabla C\|$是由步长限制$\|\Delta v\| = \epsilon$所决定的。因此，梯度下降法可以被视为一种通过在$C$下降最快的方向上做微小变化来使得$C$立即下降的方法。

**练习**

* 证明最后一段的断言。提示：利用[柯西-斯瓦茨不等式](http://en.wikipedia.org/wiki/Cauchy%E2%80%93Schwarz_inequality)。

* 我已经解释了当$C$是二元及其多元函数的情况。那如果$C$是一个单变量的函数呢？你能给出梯度下降法在一元函数的几何解释么？


人们已经研究出很多梯度下降法的变种，其中还包括一些去真正模拟真实物理运动的做法。这些模拟小球的变种有很多优点，但是也有一个主要的缺点：这些方法必须去计算$C$的二阶偏导，这可能代价非常大。为了理解为什么这种做法代价高，假设我们想求所有的二阶偏导$\partial^2 C/ \partial v_j \partial v_k$。如果我们有上百万的变量$v_j$，那我们必须要计算数万亿级别的二阶偏导！这的确会造成很大的计算代价。不过也有一些避免这些问题的技巧，寻找梯度下降方法的替代品也是个很活跃的研究领域。但在这本书中我们将主要用梯度下降法（包括变种）去训练我们的神经网络。

我们在神经网络中应用梯度下降法呢？方式就是利用梯度下降法去寻找权重$w_k$和偏置$b_l$ 能减小（6）式中的代价函数。为了弄清楚具体是如何工作的，我们将用权重和偏置代替变量$v_j$来重新描述梯度下降算法的更新规则。我们描述「位置」的元素是$w_k$和$b_l$，梯度向量$\nabla C$对应$\partial C / \partial w_k$和$\partial C / \partial b_l$。申明了梯度下降中成分之后，我们得到

{% raw %}
\begin{eqnarray} w_k & \rightarrow & w_k' = w_k-\eta \frac{\partial C}{\partial w_k} \tag{16}\\ b_l & \rightarrow & b_l' = b_l-\eta \frac{\partial C}{\partial b_l}. \tag{17}\end{eqnarray}
{% endraw %}

通过重复更新我们能「滚到山底」，从而有望能找到代价函数的最小值。换句话说，这个规则能用在神经网络的学习中。

应用梯度下降规则有很多挑战。我们将在下一章深入讨论。但是现在我想提及一个问题。为了理解问题是什么，我们先回顾（6）中的二次代价函数。注意这个代价函数有着如下的形式$C = \frac{1}{n} \sum_x C_x$，也就是说，它是每个样本代价$C_x \equiv \frac{\|y(x)-a\|^2}{2}$的平均值。事实上，为了计算梯度$\nabla C$，我们需要为每个训练样本$x$单独地计算梯度值$\nabla C_x$，然后求平均值，$\nabla C = \frac{1}{n} \sum_x \nabla C_x$。不幸的是，当训练输入过大时会花费很长时间，这样会使学习变得相当缓慢。

有种叫做*随机梯度下降（SGD）*的算法能够用来加速学习过程。想法就是通过随机选取小量输入样本来计算$\nabla C_x$，进而可以计算$\nabla C$。采取少量样本的平均值可以快速地得到梯度$\nabla C$，这会加速梯度下降过程，进而加速学习过程。

更准确地说，SGD是随机地选取小量的$m$个训练数据。我们将选取的这些训练数据标号{% raw %}$X_1, X_2, \ldots, X_m${% endraw %}，并把它们称为一个*mini-batch*。我们选取的$m$要足够大才能保证$\nabla C_{X_j}$的平均值才能接近所有样本的平均值$\nabla C_x$，也就是说，  

{% raw %}
\begin{eqnarray} \frac{\sum_{j=1}^m \nabla C_{X_{j}}}{m} \approx \frac{\sum_x \nabla C_x}{n} = \nabla C, \tag{18}\end{eqnarray}
{% endraw %}

其中，第二个求和是针对整个训练数据集的。交换两边我们可以得到

{% raw %}
\begin{eqnarray} \nabla C \approx \frac{1}{m} \sum_{j=1}^m \nabla C_{X_{j}}, \tag{19}\end{eqnarray}
{% endraw %}

证实了我们可以通过计算随机选取的mini-batch的梯度来估计整体的梯度。

为了将这种想法更明确地联系到神经网络的学习中来，假设用$w_k$和$b_l$分别代表权重和偏置。SGD就是随机地选取大小为一个mini-batch的训练数据，然后去训练这些数据，

{% raw %}
\begin{eqnarray} w_k & \rightarrow & w_k' = w_k-\frac{\eta}{m} \sum_j \frac{\partial C_{X_j}}{\partial w_k} \tag{20}\\ b_l & \rightarrow & b_l' = b_l-\frac{\eta}{m} \sum_j \frac{\partial C_{X_j}}{\partial b_l}, \tag{21}\end{eqnarray}
{% endraw %}

这个和就是当前mini-batch中每个$X_j$的梯度值。然后我们再随机选取另一个mini-batch去训练，直到我们用完了所有的训练数据，这就完成了*一轮（epoch）*迭代训练。这样我们就会重新开始下一轮迭代。

值得一提的是，关于调节代价函数和mini-batch去更新权重和偏置有很多不同的约定。在（6）式中，我们通过因子$\frac{1}{n}$来缩放全部的代价函数。人们通常忽略$\frac{1}{n}$，直接计算单独训练样本代价之和，而不是求平均值。这对我们不能提前知道训练数据集大小的情况下特别有效。这可能发生在有更多的训练数据是实时加入的情况下。同样，mini-batch的更新规则（20）和（21）有时也会舍弃$\frac{1}{m}$。从概念上这也会有一点区别，因为它等价于重新调节学习速率$\eta$。但在对不同工作进行详细比较时，它是值得关注的。

我们能把SGD想象成一场民主选举：使用小规模的mini-batch计算梯度，比使用整个训练集计算梯度容易得多，就如开展一次民意调查比举行一次完整的选举容易得多。举个例子，在*MNIST*中有$60,000$个测试数据，我们选取mini-batch的大小为$m = 10$，这样，计算梯度的过程就加速了$6,000$倍！当然，这个估计可能并不准确——仍然会存在统计波动——但也没必要准确：我们只关心在某个方向上移动可以减少$C$，这意味着我们没必要准确去计算梯度的精确值。事实上，SGD在神经网络的学习中被广泛使用的十分有效的技术，它也是本书中展开的大多数学习技术的基础。

**练习**

* 梯度下降一个比较极端的版本就是让mini-batch的大小变为1。这就是说，给定一个输入$x$，我们通过规则$w_k \rightarrow w_k' = w_k - \eta \partial C_x / \partial w_k$ 和 $b_l \rightarrow b_l' = b_l - \eta \partial C_x / \partial b_l$更新权重和偏置。当我们选取另一个训练数据时也做同样的处理。其它的训练数据也做相同的处理。这种处理方式就是*在线学习（online learning）*或*增量学习（incremental learning）*。在在线学习中，神经网络在一个时刻只学习一个训练数据（正如人类的做法）。说出online learning相比mini-batch为20的SGD的一个缺点和一个优点。

让我们讨论一个令刚刚接触梯度下降的人困惑的问题，来总结这部分的内容。在神经网络中，代价函数$C$是一个关于所有权重和偏置的多元函数，因此在某种意义上来说，就是在一个高维空间定义了一个平面。有些人可能会那么想：「嗨，我必须要看见其它多出的维度」。他们会开始疑惑：「我不能想象出四维空间，更不用说五维（或者五百万维）」。是不是他们缺少某种只有超级数学家才有的超能力？当然不是。即使大多数专业的数学家也不能想象出四维空间的样子。他们会用一些其它的方式来表示什么在发生。正如我们上面所做的那样，我们可以用代数（而不是几何）来表示$\Delta C$如何变化才能让$C$减少。善于思考高维的人会在脑中做一些思想实验；我们的代数技巧是其中一个例子。这些方法可能不如观察三维那么直观，但我们熟悉这些方法之后会帮助我们思考更高维度。我不想在这里详细展开，如果你感兴趣，你可以读一下这篇关于数学家如何思考高维空间的[讨论](http://mathoverflow.net/questions/25983/intuitive-crutches-for-higher-dimensional-thinking)。我们讲的一些技术可能会有点复杂，但大多数内容还是比较直观的，任何人都能熟练掌握。

## 利用神经网络解决手写数字识别

好了，我们来利用梯度下降和MNIST训练集来写一段程序识别手写数字。我们将用短短的74行Python（2.7）代码来完成这个任务。第一步就是获得MNIST数据。如果你是一个git用户，那么你能够clone这本书的代码仓库，  
```bash
git clone https://github.com/mnielsen/neural-networks-and-deep-learning.git  
```
如果你不用git，那么可以在[链接](https://github.com/mnielsen/neural-networks-and-deep-learning/archive/master.zip)下载。  

我之前描述过，MNIST数据集包含60,000张训练图片和10,000张测试图片。这是MNIST的官方描述。实际上我们将要对数据稍微处理一下。我们保持测试集不变，但把60,000张MNIST训练集分为两部分：50,000张作为训练集，10,000张作为开发集（validation set）。我们这一章讲不会用到开发集，但是在书的后面章节我们会介绍利用开发集来调整超参数。当我提到“MNIST测试集”时，我是指那50,000张图片而不是原始的60,000张图片。  

除了MNIST数据集我们也需要一个Python库：[Numpy](http://numpy.org/),用于做快速的线性计算。如果你没有安装Numpy，你能从[这里](http://www.scipy.org/install.html)获得。  

我先解释神经网络的核心代码。最重要的是神经网络的类。下面是类初始化：  

```python
class Network(object):

    def __init__(self, sizes):
        self.num_layers = len(sizes)
        self.sizes = sizes
        self.biases = [np.random.randn(y, 1) for y in sizes[1:]]
        self.weights = [np.random.randn(y, x) for x, y in zip(sizes[:-1], sizes[1:])]
```

在这份代码中，list的大小代表神经网络的层数。例如我们想创建一个神经网络的实例，其中第一层包含2个神经元，第二层包含3个神经元，最后一层包含1个神经元，那么我们就能这么写代码：  

```python
net = Network([2,3,1])  
```

权重和偏置是利用Numpy的随机初始化函数`np.random.randn`来产生均值为0，标准差为1的高斯分布。我们的梯度下降法将从这个随机初始化开始。在后面的章节中，我们会找到一些更好的初始化方法，但目前我们先不介绍。注意初始化代码默认第一层是输入层并且没有设置第一层神经元的偏置，这是因为偏置仅仅用于计算后面层的输出。  

另外，偏置和权重存储在Numpy矩阵中。举个例子， `net.weight[1]`存储着第二层神经元到第三层神经元的权重。（它并不是第一层到第二层之间的权重，因为Python的列表开始下标是0。）`net.weight[1]`有些显得啰嗦，我们直接记做矩阵$w$。那么$w_{jk}$表示第二层的第$k^{\rm th}$个神经元到第三层的第$j^{\rm th}$个神经元的权重。$j$和$k$顺序可能看起来很奇怪，但着只是为了让激活函数写起来更优雅：  

{% raw %}
\begin{eqnarray} 
  a' = \sigma(w a + b).
\tag{22}\end{eqnarray}
{% endraw %}

有了这些，我们能够很容易从一个神经网络的实例中计算它的输出。我们先定义好*sigmoid*函数：  

```python
def sigmoid(z):
    return 1.0/(1.0+np.exp(-z))

```

注意，输入z是一个向量，Numpy会自动地把sigmoid函数应用到每个元素上。
接下来，我们为神经网络加上前馈神经网络的方法，当我们给出神经网络的输入a就能返回对应的输出。就是将等式（22）应用到每一层：  

```Python
def feedforward(self, a):
    """Return the output of the network if "a" is input."""
    for b, w in zip(self.biases, self.weights):
        a = sigmoid(np.dot(w, a)+b)
    return a

```

对我们神经网络最重要的事就是学习。为了达到这个目的，我们需要加上一个应用到随机梯度下降法的SGD方法。下面是代码。有些地方可能有点神秘，但我会在下面慢慢讲解。  

```Python
 def SGD(self, training_data, epochs, mini_batch_size, eta,test_data=None):
      """Train the neural network using mini-batch stochastic
      gradient descent.  The "training_data" is a list of tuples
      "(x, y)" representing the training inputs and the desired
      outputs.  The other non-optional parameters are
      self-explanatory.  If "test_data" is provided then the
      network will be evaluated against the test data after each
      epoch, and partial progress printed out.  This is useful for
      tracking progress, but slows things down substantially."""
      if test_data: n_test = len(test_data)
      n = len(training_data)
      for j in xrange(epochs):
          random.shuffle(training_data)
          mini_batches = [
              training_data[k:k+mini_batch_size]
              for k in xrange(0, n, mini_batch_size)]
          for mini_batch in mini_batches:
              self.update_mini_batch(mini_batch, eta)
          if test_data:
              print "Epoch {0}: {1} / {2}".format(
                  j, self.evaluate(test_data), n_test)
          else:
              print "Epoch {0} complete".format(j)
```

训练数据是tuple(x,y)，内容分别对应着输入和期望的输出。变量`epochs`和`mini-batch-size`分别代表训练时迭代的轮次和sample时一个batch的大小。eta是学习率，$\eta$。如果可选参数`test_data`被提供了，那么程序将会字每次迭代之后就会评估结果，打印出我们部分的过程。这对我们追踪过程有帮助，但是会明显降低程序的速度。  

代码的工作方式如下。在每轮迭代时，首先开始随机打乱训练数据，然后将它分为合适大小的mini-batches。这是一种简单的打乱数据的方式。对于每个mini-batch我们都会应用梯度下降法。这是通`self.update_mini_batch(mini_batch, eta)`,这会更新神经网络的权重和偏置。下面是`update_mini_batch`的代码：  

```python
def update_mini_batch(self, mini_batch, eta):
    """Update the network's weights and biases by applying
    gradient descent using backpropagation to a single mini batch.
    The "mini_batch" is a list of tuples "(x, y)", and "eta"
    is the learning rate."""
    nabla_b = [np.zeros(b.shape) for b in self.biases]
    nabla_w = [np.zeros(w.shape) for w in self.weights]
    for x, y in mini_batch:
        delta_nabla_b, delta_nabla_w = self.backprop(x, y)
        nabla_b = [nb+dnb for nb, dnb in zip(nabla_b, delta_nabla_b)]
        nabla_w = [nw+dnw for nw, dnw in zip(nabla_w, delta_nabla_w)]
    self.weights = [w-(eta/len(mini_batch))*nw 
                    for w, nw in zip(self.weights, nabla_w)]
    self.biases = [b-(eta/len(mini_batch))*nb 
                   for b, nb in zip(self.biases, nabla_b)]
```

大部分工作是由下面的这行完成：  

```python
delta_nabla_b, delta_nabla_w = self.backprop(x, y)
```

这其中包含着反向传播算法，这是一种快速计算代价函数梯度的算法。因此`update_mini_batch`的工作只是通过计算mini_bach的每个训练样本的梯度然后更新`self.weights`和`self.biases`。  

我不打算现在展示`self.backprop`的代码。我们将在下一节反向传播算法，包括`self.backprop`的代码。目前，我们就把它当成一个黑箱子就可以了，它能够计算每个训练样本的梯度。  

让我们现在看看完整的程序，包括注释。除了`self.backprop`之外，程序的其他部分都是很好解释的，大部分的工作都是`self.SGD`和`self.update_mini_batch`完成的。`self.backprop`用了一些额外的函数来辅助计算梯度，`sigmoid_prime`是用来计算$\sigma$函数的梯度，而关于`self.cost_derivative`函数我在这里不会讲。你能够从下面的代码和注释看懂程序的流程。我将会在下一章中讨论一些细节。虽然代码篇幅看起来很长，但是去掉注释也就只有74行。你能够从[Github](https://github.com/mnielsen/neural-networks-and-deep-learning/blob/master/src/network.py)获取全部的代码。  

```python
"""
network.py
​~~~~~~~~~~

A module to implement the stochastic gradient descent learning
algorithm for a feedforward neural network.  Gradients are calculated
using backpropagation.  Note that I have focused on making the code
simple, easily readable, and easily modifiable.  It is not optimized,
and omits many desirable features.
"""

#### Libraries
# Standard library
import random

# Third-party libraries
import numpy as np

class Network(object):

    def __init__(self, sizes):
        """The list ``sizes`` contains the number of neurons in the
        respective layers of the network.  For example, if the list
        was [2, 3, 1] then it would be a three-layer network, with the
        first layer containing 2 neurons, the second layer 3 neurons,
        and the third layer 1 neuron.  The biases and weights for the
        network are initialized randomly, using a Gaussian
        distribution with mean 0, and variance 1.  Note that the first
        layer is assumed to be an input layer, and by convention we
        won't set any biases for those neurons, since biases are only
        ever used in computing the outputs from later layers."""
        self.num_layers = len(sizes)
        self.sizes = sizes
        self.biases = [np.random.randn(y, 1) for y in sizes[1:]]
        self.weights = [np.random.randn(y, x)
                        for x, y in zip(sizes[:-1], sizes[1:])]

    def feedforward(self, a):
        """Return the output of the network if ``a`` is input."""
        for b, w in zip(self.biases, self.weights):
            a = sigmoid(np.dot(w, a)+b)
        return a

    def SGD(self, training_data, epochs, mini_batch_size, eta,
            test_data=None):
        """Train the neural network using mini-batch stochastic
        gradient descent.  The ``training_data`` is a list of tuples
        ``(x, y)`` representing the training inputs and the desired
        outputs.  The other non-optional parameters are
        self-explanatory.  If ``test_data`` is provided then the
        network will be evaluated against the test data after each
        epoch, and partial progress printed out.  This is useful for
        tracking progress, but slows things down substantially."""
        if test_data: n_test = len(test_data)
        n = len(training_data)
        for j in xrange(epochs):
            random.shuffle(training_data)
            mini_batches = [
                training_data[k:k+mini_batch_size]
                for k in xrange(0, n, mini_batch_size)]
            for mini_batch in mini_batches:
                self.update_mini_batch(mini_batch, eta)
            if test_data:
                print "Epoch {0}: {1} / {2}".format(
                    j, self.evaluate(test_data), n_test)
            else:
                print "Epoch {0} complete".format(j)

    def update_mini_batch(self, mini_batch, eta):
        """Update the network's weights and biases by applying
        gradient descent using backpropagation to a single mini batch.
        The ``mini_batch`` is a list of tuples ``(x, y)``, and ``eta``
        is the learning rate."""
        nabla_b = [np.zeros(b.shape) for b in self.biases]
        nabla_w = [np.zeros(w.shape) for w in self.weights]
        for x, y in mini_batch:
            delta_nabla_b, delta_nabla_w = self.backprop(x, y)
            nabla_b = [nb+dnb for nb, dnb in zip(nabla_b, delta_nabla_b)]
            nabla_w = [nw+dnw for nw, dnw in zip(nabla_w, delta_nabla_w)]
        self.weights = [w-(eta/len(mini_batch))*nw
                        for w, nw in zip(self.weights, nabla_w)]
        self.biases = [b-(eta/len(mini_batch))*nb
                       for b, nb in zip(self.biases, nabla_b)]

    def backprop(self, x, y):
        """Return a tuple ``(nabla_b, nabla_w)`` representing the
        gradient for the cost function C_x.  ``nabla_b`` and
        ``nabla_w`` are layer-by-layer lists of numpy arrays, similar
        to ``self.biases`` and ``self.weights``."""
        nabla_b = [np.zeros(b.shape) for b in self.biases]
        nabla_w = [np.zeros(w.shape) for w in self.weights]
        # feedforward
        activation = x
        activations = [x] # list to store all the activations, layer by layer
        zs = [] # list to store all the z vectors, layer by layer
        for b, w in zip(self.biases, self.weights):
            z = np.dot(w, activation)+b
            zs.append(z)
            activation = sigmoid(z)
            activations.append(activation)
        # backward pass
        delta = self.cost_derivative(activations[-1], y) * \
            sigmoid_prime(zs[-1])
        nabla_b[-1] = delta
        nabla_w[-1] = np.dot(delta, activations[-2].transpose())
        # Note that the variable l in the loop below is used a little
        # differently to the notation in Chapter 2 of the book.  Here,
        # l = 1 means the last layer of neurons, l = 2 is the
        # second-last layer, and so on.  It's a renumbering of the
        # scheme in the book, used here to take advantage of the fact
        # that Python can use negative indices in lists.
        for l in xrange(2, self.num_layers):
            z = zs[-l]
            sp = sigmoid_prime(z)
            delta = np.dot(self.weights[-l+1].transpose(), delta) * sp
            nabla_b[-l] = delta
            nabla_w[-l] = np.dot(delta, activations[-l-1].transpose())
        return (nabla_b, nabla_w)

    def evaluate(self, test_data):
        """Return the number of test inputs for which the neural
        network outputs the correct result. Note that the neural
        network's output is assumed to be the index of whichever
        neuron in the final layer has the highest activation."""
        test_results = [(np.argmax(self.feedforward(x)), y)
                        for (x, y) in test_data]
        return sum(int(x == y) for (x, y) in test_results)

    def cost_derivative(self, output_activations, y):
        """Return the vector of partial derivatives \partial C_x /
        \partial a for the output activations."""
        return (output_activations-y)

#### Miscellaneous functions
def sigmoid(z):
    """The sigmoid function."""
    return 1.0/(1.0+np.exp(-z))

def sigmoid_prime(z):
    """Derivative of the sigmoid function."""
    return sigmoid(z)*(1-sigmoid(z))

```
那么程序如何识别手写数字呢？让我们先加载MNIST数据集。我将用`mnist_loader.py`来做这个：  

```bash
>>> import mnist_loader
>>> training_data, validation_data, test_data = \
... mnist_loader.load_data_wrapper()

```

在加载了MNIST数据集之后我们将会搭建一个有30个隐层神经元的网络。我们将会import之前的python程序：  

```bash
>>> import network
>>> net = network.Network([784, 30, 10])

```

最后我们利用随机梯度下降法来对MNIST中的`train_data`进行学习，迭代轮次为30，mini-batch的大小设置为10，学习率设置为$\eta = 3.0$，  

```bash
>>> net.SGD(training_data, 30, 10, 3.0, test_data=test_data)

```

当你运行这段代码的时候就会发现输出很多信息，这些输出的目的并不是为了高效的代码，相反这些输出会降低代码的时间效率，但是这些信息是为了告诉我们神经网络目前的状态。正如你所见，在仅仅一个迭代之后准确率就达到了9,129/10,000,而且正确率逐渐增长：  

```bash
Epoch 0: 9129 / 10000
Epoch 1: 9295 / 10000
Epoch 2: 9348 / 10000
...
Epoch 27: 9528 / 10000
Epoch 28: 9542 / 10000
Epoch 29: 9534 / 10000
```
最终的训练结果大概在95%~95.42%之间！这是相当令人激动的。我应该提醒你，你的代码运行结果未必和我的完全一致，因为我们的权重和偏置是随机初始化的。本章给出的结果是我运行了三次之后取的最好的一次。  

让我们回到实验上来，我们把隐藏层的神经元变为100，这将会花费更长的时间来执行。  

```bash
>>> net = network.Network([784, 100, 10])
>>> net.SGD(training_data, 30, 10, 3.0, test_data=test_data)

```

可以确信的是这将会把准确率提高到96.59%。至少在这个例子中，用更多的隐藏层神经元能够帮助我们的得到更好的结果。  

当然，为了获得这样的准确率我已经对训练的迭代次数，mini-batch的大小和学习率$\eta$做了精心的选择。正如上面提到过的那样，这些就是我们神经网络的超参数，这和我们算法学习得到的参数(权重和偏置）有所不同。如果我们选了不好的超参数，那么我们可能得不到比较好的结果。例如，我们把学习率设置为$\eta=0.001$，  

```bash
>>> net = network.Network([784, 100, 10])
>>> net.SGD(training_data, 30, 10, 0.001, test_data=test_data)

```

结果就不是那么理想了,  

```bash
Epoch 0: 1139 / 10000
Epoch 1: 1136 / 10000
Epoch 2: 1135 / 10000
...
Epoch 27: 2101 / 10000
Epoch 28: 2123 / 10000
Epoch 29: 2142 / 10000
```

因此，即使我们刚开始的时候选择了比较差的超参数，但是这些差的结果也能给我们一些信息来帮助我们调整神经网络的超参数。  

通常来讲，调试神经网络是一项挑战，尤其是当我们的初始超参数选择比较差劲的时候。例如，我们设置30个隐藏层神经元，但是我们的学习率设置为$\eta=100.0$：  

```bash
>>> net = network.Network([784, 30, 10])
>>> net.SGD(training_data, 30, 10, 100.0, test_data=test_data)

```

这种高学习率的设置会导致以下的结果：  

```bash
Epoch 0: 1009 / 10000
Epoch 1: 1009 / 10000
Epoch 2: 1009 / 10000
Epoch 3: 1009 / 10000
...
Epoch 27: 982 / 10000
Epoch 28: 982 / 10000
Epoch 29: 982 / 10000
```

现在想一下我们是第一次遇到这个问题。当然，我们可以从之前的实验中得知需要降低学习率。但是当我们第一次遇到这个问题时，我们没有太多的实验结果能够指导我们该如何调整。我们可能不仅仅怀疑我们的学习率，还会怀疑神经网络的方方面面。我们可能会怀疑初始的权重和偏置是不是合理，或者怀疑训练数据胡是不是不够大，或者怀疑迭代次数不够多等等。当你最初遇到这个问题时候我们可能并不能确定是哪个方面出现的问题。  

我们所学到的教训就是调试神经网络不是那么一件容易的事情，它可能是一门艺术。你需要学会调试以求获得好的结果。更一般地说，我们需要培养一些启发式的直觉去选择好的超参数和好的网络结构。我们将会在本书中详细地讨论这些话题。  

之前我跳过了MNIST数据集加载的细节。其实是非常直接的。为了完整性，下面就是加载数据集的代码。MNIST数据集的结果在注释中解释。  

```python
"""
mnist_loader
​~~~~~~~~~~~~

A library to load the MNIST image data.  For details of the data
structures that are returned, see the doc strings for ``load_data``
and ``load_data_wrapper``.  In practice, ``load_data_wrapper`` is the
function usually called by our neural network code.
"""

#### Libraries
# Standard library
import cPickle
import gzip

# Third-party libraries
import numpy as np

def load_data():
    """Return the MNIST data as a tuple containing the training data,
    the validation data, and the test data.

    The ``training_data`` is returned as a tuple with two entries.
    The first entry contains the actual training images.  This is a
    numpy ndarray with 50,000 entries.  Each entry is, in turn, a
    numpy ndarray with 784 values, representing the 28 * 28 = 784
    pixels in a single MNIST image.

    The second entry in the ``training_data`` tuple is a numpy ndarray
    containing 50,000 entries.  Those entries are just the digit
    values (0...9) for the corresponding images contained in the first
    entry of the tuple.

    The ``validation_data`` and ``test_data`` are similar, except
    each contains only 10,000 images.

    This is a nice data format, but for use in neural networks it's
    helpful to modify the format of the ``training_data`` a little.
    That's done in the wrapper function ``load_data_wrapper()``, see
    below.
    """
    f = gzip.open('../data/mnist.pkl.gz', 'rb')
    training_data, validation_data, test_data = cPickle.load(f)
    f.close()
    return (training_data, validation_data, test_data)

def load_data_wrapper():
    """Return a tuple containing ``(training_data, validation_data,
    test_data)``. Based on ``load_data``, but the format is more
    convenient for use in our implementation of neural networks.

    In particular, ``training_data`` is a list containing 50,000
    2-tuples ``(x, y)``.  ``x`` is a 784-dimensional numpy.ndarray
    containing the input image.  ``y`` is a 10-dimensional
    numpy.ndarray representing the unit vector corresponding to the
    correct digit for ``x``.

    ``validation_data`` and ``test_data`` are lists containing 10,000
    2-tuples ``(x, y)``.  In each case, ``x`` is a 784-dimensional
    numpy.ndarry containing the input image, and ``y`` is the
    corresponding classification, i.e., the digit values (integers)
    corresponding to ``x``.

    Obviously, this means we're using slightly different formats for
    the training data and the validation / test data.  These formats
    turn out to be the most convenient for use in our neural network
    code."""
    tr_d, va_d, te_d = load_data()
    training_inputs = [np.reshape(x, (784, 1)) for x in tr_d[0]]
    training_results = [vectorized_result(y) for y in tr_d[1]]
    training_data = zip(training_inputs, training_results)
    validation_inputs = [np.reshape(x, (784, 1)) for x in va_d[0]]
    validation_data = zip(validation_inputs, va_d[1])
    test_inputs = [np.reshape(x, (784, 1)) for x in te_d[0]]
    test_data = zip(test_inputs, te_d[1])
    return (training_data, validation_data, test_data)

def vectorized_result(j):
    """Return a 10-dimensional unit vector with a 1.0 in the jth
    position and zeroes elsewhere.  This is used to convert a digit
    (0...9) into a corresponding desired output from the neural
    network."""
    e = np.zeros((10, 1))
    e[j] = 1.0
    return e


```

我已经说过上面的程序能够产生比较好的结果。但是着意味着什么呢？好是和谁比较的呢？我们需要找出一个baseline来证明我们的程序能够得到比较好的结果。当然，这些baseline中最简单的就是随机猜数字。这将大概有十分之一的概率成功。我们目前达到的效果比这好多了。  

那么有没有更普通的baseline呢？让我们想一个极端的例子：我们将考虑一个图片有多黑。举例来说，有着数字2的图片应该会比有着1的图片更黑，这是因为2比1有着更多的黑色像素点。  

<img src="/images/c1s6-1.png" height="10%" width="20%" />

这意味着用训练数据计算每个数字$0, 1, 2,\ldots, 9$的平均的黑色程度。当我们遇到新的数字时计算它的黑色程度，然后猜测离它黑度值最接近的那一个就是正确的数字。这个过程很简单，也容易实现。这比随机猜要准确，结果表明在10,000个测试数据中答对了2,225个，也就是22.25%的准确率。  

不难发现其他的想法能达到的准确率一般在20%到25%之间。如果你更努力一点，可能达到50%的准确率。但是要想达到更高的准确率需要用机器学习的算法。让我们尝试一种广为人知的机器学习算法：支持向量机（SVM）。如果你对SVM不熟悉也不要沮丧，我们不打算深究SVM的底层细节。我们可以用一个python库：[scikit-learn](http://scikit-learn.org/stable/),这里面提供了一个基于C库的SVM实现：[LIBSVM](http://www.csie.ntu.edu.tw/~cjlin/libsvm/)  

如果我们用SVM默认的设置那么最后会得到9,435/10,000的正确率。（代码在[这里](https://github.com/mnielsen/neural-networks-and-deep-learning/blob/master/src/mnist_svm.py)。这对于之前的简单分类方法效果好了很多。事实上，SVM的表现只比我们的神经网络差一点点。在后面的章节我们会介绍一些新的技术能够大大改善我们的神经网络。  

这并不是故事的结束。9435/10000是SVM默认参数所能够达到的结果。SVM有着很多可调参数。我在这里并不打算展示调参的过程，如果你感兴趣可以看[Andreas Mueller](http://peekaboo-vision.blogspot.ca/)的这篇[博客](http://peekaboo-vision.blogspot.de/2010/09/mnist-for-ever.html)。Mueller的结果证明一个精心调参的SVM能够的达到98.5%的准确率。换句话，一个精心调参的SVM能够仅仅之别错70个数据。这是个非常漂亮的结果。那么神经网络能做的更好么？  

事实上，能。目前，一个精心构造的神经网络性能比其他任何技术解决MNIST都要好，包括SVM。当前的技术是2013年达到的9979/10000。这是由 Li Wan, Matthew Zeiler, Sixin Zhang, Yann LeCun, and Rob Fergus共同完成的。我们将在书的稍后章节看到这些技术。这个水平已经超过人的识别水平了。而那些不能被正确识别的数字如下：  

<img src="/images/c1s6-2.png" height="15%" width="60%" />

我相信你肯定会承认这些数字是很难识别的！神经网络能够识别除了上述的21个数字之外的所有数字是非常了不起的一件事。通常，我们认为写一个程序解决手写数字识别需要一个复杂的算法。但神经网络其实并没有什么复杂的算法。它的困难点在于如何自动地学习。从某种意义上讲，我们的结果和那些复杂论文里面的结果的寓意都是下面的公式：  

> **sophisticated algorithm <= simple learning algorithm + good training data**

## 迈向深度学习

当我们对神经网络给出的效果感到振奋时，也会感受很疑惑。权重和偏置是被自动发现的。这就意味着我们不能立即给出神经网络的工作原理。我们能不能找到一种方式来理解网络分类手写数字的原则？更进一步，如果给出这样的原则，我们能做的更好么？  

让我们把问题表述更为明显，假设自从神经网络主导人工智能的几十年后。我们能够理解一个智能网络的工作方式么？可能这些网络对我们来说是不透明的，我们不理解权重和偏置，因为它们是自动学习的。在早期，人工智能研究人员希望构建人工智能的工作能够帮助人们理解智能或人脑的机制。但是结果可能是我们理解它们既不是人脑机制或人工智能的工作原理。  

为了强调这些问题，让我们回想在这章开始所提到的人工神经元。假设我们想判断一幅图片是不是人脸：  

<img src="/images/c1s7-1.JPG" height="30%" width="30%"/><img src="/images/c1s7-2.jpg" height="30%" width="30%"/><img src="/images/c1s7-3.jpg" height="30%" width="30%"/>

我们以解决手写数字识别问题的方法来解决这个问题。输入就是这些图片的像素值，输出用一个神经元来判断“是人脸”或“不是人脸”。  

假设我们真的如此设计，但是没有用到学习算法。相反，我们打算人工设计这个网络，选择一些合适的权重和偏置。那么我们该如何做呢？暂时先忘掉人工神经网络，我们能用的一个启发式想法就是把这个问题分解成一些子问题：图片上面的位置有眼睛么？图片中间有鼻子么？图片下面有眼睛么？等等。  

如果上面的问题的答案是“是”或甚至是“可能是”，那么我们就能推断这张图片有可能是人脸。相反的，如果答案是“否”，那么就不是人脸。  

当然这只是个粗略的启发式想法，它会有很多缺点。可能这个人是秃头，因此他们没有头发。可能我们仅仅能够看到脸的一部分，或者脸的呈现是有一定角度的，因此一些特征很难获取。这些启发式的思考也让我们想到利用神经网络解决这些子问题，然后通过把这些子问题结合起来构建一个人脸识别的神经网络。下面是可能的网络结构。注意这并不是现实中解决人脸识别的算法。它只是帮助我们建立网络工作机制的直觉。  

<img src="/images/c1s7-4.png" height="60%" width="80%" />  

很显然把这些子网络再分解也是合理的。假设我们考虑这样一个问题：“左上角有眼睛么？”这能够被分解为子问题：“这个地方有睫毛么？”；“这个地方有眉毛么？”；“这个地方有虹膜么？”。等等。当然这些问题在现实中应该包含位置信息。让我们把问题简化一下，“左上角有眼睛么？”能够被分解为：  

<img src="/images/c1s7-5.png" height="60%" width="80%" />  

这些问题同样也能被分解，一直到很多很多层。最后，我们所处理的子问题网络能够用简单的几个像素点所回答。  

最后把这个复杂的问题——识别人脸——分解成非常多的能够用几个像素点回答的问题。这可能需要很多层神经网络，前面的层用来回答简答的问题，而后面的层用来回答更困难的更抽象的问题。这种多层的神经网络也被称为*深度神经网络(deep neural networks)*  

当然我没说如何递归的分解子网络。人工设计权重和偏置显然是不实际的。相反的，我们想利用一种学习算法自动地学习权重和偏置，这样就能够训练多层神经网络了。研究人员在上个世纪80年代到90年代尝试利用*随机梯度下降法（stochastic gradient descent）*和*反向传播（backpropagation）*用来训练深度神经网络。但是很不幸，他们并没有取得很大的成功。因为这种方法的速度非常慢，在实际中并不实用。  

自从2006年以来，已经有大量的技术来学习深度神经网络。这些技术也利用了随机梯度下降法和反向传播，但引入了一些新的想法。这些技术让深层神经网络的学习变为可能。事实证明这些深度网络在很多问题上性能比浅层神经网络要好，因为它们能够解决抽象层次更高的问题。这有点像传统的编程语言使用模块化的设计和抽象的思想，使复杂的计算机程序的能够被创建。

# 反向传播的原理
## 前言
在上一章节中，我们看到了神经网络能够通过梯度下降法来学习权重和偏置。但是在我们的解释中有一个缺口：我们没有讨论如何计算代价函数的梯度。这的确是一个重要的缺口！在这一章节中，我会介绍通过反向传播算法（*backpropagation*）来计算梯度。  

反向传播算法最初是在20世纪70年代被提出，但是它的重要性并没有认识到，直到1986年的一篇由David Rumelhart, Geoffrey Hinton和Ronald Williams三人提出的[著名论文](http://www.nature.com/nature/journal/v323/n6088/pdf/323533a0.pdf)，它的重要性才被别人认知。这篇文章描述了几种神经网络通过反向传播算法能够得到比其他算法好的效果，解决了之前一直不能解决的问题。今天，反向传播算法成为了神经网络的标配。  

这章涉及的数学可能比剩余的其它章节都要多。如果你对数学不感兴趣，那么可以跳过这章，仅仅把反向传播算法当成一个黑箱子来使用。那么为什么要花费时间研究这些细节呢？  

理由当然是为了更好地理解神经网络。反向传播的核心思想就是对于任意权重$w$计算关于代价函数$C$的偏导：$\partial
C / \partial w$（当然对于偏置$b$也是如此）。这个表达式高数我们当改变权重或者偏置时代价函数改变地有多快。这个表达式有点复杂，它也有比较漂亮的形式，每个元素都有一个自然，直观的解释。因此，反向传播算法并不仅仅是一种快速学习的算法。它其实给了我们一种直觉要如何改变权重和偏置来从整体改变神经网络的行为。这值得我们细致地研究一下。  

## 基于矩阵计算神经网络输出

在讨论反向传播算法之前，让我们拿一个基于矩阵的快速计算神经网络输出的算法来作为热身。我们其实在上一章节的结尾已经简单描述过这个算法，但是我只是一笔带过，所以这值得我们重温以下它的细节。特别的是，在反向传播的算法中，这还是一种舒服的符号表达。  

让我们开始用一种具体的方式来表示神经网络中的权重符号。我将会用$w^l_{jk}$来表示$(l-1)^{\rm th}$层的第$k^{\rm th}$个神经元到第$l^{\rm th}$层的第$j^{\rm th}$个神经元之间的权重。我们用下图来表示从第二层的第四个神经元到第三层的第二个神经元之间的权重：  

<img src="/images/c2s1-1.png" height="50%" width="80%" />

这些记号看起来有点麻烦，需要一点时间来消化。但是很容易就会发现这些符号其实很简单，自然。符号中比较奇怪的是关于$j$和$k$。你可能觉得用$j$表示输入，用$k$表示输出比较自然，而不是相反。我稍后会解释为什么那么表示。  

我们用类似的符号来表示神经网络的偏置和输出值。更具体的说，我们用$b^l_j$表示$l^{\rm th}$层第$j^{\rm th}$个神经元的偏置。用$a^l_j$表示第$l^{\rm th}$层第$j^{\rm th}$个神经元的输出。下面的图展示了符号的表示：  

<img src="/images/c2s1-2.png" height="50%" width="50%" />

根据上述的符号表示，我们可以把第{% raw %}$l^{\rm th}${% endraw %}层第$j^{\rm th}$个神经元的输出值表示为下面的式子：  
{% raw %}
\begin{eqnarray} 
  a^{l}_j = \sigma\left( \sum_k w^{l}_{jk} a^{l-1}_k + b^l_j \right),
\tag{23}\end{eqnarray}
{% endraw %}

这个式子是所有第$\(l-1\)^{th}$层神经元的加和。为了用矩阵的形式重写表达式，我们为每一层$l$定义权重矩阵$w^l$。权重矩阵$w^l$是的实体是所有连接到第$l^{\rm th}$层神经元的权重，那就是说，矩阵的第$j^{\rm th}$行，第$k^{\rm th}$列的表示为：$w^l_{jk}$。偏置向量$b^l$和输出向量$a^l$的表示方式都一样。  

最后我们需要向量化函数例如$\sigma$。我们已经看到了向量化表示的简洁性。我们向量化函数意味着将函数应用到输入向量的每一个元素。例如，我们有下面的函数：$f(x) = x^2$那么向量化之后可以表示为:  

{% raw %}
\begin{eqnarray}
  f\left(\left[ \begin{array}{c} 2 \\ 3 \end{array} \right] \right)
  = \left[ \begin{array}{c} f(2) \\ f(3) \end{array} \right]
  = \left[ \begin{array}{c} 4 \\ 9 \end{array} \right],
\tag{24}\end{eqnarray}
{% endraw %}


这其实就是向量化的$f$是对输入矩阵的每个元素进行平方。带着这种想法，我们的[(23)]()式能被重写为以下形式：  

{% raw %}
\begin{eqnarray} 
  a^{l} = \sigma(w^l a^{l-1}+b^l).
\tag{25}\end{eqnarray}
{% endraw %}

这个表达式给我们一种更加全局的视角来思考一层的激活值是如何与上一层的激活值产生关联：我们仅仅将权值矩阵乘上这个激活值，然后加上偏置向量，最后在应用$\sigma$函数。这个全局的视角通常比逐个神经元的视角更加简单，更加简洁。这将帮我们远离复杂下标的苦海。这种表示方式在实战中也很有用，因为大多数的矩阵库都提供矩阵运算，向量加法和向量化的操作。  

当我们用（25）式计算$a^l$时，我们计算了中间量{% raw %}$z^l \equiv w^l a^{l-1}+b^l${% endraw %}。（25）式有时也写作$a^l =
\sigma(z^l)$。值得注意的是{% raw %}$z^l_j = \sum_k w^l_{jk} a^{l-1}_k+b^l_j${% endraw %}，这就是说{% raw %}$z^l_j${% endraw %}是第$l$层的第$j$个神经元的加权输入。  

## 关于代价函数的两个假设

我们反向传播的目的就是计算代价函数关于权重和偏置的偏导。为了能使反向传播可用，我们需要对代价函数的形式做两个假设。在做假设之前我们先举一个代价函数的例子。我们利用上章提到过的均方差代价函数作为我们的例子：
{% raw %}
\begin{eqnarray}C = \frac{1}{2n} \sum_x \|y(x)-a^L(x)\|^2,\tag{26}\end{eqnarray} 
{% endraw %}
这里$n$是总的训练样本数目；$y=y(x)$是每个训练样本$x$对应的输出;$L$代表网络的层数；$a^L=a^L(x)$是网络最后一层的输出。  

那么我们需要对代价函数$C$作什么样的假设才能使得反向传播好使呢？第一个关于代价函数的假设就是它能够写成每个单独训练样本的代价函数$C_x$之和的均值$C=\frac{1}{n} \sum_x C_x$.对于均方差代价函数，$ C_x=\frac{1}{2} \|\|y-a^L\|\|^2 $.这个假设对我们这本书遇到的其他所有代价函数都成立。  

我们需要这个假设的理由是反向传播实际上是让我们对每一个训练样本计算偏导$\partial C_x/\partial w$和$\partial C_x/\partial b$。然后我们通过对训练样本求平均重新得到$\partial C/\partial w$和$\partial C/\partial b$。事实上我们一旦有了这个假设就假定我们的训练样本$x$是固定的。我们最后会把$x$放回去，但是为了简化我们先把下标去掉。  

第二个关于代价函数的假设是它能表示成输出的函数：  

<img src="/images/c2s2-1.png" height="140%" width="60%" />

举个例子，我们的均方差代价函数满足这个要求，因为它能够表示为:
{% raw %}
\begin{eqnarray} C=\frac{1}{2}\|y-a^L\|^2=\frac{1}{2}\sum_j(y_j-a^L_j)^2,\tag{27}\end{eqnarray}
{% endraw %}

## Hadamard product，$s\odot t$

反向传播算法是基于常见的线性代数操作。但是有一个操作并不常用。我们假设$s$和$t$是两个同维度的向量。那么我们用$s \odot t$代表对两个向量对应位置的元素逐个求积。因此$s \odot t$就是$(s \odot t)_j=s_jt_j$.举个例子， 
{% raw %}
\begin{eqnarray}
\left[\begin{array}{c} 1 \\ 2 \end{array}\right] 
  \odot \left[\begin{array}{c} 3 \\ 4\end{array} \right]
= \left[ \begin{array}{c} 1 * 3 \\ 2 * 4 \end{array} \right]
= \left[ \begin{array}{c} 3 \\ 8 \end{array} \right].
\tag{28}\end{eqnarray}
{% endraw %}
这种形式的乘积被称为Hadamard积。好的矩阵库会直接支持这种向量运算。

## 反向传播的四个基本方程

反向传播就是要理解如何修改权重和偏置来该该改变代价函数。最终，这意味着我们要计算$\partial C /\partial w^l_{jk}$和$\partial C /\partial b^l_j$。但是为了计算这些我们先引入一个中间量$\delta^l_j$，我们称之为第$l$层第$j$个神经元产生的误差。反向传播给我们一种途径去计算误差，$\delta^l_j$，然后我们把{% raw %}$\delta^l_j${% endraw %}和{% raw %}$\partial C/\partial w^l_{jk}${% endraw %}，{% raw %}$\partial C /\partial w^l_j${% endraw %}关联起来。  

下面的图片展示了误差是如何定义的：  

<img src="/images/c2s4-1.png" height="50%" width="80%" />

恶魔位于第$l$层第$j$个神经元的位置。当输入到来时，它对加权的输入做了一点改动$\Delta z^l_j$，因此输出的结果不是$\delta(z^l_j)$，而是$\delta(z^l_j+\Delta z^l_j)$。这个改变会在后面层的神经网络中传播，最终会对总的误差造成的改变量为$\frac{\partial C}{\delta z^l_j} \Delta z^l_j$。  

现在，假设这个恶魔是一个好的恶魔，正努力帮助你减少误差，例如，它们正努力寻找$\Delta z^l_j$使得你的代价函数减小。我们现在假设$\frac{\delta C}{\delta z^l_j}$有一个较大的值（无论是正数还是负数）。那么我们能够通过取与$\frac{\delta C}{\delta z^l_j}$相反符号的$\Delta z^l_j$来大大降低代价函数。相反，当$\frac{\delta C}{\delta z^l_j}$接近0时，并不能带来多大提升。因此我们有一个启发式的直觉，可以通过$\frac{\partial C}{\partial z^l_j}$来衡量一个神经元的误差。  

我们定义$\delta^l_j$为
{% raw %}
\begin{eqnarray} \delta^l_j \equiv \frac {\partial C}{\delta z^l_j}. \tag{29} \end{eqnarray}
{% endraw %}
我们用$\delta^l$来表示第$l$层的误差，然后将这些误差关联到{% raw %}$\partial C/\partial w^l_{jk}${% endraw %}和{% raw %}$\partial C/\delta b^l_j${% endraw %}.  

你可能会好奇为什么我们改变加权后的输入$z^l_j$。当然，如果我们改变输出的激活值$a^l_j$，用$\frac {\partial C}{\partial a^l_j}$可能看起来更自然。如果你那么做的话，你会发现反向传播算法会在代数形式上变得异常复杂。因此我们坚持用$\delta^l_j=\frac {\partial C}{\partial z^l_j}$作为衡量误差的标准。  

反向传播是基于四个基本方程式。这些方程式会给我们提供一种方式来计算误差$\delta^l$和代价函数的梯度。我将会在下面提出这四个方程。但是需要提醒一些，理解这些方程需要一些耐心和时间，期望一下子理解它们会让你比较沮丧。  

### **输出层的误差，$\delta^L$** 
<span id="BP1">
{% raw %}
\begin{eqnarray} 
  \delta^L_j = \frac{\partial C}{\partial a^L_j} \sigma'(z^L_j).
\tag{BP1}\end{eqnarray}
{% endraw %}
</span>
这是个非常自然的方程。右边的第一项，$\partial C/\partial a^L_j$衡量了当输入层的第$j^{th}$个神经元的激活值个改变时代价函数改变的速度。例如，当$C$并不依赖于具体的某个输出神经元$j$，那么$\delta^L_j$将会比较小，这正是我们期望的结果。第二项{% raw %}$\sigma'(z^L-j)${% endraw %}衡量了激活函数$\sigma$在$z^L_j$改变的速度。  

注意到(**BP1**)是非常容易计算的。当我们在神经网络的前馈过程中会计算$z^L_j$的值，那我们只需要一点额外的开销就能够计算出$\sigma'(z^L_j)$。而$\partial C / \partial a^L_j$的具体形式取决于代价函数的形式，当代价函数确定时该项也比较容易计算。例如，如果我们用均方差代价函数$C=\frac{1}{2}\Sigma_j(y_j-a_j)^2 $，那么$\partial C/\partial a^L_j = (a_j-y_j)$,这显然很容易计算。  

方程(**BP1**)是对$\delta^L$的逐个元素进行计算的。这是一个极好的表达式，但并不是我们在反向传播算法中想要的矩阵形式。当然，也很容易把这个式子写成,

\begin{eqnarray} 
  \delta^L = \nabla_a C \odot \sigma'(z^L).
\tag{BP1a}\end{eqnarray}

这里，$\nabla_a C$被定义为偏导$\partial C /\partial a^L_j$组成的向量。你可以认为$\nabla_a C$是$C$对每个输出层激活值的改变率。很容易可以看出(**BP1a**)和(**BP1**)是等价的，因此我们将用(**BP1**)来代表这两种形式。举个例子，如果我们的代价函数为均方差代价函数，那么我们得到$\nabla_a C = (a^L-y)$，我们的矩阵形式的(**BP1**)变为

\begin{eqnarray} 
  \delta^L = (a^L-y) \odot \sigma'(z^L).
\tag{30}\end{eqnarray}

正如你所见，这个表达式里面的每个元素都完美的表示成矩阵形式，这很容易用一些例如Numpy之类的库来运算。 

### **误差$\delta^l$和误差$\delta^{1+1}$的方程**  
<span id="BP2" \>
\begin{eqnarray} 
  \delta^l = ((w^{l+1})^T \delta^{l+1}) \odot \sigma'(z^l),
\tag{BP2}\end{eqnarray}

这里$(w^{l+1})^T$是第$(l+1)^{th}$层权重$w^{l+1}$的转置。这个方程看起来很复杂，但是每个元素都有一个很好的解释。假设我们知道了第$(l+1)^{th}$层的误差$\delta^{l+1}$。当我们乘上转置矩阵$(w^{l+1})^T$时，我们能够直观的认为误差沿着网络向后传播，这给我们了一种计算第$l$层误差的方法。最后我们乘上Hadamard积$\odot \sigma'(z^l)$。  

通过结合(**BP1**)和(**BP2**)，我们能够计算任意层网络的误差$\delta^l$。我们首先利用(**BP1**)来计算$\delta^L$，然后利用(**BP2**)来计算$\delta^{L-1}$，然后再用(**BP2**)得到$\delta^{L-2}$,等等。  

### 代价函数关于偏置的变化率
<span id="BP3" \>
\begin{eqnarray}  \frac{\partial C}{\partial b^l_j} =
  \delta^l_j.
\tag{BP3}\end{eqnarray}

这就是说，误差$\delta^l_j$恰好等于变化率$\partial C /\partial b^l_j$。这是一个重要的消息，因为(**BP1**)和(**BP2**)已经告诉了我们如何计算$\delta^l_j$。我们能够重写(**BP3**)成如下的形式：

\begin{eqnarray}
  \frac{\partial C}{\partial b} = \delta,
\tag{31}\end{eqnarray}

这里可以理解为$\delta$和$b$在同一个神经元被计算。

### 代价函数关于权重的变化率
<span id="BP4" \>
\begin{eqnarray}
  \frac{\partial C}{\partial w^l_{jk}} = a^{l-1}_k \delta^l_j.
\tag{BP4}\end{eqnarray}

这告诉我们偏导$\partial C /\partial w^l_{jk}$的计算与$\delta^l$和$a^{l-1}$有关，而这两项我们早就知道如何计算了。这个等式能够重写成向量形式
{% raw %}
\begin{eqnarray}
\frac{\partial C}{\partial w} = a_{\rm in} \delta_{\rm out},
\tag{32}\end{eqnarray}
{% endraw %}
我们可以把这个描述为下图：

<img src="/images/c2s5-1.png" \>

(**32**)式的一个比较好的结论就是当激活值{% raw %}$a_{in}${% endraw %}很小时，$a_{\rm in} \approx 0$，梯度项$\partial C/\partial w$也将变得比较小。在这种情况下我们会说权重学习的很慢，即在梯度下降的过程中它并没有改变多少。  

我们能够从(**BP1**)-(**BP4**)中得到其他的一些直觉。让我们先从输出层开始。考虑(**BP1**)中的$\sigma'(z^L_j)$。回顾上章中的[sigmoid函数](#sigmoid)，当$\sigma(z^L_j)$接近$0$或$1$时$\sigma$函数会变得非常平坦。这会导致$\sigma'(z^L_j) \approx 0$.因此当最后一层的激活值接近$0$或接近$1$时，我们的权重将会学习的非常缓慢，这就是我们熟称的**饱和**。当然同样的情况也会发生在偏置上。  

在其它层的神经元上也有相同的结论。我们看(**BP2**)中的$\sigma'(z^l)$。这意味着当神经元接近饱和时$\delta^l_j$也会变得很小。这会导致任何层的饱和神经元都会使权重学习速度变慢。  

上面的这些结论并没有给我们带来很大的惊喜。但是他们能够帮助我们理解神经网络在学习的过程中都发生了什么。这四个方程并不依赖于任何具体形式的激活函数（我们稍后就会看到这四个方程的证明过程并不依赖于激活函数的形式）。我们也能够利用这些方程设计我们的激活函数。举个例子，假设我们选择一个激活函数$\sigma$使得$\sigma'$始终是正的，并且永远不会接近$0$。这将会避免因神经元饱和导致学习速率下降的情况。稍后我们会看到对激活函数的修改。牢记这四个方程将会帮助我们理解为什么要修改以及他们会产生什么样的影响。  

<img src="/images/c2s5-2.png" \>

## 四个方程的证明

接下来我们会证明这4个方程。所有的这四个方程的推导都利用求导中的链式法则。如果你对链式法则很熟悉，我建议你在看本节之前自己独立推导这四个方程。
### BP1的证明
让我们先从(**BP1**)开始，这个表达式给出了输出层误差$\delta^L$。为了证明这个等式，我们先回顾一下误差的定义：
\begin{eqnarray}
  \delta^L_j = \frac{\partial C}{\partial z^L_j}.
\tag{36}\end{eqnarray}
通过利用链式法则我们能够得到如下的结果，
\begin{eqnarray}
  \delta^L_j = \sum_k \frac{\partial C}{\partial a^L_k} \frac{\partial a^L_k}{\partial z^L_j},
\tag{37}\end{eqnarray}
这里的和是所有输出层神经元$k$相加。当然输出层第$k$个神经元的激活值$a^L_k$仅仅当$k=j$时才依赖于出入权重$z^L_J$。因此当$k \neq j$时$\partial a^L_k / \partial z^L_j$将会消失。这样我们就能够把这个等式简化成如下的形式：
\begin{eqnarray}
  \delta^L_j = \frac{\partial C}{\partial a^L_j} \frac{\partial a^L_j}{\partial z^L_j}.
\tag{38}\end{eqnarray}
由于$a^L_j = \sigma(z^L_j)$，因此右边的项可以写成
\begin{eqnarray}
  \delta^L_j = \frac{\partial C}{\partial a^L_j} \sigma'(z^L_j),
\tag{39}\end{eqnarray}
这就是([BP1](#BP1))的非向量形式。
### BP2的证明
接来我们证明([BP2](#BP2)),它是用下一层的误差$\delta^{l+1}$来表示当前层的误差$\delta^l$。为了达到这个目的我们要把$\delta^l_j = \partial C / \partial
z^l_j$重写成项为$\delta^{l+1}_k = \partial C / \partial z^{l+1}_k$的形式。我们可以利用链式法则，
{% raw %}
\begin{eqnarray}
  \delta^l_j & = & \frac{\partial C}{\partial z^l_j} \tag{40}\\
  & = & \sum_k \frac{\partial C}{\partial z^{l+1}_k} \frac{\partial z^{l+1}_k}{\partial z^l_j} \tag{41}\\ 
  & = & \sum_k \frac{\partial z^{l+1}_k}{\partial z^l_j} \delta^{l+1}_k,
\tag{42}\end{eqnarray}
{% endraw %}
在最后一行我们交换了左右两式，并且用$\delta^{l+1}_k$代替了它的定义$\frac {\partial z^{l+1}_k}{\partial z^l_j}$。为了计算第一项，我们首先看一下展开形式：
{% raw %}
\begin{eqnarray}
  z^{l+1}_k = \sum_j w^{l+1}_{kj} a^l_j +b^{l+1}_k = \sum_j w^{l+1}_{kj} \sigma(z^l_j) +b^{l+1}_k.
\tag{43}\end{eqnarray}
{% endraw %}
微分之后我们得到
{% raw %}
\begin{eqnarray}
  \frac{\partial z^{l+1}_k}{\partial z^l_j} = w^{l+1}_{kj} \sigma'(z^l_j).
\tag{44}\end{eqnarray}
{% endraw %}
替换回**(42)**式我们有
{% raw %}
\begin{eqnarray}
  \delta^l_j = \sum_k w^{l+1}_{kj}  \delta^{l+1}_k \sigma'(z^l_j).
\tag{45}\end{eqnarray}
{% endraw %}
这就是**(BP2)**的非向量形式。
### BP3的证明
通过上面的两个方程我们可以计算出每一层的误差，那么下面我们就需要利用这个计算误差函数对权重和偏置的偏导用以梯度下降。首先我们先证明误差对偏置的偏导，通过链式法则我们有
{% raw %}
\begin{eqnarray}
\nonumber\frac {\partial C}{\partial b^l_j} & = & \frac {\partial C}{\partial x^l_j} \frac {\partial z^l_j}{\partial b^l_j} \\
& = & \delta^l_j
\nonumber\end{eqnarray}
{% endraw %}
### BP4的证明
接下来我们证明代价函数对权重的偏导。
根据链式法则有
{% raw %}
\begin{eqnarray}
\nonumber\frac {\partial C}{\partial w^l_{jk}} & = & \frac {\partial C}{\partial z^l_j} \frac {\partial z^l_j}{\partial w^l_{jk}} \\
& = & \frac {\partial z^l_j}{\partial w^l_{jk}}{\delta^l_j}
\nonumber\end{eqnarray}
{% endraw %}
我们展开$z^l_j$有
{% raw %}
\begin{eqnarray}
z^l_j = \sum_m w^l_{jm}a^{l-1}_m + b^l_j
\nonumber\end{eqnarray}
{% endraw %}
只有当$m = k$时{% raw %}$z^l_j${% endraw %}才和{% raw %}$w^l_{jk}${% endraw %}有关联，因此求偏导我们可以得到
{% raw %}
\begin{eqnarray}
\frac {\partial C}{\partial w^l_{jk}} = a^{l-1}_k \delta^l_j
\nonumber\end{eqnarray}
{% endraw %}

## 反向传播算法
反向传播的方程给我们提供了一种计算代价函数梯度的方法。让我们精确地描述一下该算法：

1. **Input $x$:**Set the corresponding activation $a^1$ for the input layer.
2. **Feedforward:** For each $l=2,3,\ldots,L$ compute $z^{l} = w^l a^{l-1}+b^l$ and $a^{l} = \sigma(z^{l})$.
3. **Output error $\delta^L$:** Compute the vector $\delta^{L} = \nabla_a C \odot \sigma'(z^L)$.
4. **Backpropagate the error:**For each $l = L-1, L-2,\ldots, 2$ compute $\delta^{l} = ((w^{l+1})^T \delta^{l+1}) \odot \sigma'(z^{l})$.
5. **Output:** The gradient of the cost function is given by $\frac{\partial C}{\partial w^l_{jk}} = a^{l-1}_k \delta^l_j$ and $\frac{\partial C}{\partial b^l_j} = \delta^l_j$  

通过这个算法你就看出为什么它被称之为反向传播算法(*backpropagation*)。我们从最后一层开始向后迭代计算误差向量$\delta^l$。为了理解代价函数是如何随着前面的权重和误差改变的，我们需要反复利用利用链式法则来得到我们需要的表达式。

正如我所描述的那样，反向传播算法计算了单个训练样本的代价函数的梯度$C=C_x$。实际上，我们通常结合一种学习算法例如梯度下降法来计算多个训练样本的梯度。给定一个*mini-batch*大小为$m$的训练样本，我们应用基于*mini-batch*的梯度下降法：

1. **Input a set of training examples**
2. **For each training example$x$:** Set the corresponding input activation $a^{x,1}$,and perform the following steps:
+ **Feedforward:**For each $l=2,3,\ldots,L$ compute $z^{x,l} = w^l a^{x,l-1}+b^l$ and $a^{x,l} = \sigma(z^{x,l})$.
+ **Output error $\delta^{x,L}$:** Compute the vector $\delta^{x,L} = \nabla_a C_x \odot \sigma'(z^{x,L})$.
+ **Backpropagate the error:**For each $l = L-1, L-2,\ldots, 2$ compute $\delta^{x,l} = ((w^{l+1})^T \delta^{x,l+1})\odot \sigma'(z^{x,l})$.
1. **Gradient descent:**For each $l = L, L-1, \ldots, 2$ update the weights according to the rule $w^l \rightarrow w^l-\frac{\eta}{m} \sum_x \delta^{x,l} (a^{x,l-1})^T$ ,and the biases according to the rule $b^l \rightarrow l-\frac{\eta}{m}
   \sum_x \delta^{x,l}$.  

当然，在实际应用中，你需要循环的划分*mini-batch*大小的训练样本，我为了简洁忽略了这些东西。

## 反向传播的代码

在抽象地理解了反向传播算法之后，我们现在能够理解上一章应用反向传播的[代码](https://github.com/mnielsen/neural-networks-and-deep-learning)了。这些代码是上面算法的直接翻译。特别的，`update_mini_batch`方法更新了一个*mini-batch*大小训练样本的权重和偏置：

```python
class Network(object):
...
    def update_mini_batch(self, mini_batch, eta):
        """Update the network's weights and biases by applying
        gradient descent using backpropagation to a single mini batch.
        The "mini_batch" is a list of tuples "(x, y)", and "eta"
        is the learning rate."""
        nabla_b = [np.zeros(b.shape) for b in self.biases]
        nabla_w = [np.zeros(w.shape) for w in self.weights]
        for x, y in mini_batch:
            delta_nabla_b, delta_nabla_w = self.backprop(x, y)
            nabla_b = [nb+dnb for nb, dnb in zip(nabla_b, delta_nabla_b)]
            nabla_w = [nw+dnw for nw, dnw in zip(nabla_w, delta_nabla_w)]
        self.weights = [w-(eta/len(mini_batch))*nw 
                        for w, nw in zip(self.weights, nabla_w)]
        self.biases = [b-(eta/len(mini_batch))*nb 
                       for b, nb in zip(self.biases, nabla_b)]

```
大部分的工作是由`delta_nabla_b, delta_nabla_w = self.backprop(x, y)`这一行代码完成的，这行代码利用`backprop`方法计算了偏导$\partial C_x / \partial b^l_j$和偏导$\partial C_x / \partial w^l_{jk}$。`backprop`的实现如下：

```python
class Network(object):
...
   def backprop(self, x, y):
        """Return a tuple "(nabla_b, nabla_w)" representing the
        gradient for the cost function C_x.  "nabla_b" and
        "nabla_w" are layer-by-layer lists of numpy arrays, similar
        to "self.biases" and "self.weights"."""
        nabla_b = [np.zeros(b.shape) for b in self.biases]
        nabla_w = [np.zeros(w.shape) for w in self.weights]
        # feedforward
        activation = x
        activations = [x] # list to store all the activations, layer by layer
        zs = [] # list to store all the z vectors, layer by layer
        for b, w in zip(self.biases, self.weights):
            z = np.dot(w, activation)+b
            zs.append(z)
            activation = sigmoid(z)
            activations.append(activation)
        # backward pass
        delta = self.cost_derivative(activations[-1], y) * \
            sigmoid_prime(zs[-1])
        nabla_b[-1] = delta
        nabla_w[-1] = np.dot(delta, activations[-2].transpose())
        # Note that the variable l in the loop below is used a little
        # differently to the notation in Chapter 2 of the book.  Here,
        # l = 1 means the last layer of neurons, l = 2 is the
        # second-last layer, and so on.  It's a renumbering of the
        # scheme in the book, used here to take advantage of the fact
        # that Python can use negative indices in lists.
        for l in xrange(2, self.num_layers):
            z = zs[-l]
            sp = sigmoid_prime(z)
            delta = np.dot(self.weights[-l+1].transpose(), delta) * sp
            nabla_b[-l] = delta
            nabla_w[-l] = np.dot(delta, activations[-l-1].transpose())
        return (nabla_b, nabla_w)

...

    def cost_derivative(self, output_activations, y):
        """Return the vector of partial derivatives \partial C_x /
        \partial a for the output activations."""
        return (output_activations-y) 

def sigmoid(z):
    """The sigmoid function."""
    return 1.0/(1.0+np.exp(-z))

def sigmoid_prime(z):
    """Derivative of the sigmoid function."""
    return sigmoid(z)*(1-sigmoid(z))

```
## 为何说反向传播是一种快速的算法？
为什么说反向传播是一种快速的算法呢？为了回答这个问题，我们来考虑一下另一种计算梯度的方法。想象一下这是神经网络研究的早期，大概是20世纪50年代或60年代，你是世界上第一个想出用梯度下降法学习神经网络的人！但是为了让这个想法行得通，你需要一种方法来计算代价函数的梯度。你回顾了一下你的算术的知识，决定利用链式法则来计算梯度。当你尝试了之后你会发现代数式会变得极其复杂，这会让你很沮丧。因此你努力寻找另一种方法。你决定单独考虑仅仅以$w$作为自变量时$C=C(w)$(之后我们再考虑仅仅以偏置为自变量)。首先你给权重编号$w_1, w_2, \ldots$，然后对每个权重$w_j$计算$\partial C/\partial w_j$。我们可以利用下面的近似来计算偏导

\begin{eqnarray}  \frac{\partial
    C}{\partial w_{j}} \approx \frac{C(w+\epsilon
    e_j)-C(w)}{\epsilon},
\tag{46}\end{eqnarray}

这里$\epsilon > 0$是个很小的正数，$e_j$是在$j^{th}$方向上的单位向量。换句话说，我们可以通过计算两个不同的$w_j$来计算$C$的偏导。同样的想法也能用于计算代价函数关于偏置的偏导。  

这个方法看起来很诱人。它在概念上比较简单，而且很容易实现，只用几行代码就能完成。它看起来比利用链式法则计算梯度更有前景。

不幸的是，当你实现它之后你会发现速度极其慢。为了理解为什么，我们假设网络中有一百万个权重。对于每个不同的权重$w_j$我们都需要计算$C(w+\epsilon e_j)$来获得$\partial C / \partial w_j$。这意味着我们对于每个训练样本都要一百万次计算。

反向传播比较聪明的一点就是能够让我们在一次前馈和一次后馈就能一次性地计算所有的偏导$\partial C/ \partial w_j$。虽然反向传播看起来公式更复杂一些，但是它的确快了非常多！

这种加速在1986年被首次全面认可，它极大地扩大了神经网络所能解决的问题。这也导致了很多人都开始使用神经网络。当然，反向传播算法也不是灵丹妙药。即使在1980年的末期，人们也发现了它在解决深层网络时会出现问题。后面我们会看到现代计算机和一些聪明的想法是如何让反向传播算法训练深层网络成为可能。
<!DOCTYPE html>



  


<html class="theme-next muse use-motion" lang="zh-Hans">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Hexo, NexT" />





  <link rel="alternate" href="/atom.xml" title="Huaipeng Zhao" type="application/atom+xml" />






<meta property="og:type" content="website">
<meta property="og:title" content="Huaipeng Zhao">
<meta property="og:url" content="http://yoursite.com/index.html">
<meta property="og:site_name" content="Huaipeng Zhao">
<meta property="og:locale" content="zh-Hans">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Huaipeng Zhao">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Muse',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/"/>





  <title>Huaipeng Zhao</title>
  




<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
            (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
          m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
  ga('create', 'UA-75595176-1', 'auto');
  ga('send', 'pageview');
</script>





</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <div class="container sidebar-position-left 
  page-home">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Huaipeng Zhao</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2017/03/29/机器学习中的正则化/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Huaipeng Zhao">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Huaipeng Zhao">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2017/03/29/机器学习中的正则化/" itemprop="url">过拟合与正则</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2017-03-29T13:43:10+08:00">
                2017-03-29
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Machine-Learning/" itemprop="url" rel="index">
                    <span itemprop="name">Machine Learning</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>之前对于正则的理解都行于一些肤浅的理解，没有深入去想其中的一些细节。这篇文章也不会涉及比较深入的数学证明，旨在能够给一个明确的形象化的解释。有什么地方不正确的请指正。</p>
<h1 id="过拟合"><a href="#过拟合" class="headerlink" title="过拟合"></a>过拟合</h1><p>在机器学习中，过拟合现象是普遍存在的。所谓过拟合就是模型过分拟合了每个数据点，导致模型变得比较复杂，在测试数据集上的泛化性较差。下面的这张图会更能直观感受：  </p>
<image src="/images/over_fitting.png" width="100%" height="100%">   



<p>防止过拟合的方式有很多，一般来说可以加正则，<em>early-stopping，dropout</em>等。加正则可能是最广泛的一种方式，更加准确一点说，一般加的是<strong>L2正则</strong>，而<strong>L0</strong>和<strong>L1</strong>正则严格来说并是不用来防止过拟合的，下面会简单介绍。当然正则的手段比较多，还有人会加KL散度作为正则，这里并不加以扩展。  </p>
<h1 id="常见正则"><a href="#常见正则" class="headerlink" title="常见正则"></a>常见正则</h1><h2 id="L0正则与L1正则"><a href="#L0正则与L1正则" class="headerlink" title="L0正则与L1正则"></a>L0正则与L1正则</h2><p><strong>L0正则</strong>的定义是<strong>求向量向量中非0元素的个数</strong>。这是一种比较强的条件。那么这么做的目的是什么呢？我们知道，如果模型能够帮我们筛选哪些特征是重要的，哪些特征是可以忽略的，那将是我们想要的结果。而L0正则的优化目的就是让向量中的更多参数趋近于0，这样可以保留重要的参数，不重要的对应的权重设置为0。所以说L0正则主要用来做<strong>特征选择</strong>. <strong>L1正则</strong>的定义是<strong>求向量的各个元素的绝对值之和</strong>。这个也可以实现<strong>稀疏</strong>.这个为什么会使权值稀疏呢？数学上的解释是：<strong>任何规则化算子，如果在W = 0的地方不可微，并且可以分解为“求和”形式，那么这个规则化算子就能实现稀疏。</strong>关于这个描述的证明大家可以去查阅一些相关资料，这里并不做严格证明，下一节和<strong>L2正则</strong>对比的时候会给一个图像解释。那么大家为什么选择L1正则而很少用L0呢？因为L0的求解是个NP问题，而L1相对更好求一点，关于L1正则的求导大家可以查阅<strong>次梯度</strong>的相关资料。</p>
<h2 id="L2正则"><a href="#L2正则" class="headerlink" title="L2正则"></a>L2正则</h2><p><strong>L2正则</strong>是求各个参数的平方和。用在我们的损失函数中的定义如下：  </p>
<image src="/images/L2_1.jpg" width="30%" height="30%">   

<p>那么，我们在用梯度下降更新参数时候L2正则做了什么呢？我们看权重的更新公式：  </p>
<image src="/images/L2_2.jpg" width="40%" height="40%">  

<p>我们从权重更新公式可以看出，L2正则起到的作用就是使得权重<strong>趋于变小</strong>。那么变小为什么能够减缓过拟合现象呢。<strong>之前的理解是如果很多参数变小会近似为0，这样模型的参数就会变少，也就是说模型的规模会变小。但是，这种理解是错误的。</strong> L1正则和L2正则可以<strong>等价于</strong>下面的优化问题（利用拉格朗日定理）：  </p>
<image src="/images/L2_3.png" width="30%" height="30%">    

<p>那么我们就可以据此得到L1正则和L2正则的图像化表示：  </p>
<image src="/images/L2_4.png" width="50%" height="50%">    


<p>因此我们可以形象化看出L1正则的效果是产生更少的参数，其他的参数为0（因为最优解在会与坐标轴相交）。所以我们会说L1正则能产生稀疏特征，这种特性也能很好用于特征选择。而L2正则起到的作用是产生更多参数，而参数趋向于变得比较小。那么核心问题就是<strong>为什么参数小会减缓过拟合？</strong> 实际上过拟合的一个重要原因是我们的数据是有噪声的，而如果我们的模型学习过度而模拟这个点的时候就会使得这个数据特征的权重变大，这样的风险就会很大。从另一个角度来看，如果从最大似然角度来讲，假设我们的训练数据极大，而噪声点只出现过一次，那么在最大似然策略优化的时候会让噪声点表示的特征的权重变得<strong>无穷大</strong>。从这两个角度来讲，权重大是有很大风险的。因此我们想通过L2来进行“权值衰减”，从而降低每个分量带来的风险。 </p>
<p><strong>总结：L0和L1会产更少特征，用于特征选择。L2正则会产生更多特征，而权重会趋向变小。</strong></p>
<h1 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h1><ul>
<li><a href="http://blog.csdn.net/zouxy09/article/details/24971995" target="_blank" rel="noopener">机器学习中的范数规则化之（一）L0、L1与L2范数</a></li>
<li><a href="http://blog.csdn.net/u012162613/article/details/44261657" target="_blank" rel="noopener">正则化方法：L1和L2 regularization、数据集扩增、dropout</a></li>
<li><a href="https://www.amazon.cn/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95-%E6%9D%8E%E8%88%AA/dp/B007TSFMTA" target="_blank" rel="noopener">《统计机器学习》(李航)</a></li>
</ul>
</image></image></image></image></image>
          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2017/02/25/git远程仓库操作简单教程/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Huaipeng Zhao">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Huaipeng Zhao">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2017/02/25/git远程仓库操作简单教程/" itemprop="url">Git远程仓库操作简单教程</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2017-02-25T17:17:31+08:00">
                2017-02-25
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Git/" itemprop="url" rel="index">
                    <span itemprop="name">Git</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="准备工作"><a href="#准备工作" class="headerlink" title="准备工作"></a>准备工作</h1><p>本教程只涉及远程仓库的一些基本操作，不涉及更细节的问题。<br>首先在Github上创建了一个<code>git_usage</code>的项目。然后创建了两个分支，一个分支是<em>master</em>分支，该分支下有一个名为<code>m1.txt</code>的文件，另一个分支是<em>b1</em>分支，该分支目录下有一个名为<code>b1.txt</code>的文件。</p>
<h1 id="从远程仓库到本地仓库"><a href="#从远程仓库到本地仓库" class="headerlink" title="从远程仓库到本地仓库"></a>从远程仓库到本地仓库</h1><h2 id="克隆项目-git-clone"><a href="#克隆项目-git-clone" class="headerlink" title="克隆项目: git clone"></a>克隆项目: git clone</h2><p>首先是从远程服务器克隆项目。默认的远程主机是<code>origin</code>。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git clone git@github.com:hpzhao/git_usage.git</span><br></pre></td></tr></table></figure>
<h2 id="查看分支-git-branch"><a href="#查看分支-git-branch" class="headerlink" title="查看分支: git branch"></a>查看分支: git branch</h2><p>接下来先查看下本地分支:  </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git branch</span><br></pre></td></tr></table></figure>
<p>我们可以得到如下图的结果：</p>
<image src="/images/git_branch.png" width="40%" height="20%">

<p>我们看到只有一个master分支。因为git clone默认只会拷贝master分支的数据到本地，所以本地不会有其他分支，但是会有远程分支的索引。接下来我们利用<code>-a</code>参数来查看所有的分支：  </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git branch -a</span><br></pre></td></tr></table></figure>
<p>结果如下所示：  </p>
<image src="/images/git_branch-a.png" width="50%" height="40%">

<p>这里我们看到了前缀有<code>remote</code>的分支。这就是未同步到本地的分支。下面会讲解如何将远程分支同步到本地。</p>
<h2 id="从远程仓库获取分支：git-checkout"><a href="#从远程仓库获取分支：git-checkout" class="headerlink" title="从远程仓库获取分支：git checkout"></a>从远程仓库获取分支：git checkout</h2><p>我们需要用<code>git checkout</code>来建立新的分支并获取相应远程分支的数据。 </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git checkout --track origin/b1</span><br></pre></td></tr></table></figure>
<p>得到的结果如下图：</p>
<image src="/images/git_checkout-track.png" width="60%" height="60%">

<p>这样我们就得到远程分支<code>b1</code>。<strong>注意</strong>：<code>--track</code>是git 2.6版本之后的参数，如果是旧版本的git，需要自己命名本地分支：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git checkout b1 origin/b1</span><br></pre></td></tr></table></figure>
<h2 id="同步远程分支最新变动-git-fetch和git-pull"><a href="#同步远程分支最新变动-git-fetch和git-pull" class="headerlink" title="同步远程分支最新变动: git fetch和git pull"></a>同步远程分支最新变动: git fetch和git pull</h2><p>如果远程服务器的内容变更了之后如何得到最新的变动呢？常用的命令有<code>git pull</code>和<code>git fetch</code>两种。我们在远程服务器的<code>master</code>分支下添加<code>m2.txt</code>文件。那么我们首先用<code>git fetch</code>来演示效果：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git fetch</span><br></pre></td></tr></table></figure>
<p>我们可以得到如图的结果：</p>
<image src="/images/git_fetch.png" width="60%" height="60%">

<p>然后查看本地<code>master</code>分支下的文件，发现并没有新增文件。这也就是说<code>git fetch</code>文件并没有真正获取远程仓库的数据，这条命令只是在更新远程分支的索引，包括commit id。而<code>git pull</code>会直接将数据同步到本地，并且会进行<code>merge</code>操作。所以一般来说用<code>git fetch</code>更加安全，因为可以手工地决定合并哪些内容。</p>
<h1 id="从本地仓库到远程仓库"><a href="#从本地仓库到远程仓库" class="headerlink" title="从本地仓库到远程仓库"></a>从本地仓库到远程仓库</h1><h2 id="创建本地分支"><a href="#创建本地分支" class="headerlink" title="创建本地分支"></a>创建本地分支</h2><p>我们首先创建一个远程仓库没有的本地分支<code>b2</code>。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git checkout -b b2</span><br></pre></td></tr></table></figure>
<h2 id="将本地分支推送到远程服务器"><a href="#将本地分支推送到远程服务器" class="headerlink" title="将本地分支推送到远程服务器"></a>将本地分支推送到远程服务器</h2><p>我们将<code>b2</code>分支推送到远程服务器。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git push origin b2:b2</span><br></pre></td></tr></table></figure>
<h2 id="将本地分支与远程分支推送关联"><a href="#将本地分支与远程分支推送关联" class="headerlink" title="将本地分支与远程分支推送关联"></a>将本地分支与远程分支推送关联</h2><p>就是你当前分支执行<code>git push</code>默认的关联的远程分支。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git push --set-upstream origin hpzhao_resume</span><br></pre></td></tr></table></figure>
<h1 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h1><ul>
<li><a href="https://git-scm.com/book/zh/v1/Git-%E5%88%86%E6%94%AF-%E8%BF%9C%E7%A8%8B%E5%88%86%E6%94%AF" target="_blank" rel="noopener">Git分支-远程分支</a></li>
<li><a href="http://www.huxiusong.com/?p=365" target="_blank" rel="noopener">git pull和git fetch区别</a></li>
</ul>
</image></image></image></image>
          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2016/10/29/tensorflow中设置GPU/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Huaipeng Zhao">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Huaipeng Zhao">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2016/10/29/tensorflow中设置GPU/" itemprop="url">TensorFlow中设置GPU</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2016-10-29T10:26:36+08:00">
                2016-10-29
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/TensorFlow/" itemprop="url" rel="index">
                    <span itemprop="name">TensorFlow</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="设置GPU个数"><a href="#设置GPU个数" class="headerlink" title="设置GPU个数"></a>设置GPU个数</h1><h2 id="设置单个GPU或者CPU"><a href="#设置单个GPU或者CPU" class="headerlink" title="设置单个GPU或者CPU"></a>设置单个GPU或者CPU</h2><p>如果没有事先指定GPU或者CPU，那么TensorFlow默认的机制是寻找在机器中找到的第一个GPU，<strong>并且占有该GPU所有的显存资源</strong>。如果没有找到GPU那么默认是利用第一个CPU来做运算。首先说一下TensorFlow对于设备的表示：”/cpu:0”表示第一块CPU，”/gpu:0”表示第一块GPU，其他情况以此类推。我们可以用<em>nvidia-smi</em>来查看设备GPU信息。如下图所示：  </p>
<p><img src="/images/nvidia-smi.png" width="90%" height="80%"></p>
<p>我们就用TensorFlow官网的例子作为示例：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Creates a graph.</span></span><br><span class="line"><span class="keyword">with</span> tf.device(<span class="string">'/gpu:2'</span>):</span><br><span class="line">  a = tf.constant([<span class="number">1.0</span>, <span class="number">2.0</span>, <span class="number">3.0</span>, <span class="number">4.0</span>, <span class="number">5.0</span>, <span class="number">6.0</span>], shape=[<span class="number">2</span>, <span class="number">3</span>], name=<span class="string">'a'</span>)</span><br><span class="line">  b = tf.constant([<span class="number">1.0</span>, <span class="number">2.0</span>, <span class="number">3.0</span>, <span class="number">4.0</span>, <span class="number">5.0</span>, <span class="number">6.0</span>], shape=[<span class="number">3</span>, <span class="number">2</span>], name=<span class="string">'b'</span>)</span><br><span class="line">  c = tf.matmul(a, b)</span><br><span class="line"><span class="comment"># Creates a session with log_device_placement set to True.</span></span><br><span class="line">sess = tf.Session(config=tf.ConfigProto(log_device_placement=<span class="keyword">True</span>))</span><br><span class="line"><span class="comment"># Runs the op.</span></span><br><span class="line"><span class="keyword">print</span> sess.run(c)</span><br></pre></td></tr></table></figure>
<p>还有一个方法使程序强制使用具体的设备。这是CUDA的参数设置：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">CUDA_VISIBLE_DEVICES=<span class="string">"1"</span> python tf.py</span><br></pre></td></tr></table></figure>
<h2 id="使用多GPU"><a href="#使用多GPU" class="headerlink" title="使用多GPU"></a>使用多GPU</h2><p>上面演示了我们如何为一些操作指定固定的GPU或者CPU。但是如果我们有多个GPU可以使用，而且想为一些运算指定多块GPU同时承担运算任务时，我们就可以用以下的方式来完成。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Creates a graph.</span></span><br><span class="line">c = []</span><br><span class="line"><span class="keyword">for</span> d <span class="keyword">in</span> [<span class="string">'/gpu:2'</span>, <span class="string">'/gpu:3'</span>]:</span><br><span class="line">  <span class="keyword">with</span> tf.device(d):</span><br><span class="line">    a = tf.constant([<span class="number">1.0</span>, <span class="number">2.0</span>, <span class="number">3.0</span>, <span class="number">4.0</span>, <span class="number">5.0</span>, <span class="number">6.0</span>], shape=[<span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line">    b = tf.constant([<span class="number">1.0</span>, <span class="number">2.0</span>, <span class="number">3.0</span>, <span class="number">4.0</span>, <span class="number">5.0</span>, <span class="number">6.0</span>], shape=[<span class="number">3</span>, <span class="number">2</span>])</span><br><span class="line">    c.append(tf.matmul(a, b))</span><br><span class="line"><span class="keyword">with</span> tf.device(<span class="string">'/cpu:0'</span>):</span><br><span class="line">  sum = tf.add_n(c)</span><br><span class="line"><span class="comment"># Creates a session with log_device_placement set to True.</span></span><br><span class="line">sess = tf.Session(config=tf.ConfigProto(log_device_placement=<span class="keyword">True</span>))</span><br><span class="line"><span class="comment"># Runs the op.</span></span><br><span class="line"><span class="keyword">print</span> sess.run(sum)</span><br></pre></td></tr></table></figure>
<h1 id="设置GPU资源分配"><a href="#设置GPU资源分配" class="headerlink" title="设置GPU资源分配"></a>设置GPU资源分配</h1><p>前面说过TF会默认抓取当前使用GPU的所有显存。官网上解释说这样设计初衷就是为了提高计算效率。但是我们工作在多人使用的服务器上时就应该有所约束。官网提供了两种方式。第一种方式就是让TensorFlow在运行过程中动态申请显存，需要多少就申请多少。第二种方式就是限制GPU的使用率。<br>第一种方式如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">config = tf.ConfigProto()</span><br><span class="line">config.gpu_options.allow_growth = <span class="keyword">True</span></span><br><span class="line">session = tf.Session(config=config, ...)</span><br></pre></td></tr></table></figure>
<p>第二种方式如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">config = tf.ConfigProto()</span><br><span class="line">config.gpu_options.per_process_gpu_memory_fraction = <span class="number">0.4</span></span><br><span class="line">session = tf.Session(config=config, ...)</span><br></pre></td></tr></table></figure>
<h1 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h1><ul>
<li><a href="https://www.tensorflow.org/how_tos/using_gpu/" target="_blank" rel="noopener">Using GPUs</a></li>
</ul>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2016/08/11/python数据持久化模块Pickle/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Huaipeng Zhao">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Huaipeng Zhao">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2016/08/11/python数据持久化模块Pickle/" itemprop="url">Python数据持久化模块Pickle</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2016-08-11T10:50:30+08:00">
                2016-08-11
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Python/" itemprop="url" rel="index">
                    <span itemprop="name">Python</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>一般在程序的运行中会产生一些我们想要的中间结果，保留这些中间结果能够让我们节省计算或者为其他模块服务。因此，保存程序运行中的一些数据结构是一个常见的需求。当然，我们可以自己定义一些格式来存储这些数据结构，当我们需要恢复数据时候重新构造这些数据结构，然后把数据逐条加进去。但是，显然这不是一种优雅的方式，尤其是对象比较复杂或者比较大时。python给我们提供了一个方便的序列化和反序列化的模块——Pickle。  </p>
<p>pickle是python的内置库，它还有一个用C实现的版本cPickle，C版本速度上会快很多，因此我们一般会选择使用cPickle。cPickle把数据序列化的函数为：  </p>
<p><code>cPickle.dump(obj, file[, protocol])</code>  </p>
<p>这行代码就把我们的对象<code>obj</code>保存到文件file中。其中<code>obj</code>是对象名，<code>file</code>则是文件流，<code>protocol</code>是文件的读写方式。如果为了压缩数据protocol可以加上<code>r</code>。下面是一个简单的用法示例：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#!/usr/bin/env python</span></span><br><span class="line"><span class="comment">#coding:utf8</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> cPickle <span class="keyword">as</span> pkl</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__==<span class="string">'__main__'</span>:</span><br><span class="line"></span><br><span class="line">    source_data = [str(i) <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1000</span>)]</span><br><span class="line"></span><br><span class="line">    pkl.dump(source_data,open(<span class="string">'data.dat'</span>,<span class="string">'wb'</span>))</span><br><span class="line"></span><br><span class="line">    recover_data = pkl.load(open(<span class="string">'data.dat'</span>,<span class="string">'rb'</span>))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">print</span> <span class="string">' '</span>.join(recover_data)</span><br></pre></td></tr></table></figure>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2016/07/24/python模块与包/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Huaipeng Zhao">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Huaipeng Zhao">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2016/07/24/python模块与包/" itemprop="url">Python模块与包</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2016-07-24T15:52:49+08:00">
                2016-07-24
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Python/" itemprop="url" rel="index">
                    <span itemprop="name">Python</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="包和模块概述"><a href="#包和模块概述" class="headerlink" title="包和模块概述"></a>包和模块概述</h1><h2 id="什么是包和模块？"><a href="#什么是包和模块？" class="headerlink" title="什么是包和模块？"></a>什么是包和模块？</h2><p>从文件系统的角度而言，一个包就是一个文件夹，而一个模块就是一个文件。一个包可以包含多个模块，而一个模块里面包含了类，函数，变量。包和模块就像其他语言的库一样，目的是提供一个可复用性，低耦合，具备特定功能的代码。python中的包和模块的导入机制是整个语言中最复杂的部分，我们在本文中只探讨包和模块的管理和使用，具体的机制可以阅读<a href="https://www.python.org/dev/peps/pep-0302/" target="_blank" rel="noopener">PEP 302</a>。我们可以使用发行的包，也可以使用自己定义的包。在自己定义包时，每个目录下要含有一个<code>__init__.py</code>文件，可以保持这个文件为空，也可以在里面自定义导入规则。</p>
<h2 id="模块的属性"><a href="#模块的属性" class="headerlink" title="模块的属性"></a>模块的属性</h2><p>模块的属性是模块的一些信息参数。</p>
<h3 id="固有属性"><a href="#固有属性" class="headerlink" title="固有属性"></a>固有属性</h3><p>固有属性是一个模块被创建就会自带的默认属性。我们以下面的例子来说明：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">$ touch mod.py</span><br><span class="line">$ python</span><br><span class="line">&gt;&gt;&gt; import mod <span class="comment"># 导入模块mod</span></span><br><span class="line">&gt;&gt;&gt; mod.__dict__ <span class="comment"># 模块mod的属性全貌</span></span><br><span class="line">&#123;<span class="string">'__builtins__'</span>: &#123;...&#125;, <span class="string">'__name__'</span>: <span class="string">'mod'</span>, <span class="string">'__file__'</span>: <span class="string">'mod.pyc'</span>, <span class="string">'__doc__'</span>: None, <span class="string">'__package__'</span>: None&#125; </span><br><span class="line">&gt;&gt;&gt; dir(mod) <span class="comment"># 只查看属性名</span></span><br><span class="line">[<span class="string">'__builtins__'</span>, <span class="string">'__doc__'</span>, <span class="string">'__file__'</span>, <span class="string">'__name__'</span>, <span class="string">'__package__'</span>]</span><br></pre></td></tr></table></figure>
<ul>
<li><code>__builtins__</code>内建名字空间（参考 名字空间）</li>
<li><code>__file__</code>文件名（对于被导入的模块，文件名为绝对路径格式；对于直接执行的模块，文件名为相对路径格式）</li>
<li><code>__name__</code>模块名（对于被导入的模块，模块名为去掉“路径前缀”和“.pyc后缀”后的文件名，即<code>os.path.splitext(os.path.basename(__file__))[0]</code>；对于直接执行的模块，模块名为<code>__main__</code>）</li>
<li><code>__doc__</code>文档字符串（即模块中在所有语句之前第一个未赋值的字符串）</li>
<li><code>__package__</code>包名（主要用于相对导入）<h3 id="新增属性"><a href="#新增属性" class="headerlink" title="新增属性"></a>新增属性</h3>新增属性是指我们自己创建的属性，其中包含：</li>
<li><code>import</code>导入的模块</li>
<li>变量</li>
<li>函数</li>
<li>类<h3 id="导出属性"><a href="#导出属性" class="headerlink" title="导出属性"></a>导出属性</h3>共有属性指的是我们在导入模块时会导入的属性。这是有一个变量<code>__all__</code>来管理的。如果没有这个变量，那么默认导入所有共有属性(即不以下划线开头的)。我们也可以自己定义要导出哪些属性。<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">__all__ = [...]</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h1 id="包和模块的管理"><a href="#包和模块的管理" class="headerlink" title="包和模块的管理"></a>包和模块的管理</h1><p>首先我们需要了解python中的包都在什么位置呢？这分为以下几种情况：</p>
<ul>
<li>sudo apt-get install 安装的package存放在<code>/usr/lib/python2.7/dist-packages</code>目录中</li>
<li>pip 或者 easy_install默认安装的package存放在<code>/usr/local/lib/python2.7/dist-packages</code>目录中</li>
<li>手动从源代码或者<code>pip install --user</code>安装的package存放在<code>./local/lib/python2.7/site-packages</code>目录中</li>
</ul>
<p>一般来说，自己手动安装的python包的优先级比系统安装的优先级高，这就意味着如果一个包安装了而不同版本，那么优先手动安装的版本。如果从源码安装包，那么安装的位置也要选择<code>./local/lib/python2.7/site-packages</code>中便于包的管理。当然你也可以安装到你喜欢的位置，但是要把包的路径添加到python的库搜索路径中，方法会在下面提到。</p>
<h1 id="包和模块的导入"><a href="#包和模块的导入" class="headerlink" title="包和模块的导入"></a>包和模块的导入</h1><h2 id="包和模块的导入风格"><a href="#包和模块的导入风格" class="headerlink" title="包和模块的导入风格"></a>包和模块的导入风格</h2><p>python提供了丰富的包的导入方式，但是有一些导入方式并不值得推荐。例如<code>from math import *</code>，这种全盘导入的风格我们是不推荐的，因为在math中可能会定义和你文件中同名的变量，函数，类等，而且这种导入会使得加载更多的模块，从而降低了效率。养成良好统一的编码风格非常重要，虽然语言提供了很多奇技淫巧，但是我们要<strong>选择那些好的语言特性，而不是糟糕的特性</strong>，下面是Google推荐的python代码风格：</p>
<ul>
<li>使用<code>import x</code>来导入包和模块</li>
<li>使用<code>from x import y</code>来导入模块.其中x是包名，y是不含包名的模块名</li>
<li>使用<code>from x import y as z</code>, 如果两个要导入的模块都叫做y或者y太长了</li>
<li>使用<code>from x.y import z</code>,导入模块z要用包的全路径，不要嵌套，import之后的要保证只是模块，不含包名</li>
</ul>
<h2 id="通过相对路径导入包和模块"><a href="#通过相对路径导入包和模块" class="headerlink" title="通过相对路径导入包和模块"></a>通过相对路径导入包和模块</h2><p>使用相对路径导入模块：</p>
<ul>
<li>要导入的模块位于同级目录，我们可以直接导入包<code>import x</code></li>
<li>要导入的模块位于同级目录下的子目录，我们可以<code>from A import x</code>或<code>from .A import x</code></li>
<li>要导入的模块位于上级目录，我们可以<code>from ..A import x</code></li>
<li>要导入的模块位于其他情况时，我们不推荐使用相对目录导入，而是采用即将介绍的方式导入</li>
</ul>
<h2 id="通过sys-path导入包"><a href="#通过sys-path导入包" class="headerlink" title="通过sys.path导入包"></a>通过sys.path导入包</h2><p>当我们想利用的包在工程的其他目录时，我们需要将该路径添加到系统的搜索路径中，有两种方式：<br>第一种方式是将包的绝对路径添加到系统的搜索路径中，只需调用<code>sys.path.insert(pos,dir)</code>或者<code>sys.path.append(dir)</code>,这种方式添加是硬链接到你的代码，只在程序运行时有效，但是这种方式在代码部署时需要手工改动代码中的所有路径，不推荐这种方式。</p>
<p>第二种方式就是将该路径添加到python的默认搜索路径中，有两种方式可以实现。</p>
<ul>
<li>在系统变量<code>PYTHONPATH</code>中添加该路径</li>
<li>在<code>.local/lib/pyhon2.7/site-packages/</code>下的.pth下添加该目录</li>
</ul>
<p>我们推荐第二种方式，因为这样在代码部署时我们只需要改动路径的配置文件即可。</p>
<h2 id="通过字符串导入包"><a href="#通过字符串导入包" class="headerlink" title="通过字符串导入包"></a>通过字符串导入包</h2><p>如果包名在字符串里面，我们可以采用importlib.import_module()函数来导入包，如下例所示：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> importlib</span><br><span class="line">math = importlib.import_module(<span class="string">'math'</span>)</span><br><span class="line"><span class="keyword">print</span> math.sin(<span class="number">2</span>)</span><br></pre></td></tr></table></figure>
<h2 id="模块的重新加载"><a href="#模块的重新加载" class="headerlink" title="模块的重新加载"></a>模块的重新加载</h2><p>模块在第一次被加载时被编译，但如果你修改了模块的代码，那么python默认不会重新编译模块，因为这是个代价较大的操作。我们可以在调试模式下用<code>reload()</code>函数重新加载模块，但是在工作环境下不要这样做，仅限于调试模式。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> imp</span><br><span class="line">imp.reload(spam)</span><br></pre></td></tr></table></figure></p>
<h1 id="包的分发"><a href="#包的分发" class="headerlink" title="包的分发"></a>包的分发</h1><p>现在你已经写好自己的包了，你想分享给别人用，那么下面就是如何把包进行封装。加入你有如下的工程目录：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">projectname/</span><br><span class="line">    README.txt</span><br><span class="line">    Doc/</span><br><span class="line">        documentation.txt</span><br><span class="line">    projectname/</span><br><span class="line">        __init__.py</span><br><span class="line">        foo.py</span><br><span class="line">        bar.py</span><br><span class="line">        utils/</span><br><span class="line">            __init__.py</span><br><span class="line">            spam.py</span><br><span class="line">            grok.py</span><br><span class="line">    examples/</span><br><span class="line">        helloworld.py</span><br><span class="line">        ...</span><br></pre></td></tr></table></figure>
<p>首先你要编写一个<code>setup.py</code>，类似下面这样：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># setup.py</span></span><br><span class="line"><span class="keyword">from</span> distutils.core <span class="keyword">import</span> setup</span><br><span class="line"></span><br><span class="line">setup(name=<span class="string">'projectname'</span>,</span><br><span class="line">    version=<span class="string">'1.0'</span>,</span><br><span class="line">    author=<span class="string">'Your Name'</span>,</span><br><span class="line">    author_email=<span class="string">'you@youraddress.com'</span>,</span><br><span class="line">    url=<span class="string">'http://www.you.com/projectname'</span>,</span><br><span class="line">    packages=[<span class="string">'projectname'</span>, <span class="string">'projectname.utils'</span>],</span><br><span class="line">)</span><br></pre></td></tr></table></figure></p>
<p>下一步，就是创建一个<code>MANIFEST.in</code>文件，列出所有在你的包中需要包含进来的非源码文件：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># MANIFEST.in</span><br><span class="line">include *.txt</span><br><span class="line">recursive-include examples *</span><br><span class="line">recursive-include Doc *</span><br></pre></td></tr></table></figure></p>
<p>确保<code>setup.py</code>和<code>MANIFEST.in</code>文件放在你的包的最顶级目录中。一旦你已经做了这些，你就可以像下面这样执行命令来创建一个源码分发包了：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">% python setup.py sdist</span><br></pre></td></tr></table></figure>
<p>它会创建一个文件比如<code>projectname-1.0.zip</code>或<code>projectname-1.0.tar.gz</code>,具体依赖于你的系统平台。如果一切正常，这个文件就可以发送给别人使用或者上传至<a href="http://pypi.python.org/" target="_blank" rel="noopener">Python Package Index</a>.</p>
<h1 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h1><ul>
<li><a href="http://python3-cookbook.readthedocs.io/zh_CN/latest/chapters/p10_modules_and_packages.html" target="_blank" rel="noopener">Python Cookbook</a></li>
<li><a href="http://zh-google-styleguide.readthedocs.io/en/latest/google-python-styleguide/contents/" target="_blank" rel="noopener">Google开源项目风格指南</a></li>
<li><a href="http://blog.sina.com.cn/s/blog_4ddef8f80102v57b.html" target="_blank" rel="noopener">python的dist-packages目录和site-packages目录的区别</a></li>
<li><a href="http://www.cnblogs.com/russellluo/p/3328683.html#2_2" target="_blank" rel="noopener">Python基础:模块</a></li>
</ul>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2016/07/19/非root权限源码安装GCC/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Huaipeng Zhao">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Huaipeng Zhao">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2016/07/19/非root权限源码安装GCC/" itemprop="url">非root权限下源码安装GCC</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2016-07-19T10:38:29+08:00">
                2016-07-19
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Linux/" itemprop="url" rel="index">
                    <span itemprop="name">Linux</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="预备知识"><a href="#预备知识" class="headerlink" title="预备知识"></a>预备知识</h1><p>当我们在服务器下工作时，我们往往是没有root权限的，而软件包的<strong>默认自动安装位置</strong>是/usr/local，因此我们并没有写入的权限。在讲解安装之前我们首先粗浅地了解一下linux的文件系统：  </p>
<table>
<thead>
<tr>
<th>目录</th>
<th>描述</th>
</tr>
</thead>
<tbody>
<tr>
<td>/</td>
<td>根目录</td>
</tr>
<tr>
<td>/bin/</td>
<td>系统可执行文件</td>
</tr>
<tr>
<td>/boot/</td>
<td>引导文件</td>
</tr>
<tr>
<td>/dev/</td>
<td>必要设备</td>
</tr>
<tr>
<td>/etc/</td>
<td>特定主机，系统范围内的配置文件</td>
</tr>
<tr>
<td>/home/</td>
<td>用户的文件目录</td>
</tr>
<tr>
<td>/lib/</td>
<td>/bin/ 和 /sbin/中二进制文件必要的库文件</td>
</tr>
<tr>
<td>/media/</td>
<td>可移除媒体(如CD-ROM)的挂载点</td>
</tr>
<tr>
<td>/mnt/</td>
<td>临时挂载的文件系统</td>
</tr>
<tr>
<td>/opt/</td>
<td>可选应用软件包</td>
</tr>
<tr>
<td>/proc/</td>
<td>虚拟文件系统</td>
</tr>
<tr>
<td>/root/</td>
<td>超级用户文件目录</td>
</tr>
<tr>
<td>/sbin/</td>
<td>必要的系统二进制文件</td>
</tr>
<tr>
<td>/usr/</td>
<td>用户程序目录</td>
</tr>
<tr>
<td>/var/</td>
<td>变量文件—在正常运行的系统中其内容不断变化的文件，如日志</td>
</tr>
</tbody>
</table>
<p>我们再看一下用户目录/usr下的文件：  </p>
<table>
<thead>
<tr>
<th>目录</th>
<th>描述</th>
</tr>
</thead>
<tbody>
<tr>
<td>/usr/bin/</td>
<td>非必要可执行文件，面向所有用户。</td>
</tr>
<tr>
<td>/usr/include/</td>
<td>标准包含文件</td>
</tr>
<tr>
<td>/usr/lib/</td>
<td>/usr/bin/和/usr/sbin/中二进制文件的库</td>
</tr>
<tr>
<td>/usr/sbin/</td>
<td>非必要的系统二进制文件，例如：大量网络服务的守护进程</td>
</tr>
<tr>
<td>/usr/share/</td>
<td>体系结构无关（共享）数据</td>
</tr>
<tr>
<td>/usr/src/</td>
<td>源代码,例如:内核源代码及其头文件</td>
</tr>
<tr>
<td>/usr/local/</td>
<td><strong>用户程序默认安装位置</strong>,里面包含bin等</td>
</tr>
</tbody>
</table>
<p>参考文献:<a href="https://zh.wikipedia.org/wiki/%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F%E5%B1%82%E6%AC%A1%E7%BB%93%E6%9E%84%E6%A0%87%E5%87%86" target="_blank" rel="noopener">wiki</a></p>
<p>我们可以看出用户程序一般默认安装到/usr/lcoal，但是对于一个多用户的服务器而言，我们一般是没有权限安装到/usr/local的，因此我们需要安装到自己的目录下。我们回到自己的用户目录下<code>~/</code>，我们新建<code>.local</code>用于我们软件的默认安装位置,因为pip等软件为用户默认安装–user的位置也是.local。</p>
<h1 id="准备资源"><a href="#准备资源" class="headerlink" title="准备资源"></a>准备资源</h1><p>这一步我们需要准备GCC和其依赖包的源码。<br><strong>步骤一:</strong>首先准备GCC的安装包，我们以GCC-4.8.5为例。我们先从GCC的<a href="https://gcc.gnu.org/" target="_blank" rel="noopener">官网</a>下载源代码。<br><strong>步骤二:</strong>解压<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ tar -zxf gcc-4.8.5.tar.gz</span><br></pre></td></tr></table></figure></p>
<p><strong>步骤三:</strong>查看gcc依赖包<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> gcc-4.8.5</span><br><span class="line">$ vim download_prerequisites</span><br></pre></td></tr></table></figure></p>
<p>我们会看到有三个依赖包和版本号：<em>MPFR=mpfr-2.4.2,GMP=gmp-4.3.2,MPC=mpc-0.8.1</em><br>下载地址：<br><a href="ftp://ftp.gnu.org/gnu/gmp/gmp-4.3.2.tar.bz2" target="_blank" rel="noopener">ftp://ftp.gnu.org/gnu/gmp/gmp-4.3.2.tar.bz2</a><br><a href="http://www.mpfr.org/mpfr-2.4.2/mpfr-2.4.2.tar.bz2" target="_blank" rel="noopener">http://www.mpfr.org/mpfr-2.4.2/mpfr-2.4.2.tar.bz2</a><br><a href="http://www.multiprecision.org/mpc/download/mpc-0.8.1.tar.gz" target="_blank" rel="noopener">http://www.multiprecision.org/mpc/download/mpc-0.8.1.tar.gz</a><br>我们把这三个依赖包拷贝到gcc源代码目录一块编译<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">$ tar jxf gmp-4.3.2.tar.bz2</span><br><span class="line">$ tar jxf mpfr-2.4.2.tar.bz2</span><br><span class="line">$ tar zxf mpc-0.8.1.tar.gz</span><br><span class="line">$ mv gmp-4.3.2 gcc-4.8.1/gmp</span><br><span class="line">$ mv mpfr-2.4.2 gcc-4.8.1/mpfr</span><br><span class="line">$ mv mpc-0.8.1 gcc-4.8.1/mpc</span><br></pre></td></tr></table></figure></p>
<p><strong>注意:</strong>拷贝之后的文件名一定要和上面一致，不然编译时会找不到路径。</p>
<h1 id="编译安装"><a href="#编译安装" class="headerlink" title="编译安装"></a>编译安装</h1><p>这一步我们需要编译gcc源文件了，我们首先进入源代码目录<code>gcc-4.8.5</code>，然后执行下面的命令：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#生成MakeFile，换成立自己的绝对路径</span></span><br><span class="line">$ ./configure --prefix=/home/simon/.<span class="built_in">local</span> --<span class="built_in">enable</span>-checking=release --<span class="built_in">enable</span>-languages=c,c++ --<span class="built_in">disable</span>-multilib</span><br><span class="line"><span class="comment">#编译，开启4个线程</span></span><br><span class="line">$　make -j4</span><br><span class="line"><span class="comment">#拷贝到.local目录</span></span><br><span class="line">$ make install</span><br></pre></td></tr></table></figure></p>
<h1 id="后续操作"><a href="#后续操作" class="headerlink" title="后续操作"></a>后续操作</h1><p>我们安装完之后怎么在终端直接用呢？有以下两种方法。</p>
<h2 id="配置环境变量"><a href="#配置环境变量" class="headerlink" title="配置环境变量"></a>配置环境变量</h2><p>将编译后的可执行文件的路径加入到环境变量中有三种方法：</p>
<ol>
<li><p>终端export，这种方法立即生效</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">export</span> PATH=/home/simon/.<span class="built_in">local</span>/bin:<span class="variable">$PATH</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>修改系统文件/etc/profile，这种方法是为所有用户设置的，需要重启或source /etc/profile生效。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">export</span> PATH=/home/simon/.<span class="built_in">local</span>/bin:<span class="variable">$PATH</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>修改用户文件.bashrc或.bashrc_profile，这是为当前用户设定，需要重启或source .bash_profile</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">export</span> PATH=/home/simon/.<span class="built_in">local</span>/bin:<span class="variable">$PATH</span></span><br></pre></td></tr></table></figure>
</li>
</ol>
<p>配置完环境变量我们就能在终端使用gcc,g++了。</p>
<h2 id="添加链接"><a href="#添加链接" class="headerlink" title="添加链接"></a>添加链接</h2><p>如果我有多个版本的gcc，想同时使用这些怎么办。那么我们只需要设置链接即可。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ ln -s /home/simon/.<span class="built_in">local</span>/bin/gcc gcc485</span><br><span class="line">$ ln -s /home/simon/.<span class="built_in">local</span>/bin/g++ g++485</span><br></pre></td></tr></table></figure>
<p>那么我们就能够在终端使用gcc485和其他版本的gcc了。</p>
<p>小结：个人感觉如果在非root权限下，操作一些东西可能更麻烦，但只有这样才能学到更多东西，才会去了解比较底层的东西，而在root权限下，一切变得很傻瓜式的了。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2016/07/11/python中的with语句/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Huaipeng Zhao">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Huaipeng Zhao">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2016/07/11/python中的with语句/" itemprop="url">Python中的with语句</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2016-07-11T15:45:19+08:00">
                2016-07-11
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Python/" itemprop="url" rel="index">
                    <span itemprop="name">Python</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="with语句的基本用法"><a href="#with语句的基本用法" class="headerlink" title="with语句的基本用法"></a>with语句的基本用法</h1><p>高级编程语言一般都会有异常处理机制来管理资源。像C++,Java,Python等语言中都会有try/finally的机制来管理资源。我们需要保证当try语句块的内容出现异常时，一些资源能够得以释放，例如文件使用后的关闭，线程中锁的自动获取和释放等。例如常见的文件操作，</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#打开文件</span></span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    f = open(filename)</span><br><span class="line"><span class="keyword">except</span>:</span><br><span class="line">    do something</span><br><span class="line"><span class="keyword">finally</span>:</span><br><span class="line">    do something</span><br><span class="line"><span class="comment">#使用文件</span></span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    do something</span><br><span class="line"><span class="keyword">except</span>:</span><br><span class="line">    do something</span><br><span class="line"><span class="keyword">finally</span>:</span><br><span class="line">    f.close()</span><br></pre></td></tr></table></figure>
<p>这样我们在处理一些遇到异常时，需要最后释放文件资源。我们会看到这种方式使得代码块变得臃肿，失去了一定的观赏性。另外这需要我们自己来处理每个异常。而with就是一种可以代替try/finally的优雅的解决方案。下面我们还是用打开文件的例子来演示with的基本用法。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> open(filename) <span class="keyword">as</span> f:</span><br><span class="line">    do something</span><br></pre></td></tr></table></figure></p>
<p>这样是不是比try/finally结构打开文件更加简洁了呢？程序的执行过程就是with后的表达式open返回了一个对象(在这里也叫作上下文管理器)，然后把对象的值赋值给f。如果打开文件有异常，那么就会退出with语句，由<code>文件</code>对象默认的异常处理机制来处理该异常。那么如果我需要同时打开多个文件呢？最开始的想法就是嵌套,<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> open(filename1) <span class="keyword">as</span> f1:</span><br><span class="line">    <span class="keyword">with</span> open(filename2) <span class="keyword">as</span> f2:</span><br><span class="line">        ...</span><br></pre></td></tr></table></figure></p>
<p>这显然不是一种比较好的解决方式，with提供了一种优雅的解决方案，<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> open(filename1) <span class="keyword">as</span> f1,open(filename2) <span class="keyword">as</span> f2:</span><br><span class="line">    do something</span><br></pre></td></tr></table></figure></p>
<p>当然利用contextlib中的nested会更加优雅，我们在下面会讲到。    </p>
<h1 id="上下文管理器-Context-Manager"><a href="#上下文管理器-Context-Manager" class="headerlink" title="上下文管理器(Context Manager)"></a>上下文管理器(Context Manager)</h1><h2 id="上下文管理器概述"><a href="#上下文管理器概述" class="headerlink" title="上下文管理器概述"></a>上下文管理器概述</h2><p>上下文管理机制就是控制了一个对象的生存周期，并且对该对象进行一些异常处理。对于一个语句块而言，“上文”就是执行之前的一些语句，一般为申请资源（文件，线程等），而“下文”就是执行完该语句块后释放资源（关闭文件，线程锁的释放等），相当于finally下的语句块。下面我们看下上下文管理器的一些基本概念：</p>
<ul>
<li><strong>上下文管理协议 (Context Management Protocol):</strong> 包含方法<code>__enter__()</code>和<code>__exit__()</code>，支持该协议的<strong>对象</strong>要实现这两种方法。</li>
<li><strong>上下文管理器 (Context Manager):</strong> 支持上下文管理协议的对象。需要实现<code>__enter__()</code>和<code>__exit__()</code>方法来建立运行时的上下文。通常用with来调用上下文管理器，也可以通过其他方法来使用。</li>
<li><strong>运行时上下文(runtime context):</strong> 由上下文管理器创建，通过上下文管理器的<code>__enter__()</code>和<code>__exit__()</code>方法实现，<code>__enter__()</code>方法在语句体执行之前进入运行时上下文，<code>__exit__()</code>在语句体执行完后从运行时上下文退出。with句支持运行时上下文这一概念。</li>
<li><strong>上下文表达式(Context Expression):</strong> with 语句中跟在关键字 with 之后的表达式，该表达式要返回一个上下文管理器对象。</li>
<li><strong>语句体(with-body):</strong> with 语句包裹起来的代码块，在执行语句体之前会调用上下文管理器的<code>__enter__()</code>方法，执行完语句体之后会执行<code>__exit__()</code>方法。</li>
</ul>
<p>除了文件之外，已经加入上下文管理协议支持的模块还有<strong>threading，decimal</strong>等。</p>
<h2 id="with语句的原理"><a href="#with语句的原理" class="headerlink" title="with语句的原理"></a>with语句的原理</h2><p>with的一般格式为：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> context_expression [<span class="keyword">as</span> target(s)]:</span><br><span class="line">    <span class="keyword">with</span>-body</span><br></pre></td></tr></table></figure></p>
<p>这里的as赋值是可选的。<br>在执行了with语句之后的具体工作原理如下：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">context_manager = context_expression</span><br><span class="line">exit = type(context_manager).__exit__  </span><br><span class="line">value = type(context_manager).__enter__(context_manager)</span><br><span class="line">exc = <span class="keyword">True</span>   <span class="comment"># True 表示正常执行，即便有异常也忽略；False 表示重新抛出异常，需要对异常进行处理</span></span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        target = value  <span class="comment"># 如果使用了 as 子句</span></span><br><span class="line">        <span class="keyword">with</span>-body     <span class="comment"># 执行 with-body</span></span><br><span class="line">    <span class="keyword">except</span>:</span><br><span class="line">        <span class="comment"># 执行过程中有异常发生</span></span><br><span class="line">        exc = <span class="keyword">False</span></span><br><span class="line">        <span class="comment"># 如果 __exit__ 返回 True，则异常被忽略；如果返回 False，则重新抛出异常</span></span><br><span class="line">        <span class="comment"># 由外层代码对异常进行处理</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> exit(context_manager, *sys.exc_info()):</span><br><span class="line">            <span class="keyword">raise</span></span><br><span class="line"><span class="keyword">finally</span>:</span><br><span class="line">    <span class="comment"># 正常退出，或者通过 statement-body 中的 break/continue/return 语句退出</span></span><br><span class="line">    <span class="comment"># 或者忽略异常退出</span></span><br><span class="line">    <span class="keyword">if</span> exc:</span><br><span class="line">        exit(context_manager, <span class="keyword">None</span>, <span class="keyword">None</span>, <span class="keyword">None</span>) </span><br><span class="line">    <span class="comment"># 缺省返回 None，None 在布尔上下文中看做是 False</span></span><br></pre></td></tr></table></figure></p>
<h2 id="自定义上下文管理器对象"><a href="#自定义上下文管理器对象" class="headerlink" title="自定义上下文管理器对象"></a>自定义上下文管理器对象</h2><p>下面我们定义自己的上下文管理器对象。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Demo</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self,tag)</span>:</span></span><br><span class="line">        self.tag = tag</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__enter__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">print</span> <span class="string">'Allocate Resource'</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__exit__</span><span class="params">(self,exc_type,exc_value,exc_tb)</span>:</span></span><br><span class="line">        <span class="keyword">print</span> <span class="string">'[Exit %s]: Free resource.'</span> % self.tag</span><br><span class="line">        <span class="keyword">if</span> exc_tb <span class="keyword">is</span> <span class="keyword">None</span>:</span><br><span class="line">            <span class="keyword">print</span> <span class="string">'[Exit %s]: Exited without exception.'</span> % self.tag</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">print</span> <span class="string">'[Exit %s]: Exited with exception raised.'</span> % self.tag</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">False</span>   <span class="comment"># 可以省略，缺省的None也是被看做是False</span></span><br></pre></td></tr></table></figure></p>
<p>下面是正常和异常测试<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> Demo(<span class="string">'Normal'</span>):</span><br><span class="line">    <span class="keyword">print</span> <span class="string">'[with-body] Run without exceptions.'</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> Demo(<span class="string">'With-Exception'</span>):</span><br><span class="line">    <span class="keyword">print</span> <span class="string">'[with-body] Run with exception.'</span></span><br><span class="line">    <span class="keyword">raise</span> Exception</span><br><span class="line">    <span class="keyword">print</span> <span class="string">'[with-body] Run with exception. Failed to finish statement-body!'</span></span><br></pre></td></tr></table></figure></p>
<h1 id="contextlib模块"><a href="#contextlib模块" class="headerlink" title="contextlib模块"></a>contextlib模块</h1><p>contextlib 模块提供了3个对象：装饰器 contextmanager、函数 nested 和上下文管理器 closing。使用这些对象，可以对已有的生成器函数或者对象进行包装，加入对上下文管理协议的支持，避免了专门编写上下文管理器来支持 with 语句。</p>
<h2 id="装饰器contextmanager"><a href="#装饰器contextmanager" class="headerlink" title="装饰器contextmanager"></a>装饰器contextmanager</h2><p>contextmanager 用于对生成器函数进行装饰，生成器函数被装饰以后，返回的是一个上下文管理器，其 <strong>enter</strong>() 和 <strong>exit</strong>() 方法由 contextmanager负责提供，而不再是之前的迭代子。被装饰的生成器函数只能产生一个值，否则会导致异常RuntimeError；产生的值会赋值给 as 子句中的 target，如果使用了 as 子句的话。下面看一个简单的例子。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> contextlib <span class="keyword">import</span> contextmanager</span><br><span class="line"></span><br><span class="line"><span class="meta">@contextmanager</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">demo</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">print</span> <span class="string">'[Allocate resources]'</span></span><br><span class="line">    <span class="keyword">print</span> <span class="string">'Code before yield-statement executes in __enter__'</span></span><br><span class="line">    <span class="keyword">yield</span> <span class="string">'*** contextmanager demo ***'</span></span><br><span class="line">    <span class="keyword">print</span> <span class="string">'Code after yield-statement executes in __exit__'</span></span><br><span class="line">    <span class="keyword">print</span> <span class="string">'[Free resources]'</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> demo() <span class="keyword">as</span> value:</span><br><span class="line">    <span class="keyword">print</span> <span class="string">'Assigned Value: %s'</span> % value</span><br></pre></td></tr></table></figure></p>
<p>执行的结果为:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[Allocate resources]</span><br><span class="line">Code before yield-statement executes <span class="keyword">in</span> __enter__</span><br><span class="line">Assigned Value: *** contextmanager demo ***</span><br><span class="line">Code after yield-statement executes <span class="keyword">in</span> __exit__</span><br><span class="line">[Free resources]</span><br></pre></td></tr></table></figure></p>
<p>可以看到，生成器函数中 yield 之前的语句在<code>__enter__()</code>方法中执行，yield 之后的语句在<code>__exit__()</code>中执行，而yield产生的值赋给了as子句中的value变量。<br>需要注意的是，contextmanager 只是省略了 <code>__enter__()</code>/ <code>__exit__()</code>的编写，但并不负责实现资源的“获取”和“清理”工作；“获取”操作需要定义在yield语句之前，“清理”操作需要定义yield语句之后，这样 with 语句在执行<code>__enter__()</code>/<code>__exit__()</code>方法时会执行这些语句以获取/释放资源，即生成器函数中需要实现必要的逻辑控制，包括资源访问出现错误时抛出适当的异常。</p>
<h2 id="nested"><a href="#nested" class="headerlink" title="nested"></a>nested</h2><p>nested 可以将多个上下文管理器组织在一起，避免使用嵌套 with 语句。我们用nested实现上述提到过的打开多个文件：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> contextlib <span class="keyword">import</span> nested</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> nested(open(filename1),open(filename2)) <span class="keyword">as</span> (f1,f2):</span><br><span class="line">    do something</span><br></pre></td></tr></table></figure></p>
<p>而nested的底层实现就是嵌套的with语句，发生异常后，如果某个上下文管理器的<code>__exit__()</code>方法对异常处理返回False，则更外层的上下文管理器不会监测到异常。</p>
<h2 id="上下文管理器-closing"><a href="#上下文管理器-closing" class="headerlink" title="上下文管理器 closing"></a>上下文管理器 closing</h2><p>closing 适用于提供了 close() 实现的对象，比如网络连接、数据库连接等，也可以在自定义类时通过接口close()来执行所需要的资源“清理”工作。其实现如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">closing</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="comment"># help doc here</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, thing)</span>:</span></span><br><span class="line">        self.thing = thing</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__enter__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.thing</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__exit__</span><span class="params">(self, *exc_info)</span>:</span></span><br><span class="line">        self.thing.close()</span><br></pre></td></tr></table></figure>
<p>上下文管理器会将包装的对象赋值给as子句的target变量，同时保证打开的对象在with-body执行完后会关闭掉。closing上下文管理器包装起来的对象必须提供 close() 方法的定义，否则执行时会报 AttributeError 错误。<br>当然我们也能自定义支持closing的对象：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ClosingDemo</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        self.acquire()</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">acquire</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">print</span> <span class="string">'Acquire resources.'</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">free</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">print</span> <span class="string">'Clean up any resources acquired.'</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">close</span><span class="params">(self)</span>:</span></span><br><span class="line">        self.free()</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> closing(ClosingDemo()):</span><br><span class="line">    <span class="keyword">print</span> <span class="string">'Using resources'</span></span><br></pre></td></tr></table></figure>
<h1 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h1><ul>
<li><a href="https://www.ibm.com/developerworks/cn/opensource/os-cn-pythonwith/#ibm-pcon" target="_blank" rel="noopener">浅谈Python的with语句</a></li>
<li><a href="http://www.hustyx.com/python/119/" target="_blank" rel="noopener">Python中使用With打开多个文件</a></li>
<li><a href="http://zhoutall.com/archives/325" target="_blank" rel="noopener">理解Python中的with…as…语法</a></li>
</ul>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2016/07/11/python解析XML/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Huaipeng Zhao">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Huaipeng Zhao">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2016/07/11/python解析XML/" itemprop="url">Python解析XML</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2016-07-11T14:06:57+08:00">
                2016-07-11
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Python/" itemprop="url" rel="index">
                    <span itemprop="name">Python</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="方法概述"><a href="#方法概述" class="headerlink" title="方法概述"></a>方法概述</h1><p>XML是一种常见的结构化的标记语言。解析XML的方法有很多，就python2.7而言就自带了四种解析XML的方法。在Python27/Lib/xml目录下我们可以看到这四种内置的库：<em>dom,etree,parsers,sax</em>。dom(<em>document object model</em>)解析XML的方法是先把数据在内存中解析成一个树，然后对树进行操作XML；etree(<em>element tree</em>)等价于一个轻量级的dom，也是先把XML数据在内存中解析成一棵树，具有方便友好的API，速度快，消耗内存小；parsers提供对C语言编写的expat解析器的一个直接的，底层的API接口，与前两者不同，这种方法并不是一次性把数据加载到内存中，而是基于事件执行解析的，需要用户自己定义回调函数来解析xml，但这个接口并不是标准化的；最后一种sax和parsers一样也是基于事件触发的，需要用户自己定义回调函数。一般第二种etree的解析方式是比较流行的，因为速度较快而且接口非常方便友好，除非在特殊的应用场景下(内存等因素)，一般我们选择比较简单的etree作为我们的工具，本文也是关于etree的常用方法的介绍。</p>
<h1 id="解析XML"><a href="#解析XML" class="headerlink" title="解析XML"></a>解析XML</h1><h2 id="调用标准库"><a href="#调用标准库" class="headerlink" title="调用标准库"></a>调用标准库</h2><p>在python2.7中的XML路径下提供了两种etree：一种是ElementTree，一种是cElementTree。加了”c”说明这是基于c语言优化的版本，效率比原始的ElementTree要高(主要是指解析过程)，因此我们优先考虑cElementTree。但由于比较低版本的python中并没有cElementTree，因此我们可以按如下方式调用库<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">try</span></span><br><span class="line">    <span class="keyword">import</span> xml.etree.cElementTree <span class="keyword">as</span> ET</span><br><span class="line"><span class="keyword">except</span> ImportError  </span><br><span class="line">    <span class="keyword">import</span> xml.etree.ElementTree <span class="keyword">as</span> ET</span><br></pre></td></tr></table></figure></p>
<h2 id="解析XML-1"><a href="#解析XML-1" class="headerlink" title="解析XML"></a>解析XML</h2><p>获取一棵XML树的方法有三种：<strong>解析XML文件</strong>，<strong>解析XML字符串</strong>，<strong>新建解析树</strong>。下面的代码展示了如何解析获得一棵XML树。</p>
<h3 id="解析XML文件"><a href="#解析XML文件" class="headerlink" title="解析XML文件"></a>解析XML文件</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tree = ET.parse(filename)</span><br><span class="line">root = tree.getroot()</span><br></pre></td></tr></table></figure>
<p>这种方法从文件中读取XML，并在内存中解析为ElementTree类型，然后通过getroot()的方法获取该树的根节点。</p>
<h3 id="解析XML字符串"><a href="#解析XML字符串" class="headerlink" title="解析XML字符串"></a>解析XML字符串</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># parse from a string</span></span><br><span class="line">root = ET.fromstring(str)</span><br></pre></td></tr></table></figure>
<p>该方法从字符串里面获得解析树的根节点，注意与从文件中解析不同，<strong>从字符串解析获得的是Element类型，而从文件中解析获取的是ElementTree类型。</strong></p>
<h3 id="新建解析树"><a href="#新建解析树" class="headerlink" title="新建解析树"></a>新建解析树</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">root = ET.Element(<span class="string">'root'</span>)</span><br><span class="line">tree = ET.ElementTree(root)</span><br></pre></td></tr></table></figure>
<p>新建一棵树首先确定一个根节点，然后把根节点放到新的解析树。</p>
<h1 id="查找XML"><a href="#查找XML" class="headerlink" title="查找XML"></a>查找XML</h1><p>在查找XML之前我们先了解一下XML的结构。ElementTree是把XML解析成嵌套的Element元素。因此我们要了解一下一个Element元素都包含哪些内容</p>
<ul>
<li>标签(tag)。这是一个说明Element存储的数据类型的字符串</li>
<li>属性(attributes)。这是一个节点Element的属性</li>
<li>文本(text)。这是节点Element存储的内容(字符串)</li>
<li>可选的尾字符(tail string)。</li>
<li>子节点(child elements)。  </li>
</ul>
<p>我们用官方文档的例子来说明：<br><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">&lt;?xml version="1.0"?&gt;</span><br><span class="line"><span class="tag">&lt;<span class="name">data</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">country</span> <span class="attr">name</span>=<span class="string">"Liechtenstein"</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">rank</span>&gt;</span>1<span class="tag">&lt;/<span class="name">rank</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">year</span>&gt;</span>2008<span class="tag">&lt;/<span class="name">year</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">gdppc</span>&gt;</span>141100<span class="tag">&lt;/<span class="name">gdppc</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">neighbor</span> <span class="attr">name</span>=<span class="string">"Austria"</span> <span class="attr">direction</span>=<span class="string">"E"</span>/&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">neighbor</span> <span class="attr">name</span>=<span class="string">"Switzerland"</span> <span class="attr">direction</span>=<span class="string">"W"</span>/&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">country</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">country</span> <span class="attr">name</span>=<span class="string">"Singapore"</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">rank</span>&gt;</span>4<span class="tag">&lt;/<span class="name">rank</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">year</span>&gt;</span>2011<span class="tag">&lt;/<span class="name">year</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">gdppc</span>&gt;</span>59900<span class="tag">&lt;/<span class="name">gdppc</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">neighbor</span> <span class="attr">name</span>=<span class="string">"Malaysia"</span> <span class="attr">direction</span>=<span class="string">"N"</span>/&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">country</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">country</span> <span class="attr">name</span>=<span class="string">"Panama"</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">rank</span>&gt;</span>68<span class="tag">&lt;/<span class="name">rank</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">year</span>&gt;</span>2011<span class="tag">&lt;/<span class="name">year</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">gdppc</span>&gt;</span>13600<span class="tag">&lt;/<span class="name">gdppc</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">neighbor</span> <span class="attr">name</span>=<span class="string">"Costa Rica"</span> <span class="attr">direction</span>=<span class="string">"W"</span>/&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">neighbor</span> <span class="attr">name</span>=<span class="string">"Colombia"</span> <span class="attr">direction</span>=<span class="string">"E"</span>/&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">country</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">data</span>&gt;</span></span><br></pre></td></tr></table></figure></p>
<h2 id="查找节点内容"><a href="#查找节点内容" class="headerlink" title="查找节点内容"></a>查找节点内容</h2><p>查找节点值就是查找该节点下存储的值，方法很简单。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#直接查找</span></span><br><span class="line">time = year.text()</span><br><span class="line"><span class="comment">#根据节点名查找,返回第一个匹配的节点的内容</span></span><br><span class="line">time = findtext(<span class="string">'year'</span>)</span><br><span class="line"><span class="comment">#迭代查找,返回所有子节点的text内容</span></span><br><span class="line"><span class="keyword">for</span> year <span class="keyword">in</span> country.itertext():</span><br><span class="line">    <span class="keyword">print</span> year</span><br></pre></td></tr></table></figure></p>
<h2 id="查找属性"><a href="#查找属性" class="headerlink" title="查找属性"></a>查找属性</h2><p>第一种需求是查找某个属性名的值，返回的是字符串类型：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">name = country.get(<span class="string">'name'</span>)</span><br></pre></td></tr></table></figure></p>
<p>第二种需求就是获取该节点的全部属性：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#第一种方法返回的是字典</span></span><br><span class="line">attribs_dict = country.attrib</span><br><span class="line">name = attribs_dict[<span class="string">'name'</span>]</span><br><span class="line"><span class="comment">#第二种方法返回的是列表，元素是key-value组成的tuple</span></span><br><span class="line">attribs_list = country.items()</span><br><span class="line"><span class="comment">#第三种方法返回属性名的列表</span></span><br><span class="line">keys = country.keys()</span><br></pre></td></tr></table></figure></p>
<h2 id="查找节点"><a href="#查找节点" class="headerlink" title="查找节点"></a>查找节点</h2><h3 id="根据子节点名查找"><a href="#根据子节点名查找" class="headerlink" title="根据子节点名查找"></a>根据子节点名查找</h3><p>根据子节点名查找有两种实现方式，第一种是利用find一次性查找，第二种是利用iter迭代查找。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#find查找第一个匹配的子节点</span></span><br><span class="line">first_child = root.find(<span class="string">'country'</span>)</span><br><span class="line">ET.dump(first_child)</span><br><span class="line"><span class="comment">#findall查找所有匹配的子节点</span></span><br><span class="line">children = root.findall(<span class="string">'country'</span>)</span><br><span class="line"><span class="keyword">for</span> child <span class="keyword">in</span> children:</span><br><span class="line">    ET.dump(child)</span><br><span class="line"><span class="comment">#迭代查找所有匹配的子节点</span></span><br><span class="line"><span class="keyword">for</span> country <span class="keyword">in</span> root.iterfind(<span class="string">'country'</span>):</span><br><span class="line">    ET.dump(country)</span><br></pre></td></tr></table></figure></p>
<h3 id="根据XPath查找"><a href="#根据XPath查找" class="headerlink" title="根据XPath查找"></a>根据XPath查找</h3><p>ElementTree还支持有限的<a href="https://www.w3.org/TR/xpath" target="_blank" rel="noopener">XPath</a>表达式。下面是一些例子。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Top-level elements</span></span><br><span class="line">root.findall(<span class="string">"."</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># All 'neighbor' grand-children of 'country' children of the top-level elements</span></span><br><span class="line">root.findall(<span class="string">"./country/neighbor"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Nodes with name='Singapore' that have a 'year' child</span></span><br><span class="line">root.findall(<span class="string">".//year/..[@name='Singapore']"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 'year' nodes that are children of nodes with name='Singapore'</span></span><br><span class="line">root.findall(<span class="string">".//*[@name='Singapore']/year"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># All 'neighbor' nodes that are the second child of their parent</span></span><br><span class="line">root.findall(<span class="string">".//neighbor[2]"</span>)</span><br></pre></td></tr></table></figure></p>
<p>下表是ElementTree支持的XPath语法：</p>
<table>
<thead>
<tr>
<th>Syntax</th>
<th>Meaning</th>
</tr>
</thead>
<tbody>
<tr>
<td>tag</td>
<td>Selects all child elements with the given tag. For example, spam selects all child elements named spam, and spam/egg selects all grandchildren named egg in all children named spam.</td>
</tr>
<tr>
<td>*</td>
<td>Selects all child elements. For example, */egg selects all grandchildren named egg.</td>
</tr>
<tr>
<td>.</td>
<td>Selects the current node. This is mostly useful at the beginning of the path, to indicate that it’s a relative path.</td>
</tr>
<tr>
<td>//</td>
<td>Selects all subelements, on all levels beneath the current element. For example, .//egg selects all egg elements in the entire tree.</td>
</tr>
<tr>
<td>..</td>
<td>Selects the parent element.</td>
</tr>
<tr>
<td>[@attrib]</td>
<td>Selects all elements that have the given attribute.</td>
</tr>
<tr>
<td>[@attrib=’value’]</td>
<td>Selects all elements for which the given attribute has the given value. The value cannot contain quotes.</td>
</tr>
<tr>
<td>[tag]</td>
<td>Selects all elements that have a child named tag. Only immediate children are supported.</td>
</tr>
<tr>
<td>[tag=’text’]</td>
<td>Selects all elements that have a child named tag whose complete text content, including descendants, equals the given text.</td>
</tr>
<tr>
<td>[position]</td>
<td>Selects all elements that are located at the given position. The position can be either an integer (1 is the first position), the expression last() (for the last position), or a position relative to the last position (e.g. last()-1).</td>
</tr>
</tbody>
</table>
<h3 id="根据下标查找"><a href="#根据下标查找" class="headerlink" title="根据下标查找"></a>根据下标查找</h3><p>我们也可以根据下标来直接查询子节点<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">year = root[<span class="number">0</span>][<span class="number">1</span>]</span><br><span class="line">ET.dump(year)</span><br></pre></td></tr></table></figure></p>
<h1 id="修改XML"><a href="#修改XML" class="headerlink" title="修改XML"></a>修改XML</h1><h2 id="修改属性和文本"><a href="#修改属性和文本" class="headerlink" title="修改属性和文本"></a>修改属性和文本</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#set(key,value)</span></span><br><span class="line">root.set(<span class="string">'name'</span>,<span class="string">'xml'</span>)</span><br><span class="line">ET.dump(root)</span><br><span class="line"></span><br><span class="line">root.text = <span class="string">'hello'</span></span><br><span class="line">ET.dump(root)</span><br></pre></td></tr></table></figure>
<h2 id="增加节点"><a href="#增加节点" class="headerlink" title="增加节点"></a>增加节点</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#新建节点</span></span><br><span class="line">new_element = ET.Element(<span class="string">'new'</span>)</span><br><span class="line"><span class="comment">#顺序插入节点</span></span><br><span class="line">root.append(new_element)</span><br><span class="line">ET.dump(root)</span><br><span class="line"><span class="comment">#按位置插入节点</span></span><br><span class="line">root.insert(<span class="number">1</span>,new_element)</span><br><span class="line">ET.dump(root)</span><br></pre></td></tr></table></figure>
<h2 id="删除节点"><a href="#删除节点" class="headerlink" title="删除节点"></a>删除节点</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#删除指定子节点</span></span><br><span class="line">root.remove(root[<span class="number">0</span>])</span><br><span class="line">ET.dump(root)</span><br><span class="line"></span><br><span class="line"><span class="comment">#清空所有子节点，并且清除属性和文本</span></span><br><span class="line">root.clear()</span><br><span class="line">ET.dump(root)</span><br></pre></td></tr></table></figure>
<h1 id="写入XML"><a href="#写入XML" class="headerlink" title="写入XML"></a>写入XML</h1><h2 id="写到控制台"><a href="#写到控制台" class="headerlink" title="写到控制台"></a>写到控制台</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#参数可以是ElementTree或Element</span></span><br><span class="line">ET.dump(root)</span><br></pre></td></tr></table></figure>
<h2 id="写到文件"><a href="#写到文件" class="headerlink" title="写到文件"></a>写到文件</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tree = ET.parse(<span class="string">'demo.xml'</span>)</span><br><span class="line">tree.write(<span class="string">'a.xml'</span>,encoding=<span class="string">'utf-8'</span>,xml_declaration=<span class="keyword">True</span>)</span><br></pre></td></tr></table></figure>
<h1 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h1><p>本文主要参考<a href="https://docs.python.org/2/library/xml.etree.elementtree.html#id5" target="_blank" rel="noopener">xml.etree.ElementTree</a>,更多的API接口可以参考该官方文档。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2016/07/10/神经网络与深度学习/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Huaipeng Zhao">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Huaipeng Zhao">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2016/07/10/神经网络与深度学习/" itemprop="url">【译】神经网络与深度学习</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2016-07-10T15:34:36+08:00">
                2016-07-10
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Deep-Learning/" itemprop="url" rel="index">
                    <span itemprop="name">Deep Learning</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p><a href="http://neuralnetworksanddeeplearning.com" target="_blank" rel="noopener"><em>NEURAL NETWORKS AND DEEP LEARNING</em></a>是<em>Michael Nielsen</em>在2015年初推出的书。他本身是个量子物理学家，因此数学功底深厚，能把数学道理讲的深入浅出。而这本书就把神经网络背后的数学讲的浅显易懂，非常适合初学者。仅供参考，不得作商业用途，水平有限，如有错误，请指正。</p>
<h1 id="用神经网络识别手写数字"><a href="#用神经网络识别手写数字" class="headerlink" title="用神经网络识别手写数字"></a>用神经网络识别手写数字</h1><h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>人类的视觉系统是是大自然的奇迹。考虑下面手写数字序列：  </p>
<p><img src="/images/c1s0-1.png" width="25%" height="20%"></p>
<p>大多数人能够轻易地是识别出是504192。在我们大脑的每个半球都有一个基础的皮质，这就是我们熟知的V1区，它包含了14亿个神经元，并有着数十亿级的神经元连接。然而人类地视觉不仅包含V1区，同时还有一系列地视觉皮质——V2，V3，V4，V5——它们进行着复杂地图像处理。我们头脑携带着一个超级计算机，经过数百万年的进化，能够很好地适应视觉的世界。识别手写数字并不容易。但是，我们人类惊人地擅长将我们所见之物转化成脑中有意义地东西。当然这些工作都是在无意识地情况下完成的。因此，通常我们不会感受到我们地视觉系统在处理着怎样的难题。  </p>
<p>当你尝试写程序去解决上述手写数字识别时，你就会发现这个问题有多难。我们大脑轻易在瞬间完成地事情变得相当困难了。一个简单地例子能描述我们识别形状为何变得不容易用算法表达——“一个9上面时一个环形，下面是一个钩”。当你想表达更加精细时，你会很快陷入特例的沼泽。这个问题看起来相当令人绝望。  </p>
<p>神经网络用另一种方式来解决这个问题。思想就是利用大量的手写数字，即我们熟知的训练数据，然后能通过学习这些训练数据来发展一个系统。  </p>
<p><img src="/images/c1s0-2.png" width="45%" height="40%"> </p>
<p>换句话说，神经网络能够利用这些例子自动地学习识别手写数字地内在规则。另外，如果增大训练样本，那么神经网络能够学到更多手写数字的信息，因此能够提高它的准确率。尽管我们上面地图只有100个训练样本，但是在实际中可能会用到数以亿计的训练样本。  </p>
<p>在这章我将写一个程序来实现一个神经网络能够学习识别这些手写数字。这段代码只有仅仅74行，没有用到任何神经网络的库。但是这段代码在没有人工的干预下达到超过96%的准确率。另外，在稍后地章节我们会用其他的一些想法来把我们的准确率提高到99%以上。实际上，最好的商用神经网络能够很好地用于银行识别支票，邮局识别地址等等。  </p>
<p>我们关注手写数字识别是因为它是个很好的一个原型能够帮助我们学习神经网络地通用原理。作为一个模型，它有两点好处：首先它是个有挑战性的项目——识别手写数字是个不小地壮举——但是它也不是需要一个及其复杂地模型或者强大的计算能力。另外，它也能帮助我们学习更高级的技术，例如深度学习，打下良好的基础。因此，在这本书我一直会贯穿手写数字识别问题。在这本书的稍后章节，我将会讨论如何将这些思想应用到计算机视觉，语音识别，自然语言处理等领域。  </p>
<p>当然，如果这章地内容仅仅关于如何写一段程序识别手写数字，那么篇幅会大大缩短！但我们将会学习神经网络地一些关键思想，包括两个重要的人工神经元（分类器和sigmoid神经元)，神经网络地标准学习算法即我们熟知的梯度下降法(stochastic gradient descent)。我将会关注为何它们能起到这样的作用，用来建立你对神经网络地一些直觉。我不仅会给出基本数学原理，还会花费更长的篇幅来讨论，但我觉得如果你能收获更深层次的理解，这也是值得的。我们将会在本章的最后理解什么是深度学习以及为什么它们如此重要。</p>
<h2 id="感知机"><a href="#感知机" class="headerlink" title="感知机"></a>感知机</h2><p>什么是神经网络？作为开始，我将会解释一种被称为感知器（perceptron）的人工神经元。感知器是20世纪50年代和60年代期间由科学家<a href="http://en.wikipedia.org/wiki/Frank_Rosenblatt" target="_blank" rel="noopener">Frank Rosenblatt</a>提出，他受到<a href="http://en.wikipedia.org/wiki/Warren_McCulloch" target="_blank" rel="noopener">Warren McCulloch</a>和<a href="http://en.wikipedia.org/wiki/Walter_Pitts" target="_blank" rel="noopener">Walter Pitts</a>早期工作的影响。当今，人们通常使用其他的人工神经元——在这本书和大量的现在神经网络的工作都使用了一种称之为<strong>sigmoid</strong>神经元。但是为了理解为什么sigmoid神经元那么定义，我们需要首先花些时间理解感知器。  </p>
<p>那么感知器是如何工作的呢？一个感知器有若干个二进制输入，$x_1, x_2, \ldots$并且产生一个二进制输出：  </p>
<p><img src="/images/c1s1-1.png">  </p>
<p>在这个例子中展示的感知器有三个输入，$x_1, x_2, x_3$。通常来讲它可能会有更多或更少的输入。Rosenblatt提出一种简单的规则来计算输出。他引入权重(<em>weight</em>)，$w_1,w_2,\ldots$，这些数字是用来表达各个输入对输出结果的重要程度。神经元的输出$0$或$1$是由加权和$\sum_j w_j x_j$是否大于或小于某个阈值(<em>threshold value</em>)来确定的。就像权重一样，阈值也是作为神经元参数的一部分。下面的公式精确地表达了这种思想：  </p>

\begin{eqnarray}
\mbox{output} & = & \left\{ \begin{array}{ll}
0 & \mbox{if } \sum_j w_j x_j \leq \mbox{ threshold} \\
1 & \mbox{if } \sum_j w_j x_j > \mbox{ threshold}
\end{array} \right.
\tag{1}\end{eqnarray}

<p>这就是一个感知器的工作方式！  </p>
<p>这是一个基础的数学模型。你可以把感知器看作通过权重实现的决策器。我将给出一个例子。这不是一个真实的例子，但它易于理解，稍后我们将接触更真实的例子。假设周末来了，你听说城市里将会举办起司节。你喜欢起司，所以你尽力去决定要不要参加这个节日。你的决定可能由衡量以下三个因素确定：  </p>
<ol>
<li>天气好么？  </li>
<li>有朋友陪你么？</li>
<li>节日旁边是否有公交站？（假设你没有车）。   </li>
</ol>
<p>我们用$x_1, x_2$和$x_3$来表示这三个因素。举个例子来说，$x_1 = 1$代表天气不错，$x_1 = 0$代表天气很糟糕。同理，$x_2 = 1$代表你有朋友陪，而$x_2 = 0$则代表没人陪。$x_3$也类似。  </p>
<p>现在假设你真心的喜欢起司，以至于即使没有朋友陪伴或者很难去起司节你也会乐意前往。但是你及其厌恶糟糕的天气，如果天气不好你就坚决不去起司节。你能用感知器来作为决策器的模型。那么做的一种方式就是选择权重$w_1=6$作为天气的权重，$w_2=2$和$w_3=2$作为其他的条件。$w_1$的权重很大暗示着你很在在意天气，这可能比朋友陪伴和交通状况更重要。最后，你选择了5作为感知器的阈值。选择之后感知器能够生成这样的决策器模型：当天气是好的时候输出为$1$，当天气差的时候输出为$0$。  </p>
<p>通过改变权重和阈值，我们能够获得不同的决策器模型。举个例子来说，假设我们设定阈值为$3$。此时，你要去起司节的情况变为：天气很好或者天气糟糕但是交通便利并有朋友陪伴。换句话说，这是一个不同的决策器。降低阈值意味着你更想去起司节。  </p>
<p>很显然，感知机和人类做决策时并不完全一样！但是这个例子很好地解释了感知器如何通过改变权重来影响决策。这应该是合理的，一个复杂的感知机网络能够做更精细的决定：  </p>
<p><img src="/images/c1s1-2.png" width="60%" height="35%">  </p>
<p>在这个网络中，第一列感知器——就是我们俗称的第一层感知器——通过改变输入的权重来做三个简单的决策。那么第二层呢？每个感知器也通过改变第一层输出的权重来做决策。 这样感知器的第二层比第一层能够在更复杂更抽象的水平上做出决策。而且，第三层能够做出更加复杂的决策。通过这种方式，一个多层感知器能够参与复杂的决策。  </p>
<p>顺便提一句，当我定义感知器时我说一个感知器仅仅有一个输出。在上述的网络中似乎有多个输出。事实上，它们仍然只是有一个输出。多个输出的箭头仅仅是一种有用的方式来解释一个感知器的输出能够作为其他感知器的输入。它比起绘制一条输出线然后分裂更方便。  </p>
<p>让我们来简化我们描述感知器的方式。条件$\sum_j w_j x_j &gt; \mbox{threshold}$有些麻烦，我们改变两种符号来简化它。第一个改变就是将$\sum_j w_j x_j$写作点积，$w \cdot x \equiv \sum_j w_j x_j$，这里的$w$和$x$分别是权重和输入组成的向量。第二个改变就是将阈值改变成不等式的形式，我们用熟知的感知器的<em>偏置</em>(bias)代替阈值，$b \equiv<br>-\mbox{threshold}$。那么感知器的规则就能重写为以下形式：  </p>

\begin{eqnarray}
  \mbox{output} = \left\{ 
    \begin{array}{ll} 
      0 & \mbox{if } w\cdot x + b \leq 0 \\
      1 & \mbox{if } w\cdot x + b > 0
    \end{array}
  \right.
\tag{2}\end{eqnarray}

<p>你可以把偏置（bias）看作衡量感知器输出$1$的难易程度的度量。或者我们用一个更生物的术语，偏置是感知器激活（fire）难易程度的度量。对于一个有着相当大偏置的感知器，它非常容易输出$1$.但如果偏置是一个很大的负数，那么它很难输出$1$。显然引入偏置只是我们对感知器描述的微小改变，但是稍后我们会看到它将带来更多的符号简化。正因如此，在这本书的剩余部分，我不会提阈值（threshold），我们将一直用偏置（bias）这个概念。  </p>
<p>我已经把感知器作为一种通过改变权重而做决策的方法。感知器的另外一种用途就是能够用于计算比较底层的逻辑计算，例如<strong>AND,OR</strong>和<strong>NAND</strong>。举个例子来说，我们有一个2个输入的感知器，每个权重都是$-2$，偏置为$3$。下图就是我们的感知器： </p>
<p><img src="/images/c1s1-3.png" width="40%" height="20%">  </p>
<p>我们会看到输入$00$将会产生$1$，因为$(-2)*0+(-2)*0+3 = 3$是正数。这里我们引入符号$*$为乘法符号。类似的，当我们输入$01$和$10$会产生输出$1$。但我们输入$11$则会产生$0$，因为$ (-2)*1+(-2)*1+3 = -1 $ 是负数。因此我们的感知器是一个<a href="https://en.wikipedia.org/wiki/NAND_gate" target="_blank" rel="noopener">与非门</a>!  </p>
<p>从与非门的例子中我们可以看出可以用感知器计算逻辑运算。事实上，我们能用感知器计算所有的逻辑运算。理由就是任何逻辑运算都能用与非门来构建。举个例子，我们用与非门来构建一个两个bits相加的电路：$x_1$和$x_2$。这需要计算按位求和，$x_1 \oplus x_2$，同时也需要尽算进位位。下图是一个示例：  </p>
<p><img src="/images/c1s1-4.png" width="65%" height="40%">  </p>
<p>为了得到等价的感知器网络，我们需要用上面的与非门感知器替换上图的与非门。下图是结果。我稍微调整了一下位置，因为这样比较容易画图：  </p>
<p><img src="/images/c1s1-5.png" width="65%" height="40%">  </p>
<p>上面这个网络的显著特征就是最左边的感知器的结果被最底面的感知器用作两次输入。当我定义感知器时并没有说这种情况是允许的。事实上，这并不会影响什么。如果我们不想允许这种事发生，那么我们能把这两条线简化成一条，但下面感知器的权重就变为$-4$。（如果你觉得看着不那么显而易见，你可以停下来去证明它的等价性。）改变之后的感知器如下图：  </p>
<p><img src="/images/c1s1-6.png" width="65%" height="40%">  </p>
<p>到目前为止，我画了$x_1$和$x_2$作为输入变量。事实上，如果我们把用输入层（<em>the input layer</em>）来编码输入那会更加方便画图：  </p>
<p><img src="/images/c1s1-7.png" width="65%" height="40%">  </p>
<p>输入感知器的明显特征就是我们有一个输出，但是我们没有输入。  </p>
<p><img src="/images/c1s1-8.png" width="15%" height="10%">  </p>
<p>这并不真实意味着这是一个没有输入的感知器。为了理解这个，我们假设的确有一个没有输入的感知器。那么加和$\sum_j w_j x_j$将会是$0$。那就是说这个感知器仅仅会输出一个定值。但我们最好不要把输入层看作感知器，而是把它仅仅看作是一个能输出定值$x_1, x_2,\ldots$的特定模块。  </p>
<p>加法器的例子告诉我们如何用感知器网络模拟包含众多与非门的电路。因为与非门可以作为通用的计算，因此感知器也能作为通用的计算。  </p>
<p>感知器具有通用计算的能力既让我们得到鼓舞也让我们失望。让我们鼓舞的是感知器能够作为一种强大的计算设备。但它也是令人失望的，因为它仅仅是一种新型的与非门。这很难说是一个大新闻！  </p>
<p>尽管如此，真实的形式比这一观点更好。现在已被证明我们能用学习算法自动地学习人工神经网络权重和偏置。这全是由于外部刺激所引起的，并没有编程人员的直接参与。这些学习算法让人工神经元的工作方法和传统的逻辑门截然不同。相比仅仅解决与非门和其它门的电路，我们的神经网络能够通过简单学习来解决问题，有时这些问题可能用传统的电路极难设计。</p>
<h2 id="Sigmoid神经元"><a href="#Sigmoid神经元" class="headerlink" title="Sigmoid神经元"></a>Sigmoid神经元</h2><p>学习算法听起来很美妙。但是我们怎么样为神经网络设计如此的算法呢？假设我们要用感知器网络学习解决一些问题。举例来说，假设输入是手写数字图片转化成的像素点。我们想让这个网络学习权重和偏置来对手写数字进行分类。为了理解学习算法是如何工作的，假设我们对权重（或偏置）做出小的改变。我们所希望的是这个小的改变只对输出相应部分造成影响。我们稍后将会看到，这种性质能够让学习算法成为一种可能。正如下图所示，这正是我们想要的。（显然这种网络太简单，难以胜任手写数字识别的问题！）：  </p>
<p><img src="/images/c1s2-1.png" width="60%" height="40%">  </p>
<p>如果权重（或偏置）的变化仅仅对输出造成影响，那么我们能利用这点修改权重和偏置来得到我们想要的神经网络。举个例子，假设网络错误把“9”识别成“8”。我们搞清楚了如何改变权重和偏置以至于让我们的网络能够更接近把图片识别为“9”。如果我们一直如此重复改变权重和偏置，那么我们的网络能够产生越来越好的结果。那么网络就能够学习了。  </p>
<p>问题是这并不是包含感知器神经元网络所发生的事情。事实上，我们对任何一个感知器神经元权重和偏置做出的小变化能够造成结果的完全翻转，比如从$0$变为$1$。这种翻转可能造成网络剩余部分行为的改变。因此当“9”被正确分类了，但对于网络其他图片的分类可能以某种不可控的方式造成分类结果完全改变的情况。那么逐渐调整权重和偏置来逼近我们想要的网络行为这种方法变得非常苦难。可能有某种聪明的方法能解决这种问题。但是我们如何学习感知器神经元网络并不是那么显而易见的。  </p>
<p>我们通过引进一种新的人工神经元——<em>sigmoid</em>神经元来客服这个问题。Sigmoid神经元和感知器神经元非常相似，但是对它权重和偏置造成微小改变仅仅对输出造成微小改变。这正是允许神经网络学习的关键点。  </p>
<p>现在我们来描述一下sigmoid神经元。我们将以描绘感知器神经元的方式来描绘sigmoid神经元：  </p>
<p><img src="/images/c1s2-2.png" width="40%" height="20%">  </p>
<p><span id="（3）">就像感知器一样，sigmoid神经元有输入，$x_1, x_2,\ldots$但是不仅仅是$0$和$1$，这些输入可以是$0$到$1$之间的任何值。举个例子，$0.638\ldots$也是一个有效的输入。和感知器一样，sigmoid神经元也有对应每个输入的权重，$w_1, w_2,\ldots$和偏置$b$。但是输出不是$0$和$1$。具体而言，输出是$\sigma(w \cdot x+b)$,这里$sigma$被称为<strong>Sigmoid函数</strong>(<em>sigmoid function</em>),它的定义如下： </span> </p>

\begin{eqnarray} 
  \sigma(z) \equiv \frac{1}{1+e^{-z}}.
  \tag{3}\end{eqnarray}

<p>更精确的说，一个输入为$x_1,x_2,\ldots$,权重为$w_1,w_2,\ldots$，偏置为$b$的sigmoid神经元的输出为：  </p>

\begin{eqnarray} 
  \frac{1}{1+\exp(-\sum_j w_j x_j-b)}.\tag{4}\end{eqnarray}

<p>在第一印象中，sigmoid神经元看起来和感知器完全不同。如果你对sigmoid函数形式不熟悉的话，它看起来是不透明的。事实上，sigmoid神经元和感知器有很多相似之处，代数形式只是一种技术细节，而不应该成为理解上的障碍。  </p>
<p>为了理解它和感知器模型的相似性，假设$z\equiv w \cdot x + b$是一个很大的正数，sigmoid神经元的输出接近$1$，这就像感知器一般。我们再假设$z\equiv w \cdot x + b$是一个很小的负数。那么$e^{-z} \rightarrow \infty$，因此$\sigma(z) \approx 0$。所以当$z\equiv w \cdot x + b$是一个很小的负数时，它的行为也和感知器类似。仅仅当$w \cdot x+b$是一个适中的数时sigmoid神经元和感知器才有所差别。  </p>
<p>那么如何理解$\sigma$呢？事实上，$\sigma$的确切形式并不是那么重要——真正有用的是它的函数图形形状。正如下图：<br><span id="sigmoid"><br><img src="/images/c1s2-3.png" width="50%" height="40%"><br></span><br>它的形状比阶跃函数更加平滑：  </p>
<p><img src="/images/c1s2-4.png" width="50%" height="40%">  </p>
<p>如果$\sigma$函数是阶跃函数，那么sigmoid神经元将会变为感知器，因为输出结果$0$或$1$将取决于$w\cdot x+b$是正数还是负数。如果函数是sigmoid的真正形式，那么我们可以把它看作是一种更加平滑的感知器。事实上，sigmoid函数图形的平滑性才是真正的关键点，而不是在于它的具体形式是如何。$\sigma$函数的平滑性意味着如果我们对权重做出小的改变$\Delta w_j$，对偏置做出微小改变$\Delta b$那么输出将会产生改变$\Delta \mbox{output}$。事实上，通过计算我们可以得到输出变化可以近似为如下形式：<br><span id="（5）"></span><br>
\begin{eqnarray} 
  \Delta \mbox{output} \approx \sum_j \frac{\partial \, \mbox{output}}{\partial w_j}
  \Delta w_j + \frac{\partial \, \mbox{output}}{\partial b} \Delta b,
\tag{5}\end{eqnarray}
 </p>
<p>不要被上面复杂的偏导形式吓到，它其实在说一件很简单的事：$\Delta\mbox{output}$是关于$\Delta w_j$和$\Delta b$的线性函数。线性使得我们选择权重和偏置的微小改变来得到期望输出的微小改变这件事变得更加容易。所以虽然sigmoid神经元和感知器有很多相似之处，但sigmoid神经元能够更容易指出我们应该如何选择权重和偏置来得到我们想要的输出。  </p>
<p>如果我们真的仅仅关注函数图形的形状而不是具体的形式，那么为什么我们选用<a href="#（3）">(3)</a>作为具体的形式呢？事实上，我们在这本书中偶尔会用到$f(\cdot)$作为激活函数。不同函数形式的主要不同在于公式<a href="#（5）">(5)</a>的偏导形式有所区别。稍后我们将看到当计算偏导时，用$\sigma$能够简化代数形式，因为e指数函数求偏导时形式比较优美。无论如何，$\sigma$都是神经网络中比较常用的一种激活函数，也是本书中用的最多的激活函数。  </p>
<p>那么如何解释sigmoid神经元的输出呢？显然，感知器和sigmoid神经元的一个重大不同就是sigmoid神经元不仅仅输出$0$或者$1$。它们可能是$0$到$1$之间的任何实数。，因此值可能像$0.173\ldots$和$0.689\ldots$都是有效的输出。这可能是有用的，举个例子来说，当我们想用输出的结果值代表输入图像的像素点平局密度时。但是有时可能是令人厌恶的。假设我们想要判断输出的结果是$9$或者不是$9$时。显然，用感知器输出的$0$或$1$来解释这件事更容易。但我们能够约定一些惯例来解释这种事，例如，当输出结果不小于$0.5$时就是$9$，小于$0.5$时不是$9$.当我们使用这种约定时，我会明确强调，因此它不应该成为一个问题。  </p>
<p><strong>Exercises</strong>  </p>
<ul>
<li><strong>sigmoid神经元模拟感知器，第一步</strong> </li>
</ul>
<p>假设我们把感知器的所有权重和偏置都乘上一个正常数，$c &gt; 0$。证明感知器的行为不会改变。  </p>
<ul>
<li><strong>sigmiod神经元模拟感知器，第二步</strong></li>
</ul>
<p>假设我们和上题的设定一样。同时也假设感知器神经网络的所有输入也都被选择。我们不需要实际的输入，我们只需要令输入值固定。假设对于输入$x$，对于任何感知器神经元选择的权重和偏置都使得$w \cdot x + b \neq 0$。现在把所有的感知器神经元换为sigmoid神经元，并把权重和偏置乘上常数$c&gt;0$。证明，当$c \rightarrow \infty$时sigmoid神经元和感知器神经元的行为一致。为什么当$w \cdot x + b = 0$时这种结论就不成立了呢？</p>
<h2 id="神经网络的结构"><a href="#神经网络的结构" class="headerlink" title="神经网络的结构"></a>神经网络的结构</h2><p>在下一节中，我会介绍一个神经网络能够很好地解决手写数字分类问题。作为准备，解释一些神经网络的术语有利于我们更好地理解。假设我们有如下的网络：  </p>
<p><img src="/images/c1s3-1.png" width="50%" height="40%">  </p>
<p>正如上面提到过的一样，最左边的一层被称为输入层，这一层的神经元被称为<em>输入神经元（input neurons）</em>.最右边或者输出层包含<em>输出神经元（output layer）</em>。中间的层被称为<em>隐藏层（hidden layer）</em>，因为它们既不是输入层也不是输出层。“隐藏”这个术语可能听起来有点神秘——我初次听到时以为它一定包含着某些深度的哲学或者数学含义——但是它真的只是意味着“既不是输入也不是输出”。这个网络包含了一个隐藏层，但是一些网络有多层的隐藏层。举例来说，下面的四层网络就包含两个隐藏层：  </p>
<p><img src="/images/c1s3-2.png" width="70%" height="50%">  </p>
<p>令人感到困惑的是，因为一些历史原因，想这种多层的神经网络有时被称为<em>多层感知器（multilayer perceptrons）</em>或者<strong>MLPs</strong>，尽管它是由sigmoid神经元而不是感知器神经元构成。在这本书中我不打算用MLP这个术语，因为它让人感到困惑，但是只是为了提醒你的确有这种称呼。  </p>
<p>输入层和输入层的设计通常是很简单的。举例来说，假设我们正努力识别一个手写数字图片是“9”或者其他数字。一个自然的方式就是设计一个神经网络去编码图片像素作为输入。如果图像是64乘64的灰度图像，那么我们一共会有$4,096 = 64 \times 64$个输入神经元，其中每个神经元的值都是$0$到$1$之间的灰度图像。那么输出层只会包含一个神经元，当输出值小于$0.5$时意味着“输入图像不是9”，当大于$0.5$时暗示着“输入图像是9”。  </p>
<p>输入层和输出层的设计通常比较直白，但是隐藏层的设计是一门艺术。当然我们也不可能把所有的设计过程总结为几条科学经验。神经网络研究员们设计了很多启发式的算法来构造隐藏层，用于完成不同的任务。举个例子来说，这样的启发式算法能用来帮助我们决定隐藏层神经元的数目。我们在这本书的后面会遇到几种启发式算法。  </p>
<p>到目前为止，我们已经讨论的神经网络都是上一层的输出作为下一层的输入，这些网络被称为前馈（<em>feedforward</em>）网络。这意味着网络结构没有环。因为有环的话停止条件将变得没有意义，因为我们不允许带环。  </p>
<p>当然也有一些人工神经网络也使得前馈环变为可能。这些模型被称为<em><a href="http://en.wikipedia.org/wiki/Recurrent_neural_network" target="_blank" rel="noopener">recurrent neural networks</a></em>。这种环不会造成问题是因为一个神经元的输出仅仅会在稍后的时间影响输入，而不是立即影响。（译者注：其实这里的解释并不是太好，循环神经网络并没有所谓的环状结构，环状的示意图只是为了便于理解。）</p>
<h2 id="用简单的神经网络识别手写数字"><a href="#用简单的神经网络识别手写数字" class="headerlink" title="用简单的神经网络识别手写数字"></a>用简单的神经网络识别手写数字</h2><p>定义了神经网络之后，让我们回到手写数字识别的问题上来。我们可以把手写数字识别问题拆分为两个子问题。首先，我们要找到一种方法能够把一张包含若干数字的图像分割为若干小图片，其中每个小图像只包含一个数字。举个例子，我们想将下面的图像</p>
<p><img src="/images/c1s4-1.png" width="20%" height="10%"></p>
<p>分割为6张小图像</p>
<p><img src="/images/c1s4-2.png" width="40%" height="10%">  </p>
<p>我们人类能够很容易地解决这个分割问题，但是让计算机去正确地分割图像是一件极具挑战的任务。当图像被分割之后，接下来的任务就是如何识别每个独立的手写数字。举个例子来说，我们想要程序去识别上述图像中的第一个数字，</p>
<p><img src="/images/c1s4-3.png" width="10%" height="10%"></p>
<p>结果是5。</p>
<p>我们将把精力集中在实现程序去解决第二个问题，即如何正确分类每个单独的手写数字。因为事实证明，只要你解决了数字分类的问题，分割问题相对来说不是那么困难。分割问题的解决方法有很多。一种方法是尝试不同的分割方式，用数字分类器对每一个切分片段打分。如果数字分类器对每一个片段的置信度都比较高，那么这个分割方式就能得到较高的分数；如果数字分类器在一或多个片段中出现问题，那么这种分割方式就会得到较低的分数。这种方法的思想是，如果分类器有问题，那么很可能是由于图像分割出错导致的。这种思想以及它的变种能够比较好地解决分割问题。因此，与其关心分割问题，我们不如把精力集中在设计一个神经网络来解决更有趣、更困难的问题，即手写数字的识别。</p>
<p>为了识别数字，我们将会使用一个三层神经网络：</p>
<p><img src="/images/c1s4-4.png" width="60%" height="50%"></p>
<p>这个网络的输入层是对输入像素编码的神经元。正如我们在下一节将要讨论的，我们的训练数据是一堆$28$乘$28$手写数字的位图，因此我们的输入层包含了$784 = 28 \times 28$个神经元。为了方便起见，我在上图中没有完全画出$784$个输入神经元。输入的像素点是其灰度值，$0.0$代表白色，$1.0$代表黑色，中间值表示不同程度的灰度值。</p>
<p>网络的第二层是隐层。我们为隐层设置了$n$个神经元，我们会实验$n$的不同取值。在这个例子中我们只展现了一个规模较小的隐层，它仅包含了$n = 15$个神经元。</p>
<p>网络的输出层包含了10个神经元。如果第一个神经元被激活，例如输出$\approx 1$，然后我们可以推断出这个网络认为这个数字是$0$。如果第二层被激活那么我们可以推断出这个网络认为这个数字是$1$。以此类推。更精确一些的表述是，我们把输出层神经元依次标记为$0$到$9$，我们要找到哪一个神经元拥有最高的激活值。如果$6$号神经元有最高值，那么我们的神经网络预测输入数字是$6$。对其它的神经元也如此。</p>
<p>你可能会好奇为什么我们用$10$个输出神经元。毕竟我们的任务是能让神经网络告诉我们哪个数字（$0, 1, 2, \ldots, 9$）能和输入图片匹配。一个看起来更自然的方式就是使用$4$个输出神经元，把每一个当做一个二进制值，结果取决于它的输出更靠近$0$还是$1$。四个神经元足够编码这个问题了，因为$2^4 = 16$大于$10$种可能的输入。为什么我们反而要用$10$个神经元呢？这样做难道效率不低吗？最终的判断是基于经验主义的：我们可以实验两种不同的网络设计，结果证明对于这个特定的问题而言，$10$个输出神经元的神经网络比$4$个的识别效果更好。但是令我们好奇的是为什么使用$10$个输出神经元的神经网络更有效呢。有没有什么启发性的思考能提前告诉我们用$10$个输出编码比使用$4$个输出编码更有好呢？</p>
<p>为了理解为什么我们这么做，我们需要从根本原理上理解神经网络究竟在做些什么。首先考虑有$10$个神经元的情况。我们首先考虑第一个输出神经元，它告诉我们一个数字是不是$0$。它能那么做是因为可以权衡从隐藏层来的信息。隐藏层的神经元在做什么呢？假设隐藏层的第一个神经元只是用于检测如下的图像是否存在：</p>
<p><img src="/images/c1s4-5.png" width="10%" height="10%"></p>
<p>为了达到这个目的，它通过对此图像对应部分的像素赋予较大权重，对其它部分赋予较小的权重。同理，我们可以假设第二，第三，第四个隐藏层的神经元是为检测下列图片是否存在：</p>
<p><img src="/images/c1s4-6.png" width="20%" height="10%"></p>
<p>正如你所猜到的那样，这四个图像拼到一起就组成了数字$0$ ：</p>
<p><img src="/images/c1s4-7.png" width="10%" height="10%"></p>
<p>如果所有这四个隐藏层的神经元被激活那么我们就可以推断出这个数字是$0$。当然，这不是我们推断出$0$的<strong>唯一</strong>方式——我们能通过很多其他合理的方式得到$0$ （举个例子来说，通过上述图像的转换，或者稍微变形）。但至少在这个例子中我们可以推断出输入的数字是$0$ 。</p>
<p>假设神经网络以上述方式运行，我们可以给出一个貌似合理的理由去解释为什么用$10$个输出而不是$4$个。如果我们有$4$个输出，那么第一个输出神经元将会尽力去判断数字的最高有效位是什么。把数字的最高有效位和数字的形状联系起来并不是一个简单的问题。很难想象出有什么恰当的历史原因，一个数字的形状要素会和一个数字的最高有效位有什么紧密联系。</p>
<p>上面我们说的只是一个启发性的思考。没有什么理由表明这个三层的神经网络必须按照我所描述的方式运行，即隐藏层是用来探测数字的组成形状。可能一个聪明的学习算法将会找到一些合适的权重能让我们仅仅用$4$个输出神经元就行。但是这个启发式的思考通常很有效，它会节省你大量时间去设计一个好的神经网络结构。</p>
<p><strong>练习</strong></p>
<ul>
<li>通过在上述的三层神经网络加一个额外的一层就可以实现按位表示数字。额外的一层把原来的输出层转化为一个二进制表示，如下图所示。为新的输出层寻找一些合适的权重和偏移。假定原先的$3$层神经网络在第三层得到正确输出（即原来的输出层）的激活值至少是$0.99$，得到错误的输出的激活值至多是$0.01$。</li>
</ul>
<p><img src="/images/c1s4-8.png" width="60%" height="50%"></p>
<h2 id="通过梯度下降法学习参数"><a href="#通过梯度下降法学习参数" class="headerlink" title="通过梯度下降法学习参数"></a>通过梯度下降法学习参数</h2><p>我们已经设计出了我们的神经网络结构，现在的问题是，它要如何学习来识别数字呢？首先，我们需要一个待学习的数据集——即所谓的训练数据集。我们将会使用<a href="http://yann.lecun.com/exdb/mnist/" target="_blank" rel="noopener">MNIST数据集</a>，其中包含了成千上万个扫描手写数字图像以及它们正确的分类。MNIST数据集是由<a href="http://en.wikipedia.org/wiki/National_Institute_of_Standards_and_Technology" target="_blank" rel="noopener">NIST</a>（the United States’ National Institute of Standards and Technology，美国国家标准与技术研究院）收集了两个数据集后修改得到的子数据集构成，所以取名叫MNIST。下面是MNIST中的一些图像：</p>
<p><img src="/images/c1s5-1.png" width="60%" height="10%"></p>
<p>正如你所见，这些数字实际上就是我们在本章开始所提到的图像。当然，当我们测试我们的神经网络时，我们需要让它去识别训练集之外的数据。</p>
<p>MNIST数据集可以分为两个部分。第一个部分包含了60,000个用于训练的图像。这些图像是扫描了250个人的手写数字，他们之中一半是来自美国人口普查局的员工，另一半是高中生。这些图像是28乘28个像素点灰度图像。MNIST的第二个部分是10,000个用于测试的图像。同样，它们也是28乘28的灰度图像。我们将会用这些测试数据去评估我们的神经网络的识别效果。为了保证测试结果的准确性，这些测试数据是采样于另一个<em>不同的</em>250人的团体（尽管这些人也是美国人口普查局的员工和高中生）。这保证了我们的系统能够识别训练集之外的手写数字图像。</p>
<p>我们用$x$去定义训练输入。为了方便起见，我们将每一个训练输入$x$视为一个$28 \times 28 = 784$维的向量。向量中的一个元素代表图像的一个像素点的灰度值。我们定义输出为$y = y(x)$，每一个$y$是一个$10$维的向量。举例来说，对于一个数字$6$的训练图像$x$，我们期待的输出是$y(x) = (0, 0, 0, 0, 0, 0, 1, 0, 0, 0)^T$。这里的$T$是转置符号，它能将一个行向量变为列向量。</p>
<p>现在我们需要一个算法能让我们找到合适的权重和偏置，从而对所有的训练输入为$x$的输出都近似于$y(x)$。为了衡量我们当前取得的结果距离目标结果的好坏程度，我们定义一个<em>代价函数</em>：</p>

\begin{eqnarray} C(w,b) \equiv \frac{1}{2n} \sum_x \| y(x) - a\|^2. \tag{6}\end{eqnarray} 

<p>公式里面的$w$表示所有的权重的集合，$b$表示所有的偏置，$n$是训练数据的个数，$a$代表$x$作为输入时输出层的向量，求和针对所有的训练输入$x$。显而易见，输出$a$取决于$x$, $w$和$b$，但是为了保持公式的简洁性，我们没有明确指出这种依赖性。符号$| v |$是指求向量$v$的范数。这时，我们称$C$为二次代价函数；有时我们也称它为<em>平方误差</em>或者<em>MSE</em>。通过代价函数的形式我们可以得知$C(w,b)$是非负的，因为加和的每一项都是非负的。另外，当所有的训练输入$x$的输出$a$基本都等于$y(x)$时，代价函数$C(w,b)$的值会变小，可能有$C(w,b) \approx 0$。因此如果我们的学习算法可以找到合适的权重和偏置使得$C(w,b) \approx 0$，那么这就是一个好的学习算法。反过来来说，如果$C(w,b)$很大，那么就说明学习算法的效果很差——这意味着对于大量的输入，我们的结果$a$与正确结果$y(x)$相差很大。因此，我们的训练算法的目标就是通过调整函数的权重和偏置来最小化代价函数$C(w,b)$。换句话说，我们想寻找合适的权重和偏置让代价函数尽可能地小。我们将使用一个叫做<em>梯度下降法（gradient descent）</em>的算法来达到这个目的。</p>
<p>为什么要介绍平方代价（quadratic cost）呢？毕竟我们最初所感兴趣的内容不是对图像正确地分类么？为什么不增大正确输出的得分，而是去最小化一个像平方代价类似的间接评估呢？这么做是因为在神经网络中，被正确分类的图像的数量关于权重、偏置的函数并不是一个平滑的函数。大多数情况下，对权重和偏置做出的微小变动并不会影响被正确分类的图像的数量。这会导致我们很难去刻画如何去优化权重和偏置才能得到更好的结果。一个类似平方代价的平滑代价函数能够更好地指导我们如何去改变权重和偏置来达到更好的效果。这就是为何我们集中精力去最小化平方代价，只有通过这种方式我们才能让分类器更精确。</p>
<p>即使知道了我们需要选择一个平滑的代价函数，你可能仍然会好奇为什么我们选择方程（6）中的二次函数。这是一个拍脑袋的决定么？是不是我们选择另一个不同的代价函数将会得到完全不同的权重和偏置呢？这是一个合理的问题，稍后我们会再次提到这个代价函数并做一些修改。尽管如此，方程（6）中的平方代价函数能让我们更好地理解神经网络的基本原理，因此我们目前先用它。</p>
<p>再次回顾一下，我们训练神经网络的目的是寻找合适的权重和偏置来最小化平方代价函数$C(w, b)$。这是一个明确定义了的问题，但是现在还存在很多让我们分散精力的部分——对权重$w$和偏置$b$合理的解释，隐藏在背后的$\sigma$函数，神经网络结构的选择，<em>MNIST</em>等等。实际上我们可以先忽略这些问题，把精力集中在最小化的问题上。现在让我们忘掉代价函数的具体形式，忘掉神经网络的结构，忘掉其它一切。现在想象我们仅仅是要去最小化一个给定的有很多变量的函数。我们即将要介绍一种可以解决最小化问题的技术，称作<em>梯度下降法</em>。然后我们再回到要在神经网络中最小化的函数上来。</p>
<p>假定我们要最小化某些函数，$C(v)$。它可能是任意的多元实值函数，$v = v_1, v_2, \ldots$。注意我们将会用$v$去代表$w$和$b$以强调它可能是任意的函数——我们不会把问题局限在神经网络中。假定$C(v)$有两个变量$v_1$ 和$v_2$ ：</p>
<p><img src="/images/c1s5-2.png" width="50%" height="45%"></p>
<p>我们的目标就是找到$C$在何处取得全局最小值。当然，对于上图的函数，我们能通过肉眼就可以找到最小值。这是因为我所展示的函数<em>太</em>简单了！一个一般的函数$C$，可能是一个复杂的多元函数，通过肉眼通常找不到它的最小值。</p>
<p>一种解决方法就是用数值计算的方法去计算出它的最小值。我们可以计算出偏导，利用偏导去寻找函数$C$的极值点。运气好的话我们的函数$C$只有一个或者少数的几个变量。但是变量过多的话将是一个噩梦。在神经网络中我们通常会需要<em>多得多得多</em>的变量——最大的神经网络的代价函数包含了数亿个权重和偏置。根本没法使用数值计算的方法！</p>
<p>（我们只讨论了$C$只有两个变量的情况，如果我说「嗨，如果函数有多于两个变量怎么办？」。对于这种情况我只能说很抱歉。请相信我把$C$看成一个二元函数是有助于我们的理解的。善于思考数学通常也包含善于利用多种直观的图片，学习什么时候用什么图片合适。）</p>
<p>所以，数值计算的方法没法完成这个任务了。幸运的是，有一个漂亮的类比预示着有一种算法能得到很好的效果。首先把我们的函数想象成一个山谷。我们想象有一个小球从山谷的斜坡滚落下来。我们的日常经验告诉我们这个球终会达到谷底。也许我们可以用这种思想来解决函数最小值的问题？我们随机地为小球选取一个起点，然后开始模拟小球滚落到谷底的运动过程。我们可以简单的通过计算$C$的导数（或者二阶导数）来模拟这个过程——这些导数将会告诉我们关于山谷「地形」的一切，从而告诉我们的小球该如何下落。</p>
<p>基于上述的想法，你可能会认为我们会写下牛顿运动定理，考虑摩擦力、重力等等。其实我们不打算真的去实现这个小球的类比——我们只想要一个算法去最小化$C$，而不是真的去模拟它真实的物理定律。小球的视角能激发我们的想象而不是束缚我们的思维。因此与其去考虑麻烦的物理定律，不如我们这样问自己：如果我们扮演一天的上帝，能够构造自己的物理定律，能够支配小球的下落方式，那么我们将会采取什么样的物理定律来让小球能够始终滑落到谷底呢？</p>
<p>为了更精确地描述这个问题，让我们想象一下如果我们在$v_1$方向移动一个很小的量$\Delta v_1$并在$v_2$方向移动一个很小的量$\Delta v_2$将会发生什么呢。通过计算可以告诉我们$C$将会产生如下改变：</p>

\begin{eqnarray} \Delta C \approx \frac{\partial C}{\partial v_1} \Delta v_1 + \frac{\partial C}{\partial v_2} \Delta v_2. \tag{7}\end{eqnarray}

<p>我们将要寻找一种方式去选择$\Delta v_1$和$\Delta v_2$使得$\Delta C$为负，因为负值能够让小球下落。为了搞清楚如何选择，我们有必要定义$\Delta v$，它是描述$v$变化的向量，$\Delta v \equiv (\Delta v_1, \Delta v_2)^T$，$T$是转置符号。我们也定义$C$用来表示偏导的向量，$\left(\frac{\partial C}{\partial v_1}, \frac{\partial C}{\partial v_2}\right)^T$。我们用$\nabla C$来表示梯度向量：</p>

\begin{eqnarray} \nabla C \equiv \left( \frac{\partial C}{\partial v_1}, \frac{\partial C}{\partial v_2} \right)^T. \tag{8}\end{eqnarray}

<p>待会我们将用$\Delta v$和$\nabla C$来重写$\Delta C$。在这之前我想先说明一些令人困惑的关于梯度的事情。当第一次碰到梯度的符号$\nabla C$时，人们常会好奇为什么$\nabla C$符号是这样。$\nabla$究竟代表什么？事实上你可以把梯度仅仅看做一个简单的数学记号——一个方便用来表示偏导的向量——这样我们就不必写两个符号了。这样来看，$\nabla$仅仅是一面旗帜，它告诉你：「嗨，$\nabla C$是一个梯度向量」。也有很多其他的关于$\nabla$的解释（比如，作为一种微分符号），但我们不需要这种观点。</p>
<p>定义好上述符号之后，关于$\Delta C$的表达式（7）能被重写成如下形式</p>

\begin{eqnarray} \Delta C \approx \nabla C \cdot \Delta v. \tag{9}\end{eqnarray}

<p>这个表达式能很好地解释为什么$\nabla C$被称作梯度向量：$\nabla C$和$C$中$v$的变化密切相关，只是我们把它称为梯度罢了。但是这个式子真正让我们兴奋的是，它让我们看到了如何选取$\Delta v$才能让$\Delta C$为负数。假设我们选取</p>

\begin{eqnarray} \Delta v = -\eta \nabla C, \tag{10}\end{eqnarray}

<p>这里的$\eta$是个很小的正数（就是我们熟知的<em>学习速率（learning rate）</em>）。等式（9）告诉我们$\Delta C \approx -\eta \nabla C \cdot \nabla C = -\eta |\nabla C|^2$。由于$| \nabla C |^2 \geq 0$，这保证了$\Delta C \leq 0$，例如，如果我们以等式（10）的方式去改变$v$，那么$C$将一直会降低，不会增加。（当然，要在（9）式的近似约束下）。这就是我们想要的特性！因此我们把（10）式看做梯度下降算法的「运动定律」。也就是说，我们用（10）式计算$\Delta v$，把小球位置$v$移动：</p>

\begin{eqnarray} v \rightarrow v' = v -\eta \nabla C. \tag{11}\end{eqnarray}

<p>然后我们可以迭代地去更新。如果我们反复那么做，那么$C$会一直降低到我们想要寻找的全局最小值。</p>
<p>总结一下，梯度下降算法工作的方式就是重复计算梯度$\nabla C$，然后沿着梯度的<em>反</em>方向运动，即沿着山谷的斜坡下降。就像下图一样：</p>
<p><img src="/images/c1s5-3.png" width="50%" height="45%"></p>
<p>需要注意这种梯度下降的规则不代表真实的物理运动。在真实世界里，小球有势能，势能让小球可以在山谷斜坡上滚动，甚至顷刻间滚向山顶。只有加上摩擦力的影响才能保证小球会下落到谷底。我们选择$\Delta v$的规则不一样，我们就好像在命令「立马下降！」。这仍然是一个寻找最小值的好办法！</p>
<p>为了使我们的梯度下降法能够正确地工作，我们需要选择足够小的学习速率$\eta$使得等式（9）能得到很好的近似。如果不那么做，我们将会以$\Delta C &gt; 0$结束，这显然不是一个很好的结果。与此同时，我们也不能把$\eta$设得过小，因为如果$\eta$过小，那么梯度下降算法就会变得异常缓慢。在真正的实现中，$\eta$通常是变化的，从而方程（9）能保持很好地近似度同时保证算法不会太慢。我们稍后会看这是如何工作的。</p>
<p>我已经解释了具有两个变量的函数$C$的梯度下降法。但事实上当$C$是其他多元函数时也能很好地运行。我们假设$C$是一个有$m$个变量$v_1,\ldots,v_m$的多元函数。我们对自变量做如下改变$\Delta v = (\Delta v_1, \ldots, \Delta v_m)^T$，那么$\Delta C$将会变为</p>

\begin{eqnarray} \Delta C \approx \nabla C \cdot \Delta v, \tag{12}\end{eqnarray}

<p>这里的$\nabla C$是<br>
\begin{eqnarray} \nabla C \equiv \left(\frac{\partial C}{\partial v_1}, \ldots, \frac{\partial C}{\partial v_m}\right)^T. \tag{13}\end{eqnarray}
</p>
<p>正如两个变量的例子一样，我们可以选取</p>

\begin{eqnarray} \Delta v = -\eta \nabla C, \tag{14}\end{eqnarray}

<p>并且我们也会保证公式（12）中$\Delta C$是负数。这使得我们能够随着梯度得到函数的最小值，即使$C$是任意的多元函数，我们也能重复运用下面的规则</p>

\begin{eqnarray} v \rightarrow v' = v-\eta \nabla C. \tag{15}\end{eqnarray}

<p>你可以把这个更新规则看作梯度下降算法的定义。这给我们提供了一种方式去通过重复改变$v$来找到函数$C$的最小值。然而这种方式并不总是有效的——有几种情况能导致错误的发生，使得我们无法从梯度得到函数$C$的全局最小值，这种情况我们会在后面的章节中讨论。但在实践中，梯度下降法通常效果非常好，在神经网络中这是一种非常有效的方式去求代价函数的最小值，从而帮助神经网络学习。</p>
<p>事实上，有一种观念认为梯度下降法是求最小值的最优策略。我们假设努力去改变$\Delta v$来让$C$尽可能地减小，减小量为$\Delta C \approx \nabla C \cdot \Delta v$。我们首先限制步长为固定值，即$| \Delta v | = \epsilon$ ，$\epsilon &gt; 0$。当步长固定时，我们要找到使得$C$减小最大的下降方向。可以证明，使得$\nabla C \cdot \Delta v$取得最小值的$\Delta v$为$\Delta v = - \eta \nabla C$，这里$\eta = \epsilon / |\nabla C|$是由步长限制$|\Delta v| = \epsilon$所决定的。因此，梯度下降法可以被视为一种通过在$C$下降最快的方向上做微小变化来使得$C$立即下降的方法。</p>
<p><strong>练习</strong></p>
<ul>
<li><p>证明最后一段的断言。提示：利用<a href="http://en.wikipedia.org/wiki/Cauchy%E2%80%93Schwarz_inequality" target="_blank" rel="noopener">柯西-斯瓦茨不等式</a>。</p>
</li>
<li><p>我已经解释了当$C$是二元及其多元函数的情况。那如果$C$是一个单变量的函数呢？你能给出梯度下降法在一元函数的几何解释么？</p>
</li>
</ul>
<p>人们已经研究出很多梯度下降法的变种，其中还包括一些去真正模拟真实物理运动的做法。这些模拟小球的变种有很多优点，但是也有一个主要的缺点：这些方法必须去计算$C$的二阶偏导，这可能代价非常大。为了理解为什么这种做法代价高，假设我们想求所有的二阶偏导$\partial^2 C/ \partial v_j \partial v_k$。如果我们有上百万的变量$v_j$，那我们必须要计算数万亿级别的二阶偏导！这的确会造成很大的计算代价。不过也有一些避免这些问题的技巧，寻找梯度下降方法的替代品也是个很活跃的研究领域。但在这本书中我们将主要用梯度下降法（包括变种）去训练我们的神经网络。</p>
<p>我们在神经网络中应用梯度下降法呢？方式就是利用梯度下降法去寻找权重$w_k$和偏置$b_l$ 能减小（6）式中的代价函数。为了弄清楚具体是如何工作的，我们将用权重和偏置代替变量$v_j$来重新描述梯度下降算法的更新规则。我们描述「位置」的元素是$w_k$和$b_l$，梯度向量$\nabla C$对应$\partial C / \partial w_k$和$\partial C / \partial b_l$。申明了梯度下降中成分之后，我们得到</p>

\begin{eqnarray} w_k & \rightarrow & w_k' = w_k-\eta \frac{\partial C}{\partial w_k} \tag{16}\\ b_l & \rightarrow & b_l' = b_l-\eta \frac{\partial C}{\partial b_l}. \tag{17}\end{eqnarray}

<p>通过重复更新我们能「滚到山底」，从而有望能找到代价函数的最小值。换句话说，这个规则能用在神经网络的学习中。</p>
<p>应用梯度下降规则有很多挑战。我们将在下一章深入讨论。但是现在我想提及一个问题。为了理解问题是什么，我们先回顾（6）中的二次代价函数。注意这个代价函数有着如下的形式$C = \frac{1}{n} \sum_x C_x$，也就是说，它是每个样本代价$C_x \equiv \frac{|y(x)-a|^2}{2}$的平均值。事实上，为了计算梯度$\nabla C$，我们需要为每个训练样本$x$单独地计算梯度值$\nabla C_x$，然后求平均值，$\nabla C = \frac{1}{n} \sum_x \nabla C_x$。不幸的是，当训练输入过大时会花费很长时间，这样会使学习变得相当缓慢。</p>
<p>有种叫做<em>随机梯度下降（SGD）</em>的算法能够用来加速学习过程。想法就是通过随机选取小量输入样本来计算$\nabla C_x$，进而可以计算$\nabla C$。采取少量样本的平均值可以快速地得到梯度$\nabla C$，这会加速梯度下降过程，进而加速学习过程。</p>
<p>更准确地说，SGD是随机地选取小量的$m$个训练数据。我们将选取的这些训练数据标号$X_1, X_2, \ldots, X_m$，并把它们称为一个<em>mini-batch</em>。我们选取的$m$要足够大才能保证$\nabla C_{X_j}$的平均值才能接近所有样本的平均值$\nabla C_x$，也就是说，  </p>

\begin{eqnarray} \frac{\sum_{j=1}^m \nabla C_{X_{j}}}{m} \approx \frac{\sum_x \nabla C_x}{n} = \nabla C, \tag{18}\end{eqnarray}

<p>其中，第二个求和是针对整个训练数据集的。交换两边我们可以得到</p>

\begin{eqnarray} \nabla C \approx \frac{1}{m} \sum_{j=1}^m \nabla C_{X_{j}}, \tag{19}\end{eqnarray}

<p>证实了我们可以通过计算随机选取的mini-batch的梯度来估计整体的梯度。</p>
<p>为了将这种想法更明确地联系到神经网络的学习中来，假设用$w_k$和$b_l$分别代表权重和偏置。SGD就是随机地选取大小为一个mini-batch的训练数据，然后去训练这些数据，</p>

\begin{eqnarray} w_k & \rightarrow & w_k' = w_k-\frac{\eta}{m} \sum_j \frac{\partial C_{X_j}}{\partial w_k} \tag{20}\\ b_l & \rightarrow & b_l' = b_l-\frac{\eta}{m} \sum_j \frac{\partial C_{X_j}}{\partial b_l}, \tag{21}\end{eqnarray}

<p>这个和就是当前mini-batch中每个$X_j$的梯度值。然后我们再随机选取另一个mini-batch去训练，直到我们用完了所有的训练数据，这就完成了<em>一轮（epoch）</em>迭代训练。这样我们就会重新开始下一轮迭代。</p>
<p>值得一提的是，关于调节代价函数和mini-batch去更新权重和偏置有很多不同的约定。在（6）式中，我们通过因子$\frac{1}{n}$来缩放全部的代价函数。人们通常忽略$\frac{1}{n}$，直接计算单独训练样本代价之和，而不是求平均值。这对我们不能提前知道训练数据集大小的情况下特别有效。这可能发生在有更多的训练数据是实时加入的情况下。同样，mini-batch的更新规则（20）和（21）有时也会舍弃$\frac{1}{m}$。从概念上这也会有一点区别，因为它等价于重新调节学习速率$\eta$。但在对不同工作进行详细比较时，它是值得关注的。</p>
<p>我们能把SGD想象成一场民主选举：使用小规模的mini-batch计算梯度，比使用整个训练集计算梯度容易得多，就如开展一次民意调查比举行一次完整的选举容易得多。举个例子，在<em>MNIST</em>中有$60,000$个测试数据，我们选取mini-batch的大小为$m = 10$，这样，计算梯度的过程就加速了$6,000$倍！当然，这个估计可能并不准确——仍然会存在统计波动——但也没必要准确：我们只关心在某个方向上移动可以减少$C$，这意味着我们没必要准确去计算梯度的精确值。事实上，SGD在神经网络的学习中被广泛使用的十分有效的技术，它也是本书中展开的大多数学习技术的基础。</p>
<p><strong>练习</strong></p>
<ul>
<li>梯度下降一个比较极端的版本就是让mini-batch的大小变为1。这就是说，给定一个输入$x$，我们通过规则$w_k \rightarrow w_k’ = w_k - \eta \partial C_x / \partial w_k$ 和 $b_l \rightarrow b_l’ = b_l - \eta \partial C_x / \partial b_l$更新权重和偏置。当我们选取另一个训练数据时也做同样的处理。其它的训练数据也做相同的处理。这种处理方式就是<em>在线学习（online learning）</em>或<em>增量学习（incremental learning）</em>。在在线学习中，神经网络在一个时刻只学习一个训练数据（正如人类的做法）。说出online learning相比mini-batch为20的SGD的一个缺点和一个优点。</li>
</ul>
<p>让我们讨论一个令刚刚接触梯度下降的人困惑的问题，来总结这部分的内容。在神经网络中，代价函数$C$是一个关于所有权重和偏置的多元函数，因此在某种意义上来说，就是在一个高维空间定义了一个平面。有些人可能会那么想：「嗨，我必须要看见其它多出的维度」。他们会开始疑惑：「我不能想象出四维空间，更不用说五维（或者五百万维）」。是不是他们缺少某种只有超级数学家才有的超能力？当然不是。即使大多数专业的数学家也不能想象出四维空间的样子。他们会用一些其它的方式来表示什么在发生。正如我们上面所做的那样，我们可以用代数（而不是几何）来表示$\Delta C$如何变化才能让$C$减少。善于思考高维的人会在脑中做一些思想实验；我们的代数技巧是其中一个例子。这些方法可能不如观察三维那么直观，但我们熟悉这些方法之后会帮助我们思考更高维度。我不想在这里详细展开，如果你感兴趣，你可以读一下这篇关于数学家如何思考高维空间的<a href="http://mathoverflow.net/questions/25983/intuitive-crutches-for-higher-dimensional-thinking" target="_blank" rel="noopener">讨论</a>。我们讲的一些技术可能会有点复杂，但大多数内容还是比较直观的，任何人都能熟练掌握。</p>
<h2 id="利用神经网络解决手写数字识别"><a href="#利用神经网络解决手写数字识别" class="headerlink" title="利用神经网络解决手写数字识别"></a>利用神经网络解决手写数字识别</h2><p>好了，我们来利用梯度下降和MNIST训练集来写一段程序识别手写数字。我们将用短短的74行Python（2.7）代码来完成这个任务。第一步就是获得MNIST数据。如果你是一个git用户，那么你能够clone这本书的代码仓库，<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git <span class="built_in">clone</span> https://github.com/mnielsen/neural-networks-and-deep-learning.git</span><br></pre></td></tr></table></figure></p>
<p>如果你不用git，那么可以在<a href="https://github.com/mnielsen/neural-networks-and-deep-learning/archive/master.zip" target="_blank" rel="noopener">链接</a>下载。  </p>
<p>我之前描述过，MNIST数据集包含60,000张训练图片和10,000张测试图片。这是MNIST的官方描述。实际上我们将要对数据稍微处理一下。我们保持测试集不变，但把60,000张MNIST训练集分为两部分：50,000张作为训练集，10,000张作为开发集（validation set）。我们这一章讲不会用到开发集，但是在书的后面章节我们会介绍利用开发集来调整超参数。当我提到“MNIST测试集”时，我是指那50,000张图片而不是原始的60,000张图片。  </p>
<p>除了MNIST数据集我们也需要一个Python库：<a href="http://numpy.org/" target="_blank" rel="noopener">Numpy</a>,用于做快速的线性计算。如果你没有安装Numpy，你能从<a href="http://www.scipy.org/install.html" target="_blank" rel="noopener">这里</a>获得。  </p>
<p>我先解释神经网络的核心代码。最重要的是神经网络的类。下面是类初始化：  </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Network</span><span class="params">(object)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, sizes)</span>:</span></span><br><span class="line">        self.num_layers = len(sizes)</span><br><span class="line">        self.sizes = sizes</span><br><span class="line">        self.biases = [np.random.randn(y, <span class="number">1</span>) <span class="keyword">for</span> y <span class="keyword">in</span> sizes[<span class="number">1</span>:]]</span><br><span class="line">        self.weights = [np.random.randn(y, x) <span class="keyword">for</span> x, y <span class="keyword">in</span> zip(sizes[:<span class="number">-1</span>], sizes[<span class="number">1</span>:])]</span><br></pre></td></tr></table></figure>
<p>在这份代码中，list的大小代表神经网络的层数。例如我们想创建一个神经网络的实例，其中第一层包含2个神经元，第二层包含3个神经元，最后一层包含1个神经元，那么我们就能这么写代码：  </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">net = Network([<span class="number">2</span>,<span class="number">3</span>,<span class="number">1</span>])</span><br></pre></td></tr></table></figure>
<p>权重和偏置是利用Numpy的随机初始化函数<code>np.random.randn</code>来产生均值为0，标准差为1的高斯分布。我们的梯度下降法将从这个随机初始化开始。在后面的章节中，我们会找到一些更好的初始化方法，但目前我们先不介绍。注意初始化代码默认第一层是输入层并且没有设置第一层神经元的偏置，这是因为偏置仅仅用于计算后面层的输出。  </p>
<p>另外，偏置和权重存储在Numpy矩阵中。举个例子， <code>net.weight[1]</code>存储着第二层神经元到第三层神经元的权重。（它并不是第一层到第二层之间的权重，因为Python的列表开始下标是0。）<code>net.weight[1]</code>有些显得啰嗦，我们直接记做矩阵$w$。那么$w_{jk}$表示第二层的第$k^{\rm th}$个神经元到第三层的第$j^{\rm th}$个神经元的权重。$j$和$k$顺序可能看起来很奇怪，但着只是为了让激活函数写起来更优雅：  </p>

\begin{eqnarray} 
  a' = \sigma(w a + b).
\tag{22}\end{eqnarray}

<p>有了这些，我们能够很容易从一个神经网络的实例中计算它的输出。我们先定义好<em>sigmoid</em>函数：  </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sigmoid</span><span class="params">(z)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">1.0</span>/(<span class="number">1.0</span>+np.exp(-z))</span><br></pre></td></tr></table></figure>
<p>注意，输入z是一个向量，Numpy会自动地把sigmoid函数应用到每个元素上。<br>接下来，我们为神经网络加上前馈神经网络的方法，当我们给出神经网络的输入a就能返回对应的输出。就是将等式（22）应用到每一层：  </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">feedforward</span><span class="params">(self, a)</span>:</span></span><br><span class="line">    <span class="string">"""Return the output of the network if "a" is input."""</span></span><br><span class="line">    <span class="keyword">for</span> b, w <span class="keyword">in</span> zip(self.biases, self.weights):</span><br><span class="line">        a = sigmoid(np.dot(w, a)+b)</span><br><span class="line">    <span class="keyword">return</span> a</span><br></pre></td></tr></table></figure>
<p>对我们神经网络最重要的事就是学习。为了达到这个目的，我们需要加上一个应用到随机梯度下降法的SGD方法。下面是代码。有些地方可能有点神秘，但我会在下面慢慢讲解。  </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">SGD</span><span class="params">(self, training_data, epochs, mini_batch_size, eta,test_data=None)</span>:</span></span><br><span class="line">     <span class="string">"""Train the neural network using mini-batch stochastic</span></span><br><span class="line"><span class="string">     gradient descent.  The "training_data" is a list of tuples</span></span><br><span class="line"><span class="string">     "(x, y)" representing the training inputs and the desired</span></span><br><span class="line"><span class="string">     outputs.  The other non-optional parameters are</span></span><br><span class="line"><span class="string">     self-explanatory.  If "test_data" is provided then the</span></span><br><span class="line"><span class="string">     network will be evaluated against the test data after each</span></span><br><span class="line"><span class="string">     epoch, and partial progress printed out.  This is useful for</span></span><br><span class="line"><span class="string">     tracking progress, but slows things down substantially."""</span></span><br><span class="line">     <span class="keyword">if</span> test_data: n_test = len(test_data)</span><br><span class="line">     n = len(training_data)</span><br><span class="line">     <span class="keyword">for</span> j <span class="keyword">in</span> xrange(epochs):</span><br><span class="line">         random.shuffle(training_data)</span><br><span class="line">         mini_batches = [</span><br><span class="line">             training_data[k:k+mini_batch_size]</span><br><span class="line">             <span class="keyword">for</span> k <span class="keyword">in</span> xrange(<span class="number">0</span>, n, mini_batch_size)]</span><br><span class="line">         <span class="keyword">for</span> mini_batch <span class="keyword">in</span> mini_batches:</span><br><span class="line">             self.update_mini_batch(mini_batch, eta)</span><br><span class="line">         <span class="keyword">if</span> test_data:</span><br><span class="line">             <span class="keyword">print</span> <span class="string">"Epoch &#123;0&#125;: &#123;1&#125; / &#123;2&#125;"</span>.format(</span><br><span class="line">                 j, self.evaluate(test_data), n_test)</span><br><span class="line">         <span class="keyword">else</span>:</span><br><span class="line">             <span class="keyword">print</span> <span class="string">"Epoch &#123;0&#125; complete"</span>.format(j)</span><br></pre></td></tr></table></figure>
<p>训练数据是tuple(x,y)，内容分别对应着输入和期望的输出。变量<code>epochs</code>和<code>mini-batch-size</code>分别代表训练时迭代的轮次和sample时一个batch的大小。eta是学习率，$\eta$。如果可选参数<code>test_data</code>被提供了，那么程序将会字每次迭代之后就会评估结果，打印出我们部分的过程。这对我们追踪过程有帮助，但是会明显降低程序的速度。  </p>
<p>代码的工作方式如下。在每轮迭代时，首先开始随机打乱训练数据，然后将它分为合适大小的mini-batches。这是一种简单的打乱数据的方式。对于每个mini-batch我们都会应用梯度下降法。这是通<code>self.update_mini_batch(mini_batch, eta)</code>,这会更新神经网络的权重和偏置。下面是<code>update_mini_batch</code>的代码：  </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">update_mini_batch</span><span class="params">(self, mini_batch, eta)</span>:</span></span><br><span class="line">    <span class="string">"""Update the network's weights and biases by applying</span></span><br><span class="line"><span class="string">    gradient descent using backpropagation to a single mini batch.</span></span><br><span class="line"><span class="string">    The "mini_batch" is a list of tuples "(x, y)", and "eta"</span></span><br><span class="line"><span class="string">    is the learning rate."""</span></span><br><span class="line">    nabla_b = [np.zeros(b.shape) <span class="keyword">for</span> b <span class="keyword">in</span> self.biases]</span><br><span class="line">    nabla_w = [np.zeros(w.shape) <span class="keyword">for</span> w <span class="keyword">in</span> self.weights]</span><br><span class="line">    <span class="keyword">for</span> x, y <span class="keyword">in</span> mini_batch:</span><br><span class="line">        delta_nabla_b, delta_nabla_w = self.backprop(x, y)</span><br><span class="line">        nabla_b = [nb+dnb <span class="keyword">for</span> nb, dnb <span class="keyword">in</span> zip(nabla_b, delta_nabla_b)]</span><br><span class="line">        nabla_w = [nw+dnw <span class="keyword">for</span> nw, dnw <span class="keyword">in</span> zip(nabla_w, delta_nabla_w)]</span><br><span class="line">    self.weights = [w-(eta/len(mini_batch))*nw </span><br><span class="line">                    <span class="keyword">for</span> w, nw <span class="keyword">in</span> zip(self.weights, nabla_w)]</span><br><span class="line">    self.biases = [b-(eta/len(mini_batch))*nb </span><br><span class="line">                   <span class="keyword">for</span> b, nb <span class="keyword">in</span> zip(self.biases, nabla_b)]</span><br></pre></td></tr></table></figure>
<p>大部分工作是由下面的这行完成：  </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">delta_nabla_b, delta_nabla_w = self.backprop(x, y)</span><br></pre></td></tr></table></figure>
<p>这其中包含着反向传播算法，这是一种快速计算代价函数梯度的算法。因此<code>update_mini_batch</code>的工作只是通过计算mini_bach的每个训练样本的梯度然后更新<code>self.weights</code>和<code>self.biases</code>。  </p>
<p>我不打算现在展示<code>self.backprop</code>的代码。我们将在下一节反向传播算法，包括<code>self.backprop</code>的代码。目前，我们就把它当成一个黑箱子就可以了，它能够计算每个训练样本的梯度。  </p>
<p>让我们现在看看完整的程序，包括注释。除了<code>self.backprop</code>之外，程序的其他部分都是很好解释的，大部分的工作都是<code>self.SGD</code>和<code>self.update_mini_batch</code>完成的。<code>self.backprop</code>用了一些额外的函数来辅助计算梯度，<code>sigmoid_prime</code>是用来计算$\sigma$函数的梯度，而关于<code>self.cost_derivative</code>函数我在这里不会讲。你能够从下面的代码和注释看懂程序的流程。我将会在下一章中讨论一些细节。虽然代码篇幅看起来很长，但是去掉注释也就只有74行。你能够从<a href="https://github.com/mnielsen/neural-networks-and-deep-learning/blob/master/src/network.py" target="_blank" rel="noopener">Github</a>获取全部的代码。  </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">network.py</span></span><br><span class="line"><span class="string">​~~~~~~~~~~</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">A module to implement the stochastic gradient descent learning</span></span><br><span class="line"><span class="string">algorithm for a feedforward neural network.  Gradients are calculated</span></span><br><span class="line"><span class="string">using backpropagation.  Note that I have focused on making the code</span></span><br><span class="line"><span class="string">simple, easily readable, and easily modifiable.  It is not optimized,</span></span><br><span class="line"><span class="string">and omits many desirable features.</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#### Libraries</span></span><br><span class="line"><span class="comment"># Standard library</span></span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"></span><br><span class="line"><span class="comment"># Third-party libraries</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Network</span><span class="params">(object)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, sizes)</span>:</span></span><br><span class="line">        <span class="string">"""The list ``sizes`` contains the number of neurons in the</span></span><br><span class="line"><span class="string">        respective layers of the network.  For example, if the list</span></span><br><span class="line"><span class="string">        was [2, 3, 1] then it would be a three-layer network, with the</span></span><br><span class="line"><span class="string">        first layer containing 2 neurons, the second layer 3 neurons,</span></span><br><span class="line"><span class="string">        and the third layer 1 neuron.  The biases and weights for the</span></span><br><span class="line"><span class="string">        network are initialized randomly, using a Gaussian</span></span><br><span class="line"><span class="string">        distribution with mean 0, and variance 1.  Note that the first</span></span><br><span class="line"><span class="string">        layer is assumed to be an input layer, and by convention we</span></span><br><span class="line"><span class="string">        won't set any biases for those neurons, since biases are only</span></span><br><span class="line"><span class="string">        ever used in computing the outputs from later layers."""</span></span><br><span class="line">        self.num_layers = len(sizes)</span><br><span class="line">        self.sizes = sizes</span><br><span class="line">        self.biases = [np.random.randn(y, <span class="number">1</span>) <span class="keyword">for</span> y <span class="keyword">in</span> sizes[<span class="number">1</span>:]]</span><br><span class="line">        self.weights = [np.random.randn(y, x)</span><br><span class="line">                        <span class="keyword">for</span> x, y <span class="keyword">in</span> zip(sizes[:<span class="number">-1</span>], sizes[<span class="number">1</span>:])]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">feedforward</span><span class="params">(self, a)</span>:</span></span><br><span class="line">        <span class="string">"""Return the output of the network if ``a`` is input."""</span></span><br><span class="line">        <span class="keyword">for</span> b, w <span class="keyword">in</span> zip(self.biases, self.weights):</span><br><span class="line">            a = sigmoid(np.dot(w, a)+b)</span><br><span class="line">        <span class="keyword">return</span> a</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">SGD</span><span class="params">(self, training_data, epochs, mini_batch_size, eta,</span></span></span><br><span class="line"><span class="function"><span class="params">            test_data=None)</span>:</span></span><br><span class="line">        <span class="string">"""Train the neural network using mini-batch stochastic</span></span><br><span class="line"><span class="string">        gradient descent.  The ``training_data`` is a list of tuples</span></span><br><span class="line"><span class="string">        ``(x, y)`` representing the training inputs and the desired</span></span><br><span class="line"><span class="string">        outputs.  The other non-optional parameters are</span></span><br><span class="line"><span class="string">        self-explanatory.  If ``test_data`` is provided then the</span></span><br><span class="line"><span class="string">        network will be evaluated against the test data after each</span></span><br><span class="line"><span class="string">        epoch, and partial progress printed out.  This is useful for</span></span><br><span class="line"><span class="string">        tracking progress, but slows things down substantially."""</span></span><br><span class="line">        <span class="keyword">if</span> test_data: n_test = len(test_data)</span><br><span class="line">        n = len(training_data)</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> xrange(epochs):</span><br><span class="line">            random.shuffle(training_data)</span><br><span class="line">            mini_batches = [</span><br><span class="line">                training_data[k:k+mini_batch_size]</span><br><span class="line">                <span class="keyword">for</span> k <span class="keyword">in</span> xrange(<span class="number">0</span>, n, mini_batch_size)]</span><br><span class="line">            <span class="keyword">for</span> mini_batch <span class="keyword">in</span> mini_batches:</span><br><span class="line">                self.update_mini_batch(mini_batch, eta)</span><br><span class="line">            <span class="keyword">if</span> test_data:</span><br><span class="line">                <span class="keyword">print</span> <span class="string">"Epoch &#123;0&#125;: &#123;1&#125; / &#123;2&#125;"</span>.format(</span><br><span class="line">                    j, self.evaluate(test_data), n_test)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="keyword">print</span> <span class="string">"Epoch &#123;0&#125; complete"</span>.format(j)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">update_mini_batch</span><span class="params">(self, mini_batch, eta)</span>:</span></span><br><span class="line">        <span class="string">"""Update the network's weights and biases by applying</span></span><br><span class="line"><span class="string">        gradient descent using backpropagation to a single mini batch.</span></span><br><span class="line"><span class="string">        The ``mini_batch`` is a list of tuples ``(x, y)``, and ``eta``</span></span><br><span class="line"><span class="string">        is the learning rate."""</span></span><br><span class="line">        nabla_b = [np.zeros(b.shape) <span class="keyword">for</span> b <span class="keyword">in</span> self.biases]</span><br><span class="line">        nabla_w = [np.zeros(w.shape) <span class="keyword">for</span> w <span class="keyword">in</span> self.weights]</span><br><span class="line">        <span class="keyword">for</span> x, y <span class="keyword">in</span> mini_batch:</span><br><span class="line">            delta_nabla_b, delta_nabla_w = self.backprop(x, y)</span><br><span class="line">            nabla_b = [nb+dnb <span class="keyword">for</span> nb, dnb <span class="keyword">in</span> zip(nabla_b, delta_nabla_b)]</span><br><span class="line">            nabla_w = [nw+dnw <span class="keyword">for</span> nw, dnw <span class="keyword">in</span> zip(nabla_w, delta_nabla_w)]</span><br><span class="line">        self.weights = [w-(eta/len(mini_batch))*nw</span><br><span class="line">                        <span class="keyword">for</span> w, nw <span class="keyword">in</span> zip(self.weights, nabla_w)]</span><br><span class="line">        self.biases = [b-(eta/len(mini_batch))*nb</span><br><span class="line">                       <span class="keyword">for</span> b, nb <span class="keyword">in</span> zip(self.biases, nabla_b)]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">backprop</span><span class="params">(self, x, y)</span>:</span></span><br><span class="line">        <span class="string">"""Return a tuple ``(nabla_b, nabla_w)`` representing the</span></span><br><span class="line"><span class="string">        gradient for the cost function C_x.  ``nabla_b`` and</span></span><br><span class="line"><span class="string">        ``nabla_w`` are layer-by-layer lists of numpy arrays, similar</span></span><br><span class="line"><span class="string">        to ``self.biases`` and ``self.weights``."""</span></span><br><span class="line">        nabla_b = [np.zeros(b.shape) <span class="keyword">for</span> b <span class="keyword">in</span> self.biases]</span><br><span class="line">        nabla_w = [np.zeros(w.shape) <span class="keyword">for</span> w <span class="keyword">in</span> self.weights]</span><br><span class="line">        <span class="comment"># feedforward</span></span><br><span class="line">        activation = x</span><br><span class="line">        activations = [x] <span class="comment"># list to store all the activations, layer by layer</span></span><br><span class="line">        zs = [] <span class="comment"># list to store all the z vectors, layer by layer</span></span><br><span class="line">        <span class="keyword">for</span> b, w <span class="keyword">in</span> zip(self.biases, self.weights):</span><br><span class="line">            z = np.dot(w, activation)+b</span><br><span class="line">            zs.append(z)</span><br><span class="line">            activation = sigmoid(z)</span><br><span class="line">            activations.append(activation)</span><br><span class="line">        <span class="comment"># backward pass</span></span><br><span class="line">        delta = self.cost_derivative(activations[<span class="number">-1</span>], y) * \</span><br><span class="line">            sigmoid_prime(zs[<span class="number">-1</span>])</span><br><span class="line">        nabla_b[<span class="number">-1</span>] = delta</span><br><span class="line">        nabla_w[<span class="number">-1</span>] = np.dot(delta, activations[<span class="number">-2</span>].transpose())</span><br><span class="line">        <span class="comment"># Note that the variable l in the loop below is used a little</span></span><br><span class="line">        <span class="comment"># differently to the notation in Chapter 2 of the book.  Here,</span></span><br><span class="line">        <span class="comment"># l = 1 means the last layer of neurons, l = 2 is the</span></span><br><span class="line">        <span class="comment"># second-last layer, and so on.  It's a renumbering of the</span></span><br><span class="line">        <span class="comment"># scheme in the book, used here to take advantage of the fact</span></span><br><span class="line">        <span class="comment"># that Python can use negative indices in lists.</span></span><br><span class="line">        <span class="keyword">for</span> l <span class="keyword">in</span> xrange(<span class="number">2</span>, self.num_layers):</span><br><span class="line">            z = zs[-l]</span><br><span class="line">            sp = sigmoid_prime(z)</span><br><span class="line">            delta = np.dot(self.weights[-l+<span class="number">1</span>].transpose(), delta) * sp</span><br><span class="line">            nabla_b[-l] = delta</span><br><span class="line">            nabla_w[-l] = np.dot(delta, activations[-l<span class="number">-1</span>].transpose())</span><br><span class="line">        <span class="keyword">return</span> (nabla_b, nabla_w)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">evaluate</span><span class="params">(self, test_data)</span>:</span></span><br><span class="line">        <span class="string">"""Return the number of test inputs for which the neural</span></span><br><span class="line"><span class="string">        network outputs the correct result. Note that the neural</span></span><br><span class="line"><span class="string">        network's output is assumed to be the index of whichever</span></span><br><span class="line"><span class="string">        neuron in the final layer has the highest activation."""</span></span><br><span class="line">        test_results = [(np.argmax(self.feedforward(x)), y)</span><br><span class="line">                        <span class="keyword">for</span> (x, y) <span class="keyword">in</span> test_data]</span><br><span class="line">        <span class="keyword">return</span> sum(int(x == y) <span class="keyword">for</span> (x, y) <span class="keyword">in</span> test_results)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">cost_derivative</span><span class="params">(self, output_activations, y)</span>:</span></span><br><span class="line">        <span class="string">"""Return the vector of partial derivatives \partial C_x /</span></span><br><span class="line"><span class="string">        \partial a for the output activations."""</span></span><br><span class="line">        <span class="keyword">return</span> (output_activations-y)</span><br><span class="line"></span><br><span class="line"><span class="comment">#### Miscellaneous functions</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sigmoid</span><span class="params">(z)</span>:</span></span><br><span class="line">    <span class="string">"""The sigmoid function."""</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">1.0</span>/(<span class="number">1.0</span>+np.exp(-z))</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sigmoid_prime</span><span class="params">(z)</span>:</span></span><br><span class="line">    <span class="string">"""Derivative of the sigmoid function."""</span></span><br><span class="line">    <span class="keyword">return</span> sigmoid(z)*(<span class="number">1</span>-sigmoid(z))</span><br></pre></td></tr></table></figure>
<p>那么程序如何识别手写数字呢？让我们先加载MNIST数据集。我将用<code>mnist_loader.py</code>来做这个：  </p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; import mnist_loader</span><br><span class="line">&gt;&gt;&gt; training_data, validation_data, test_data = \</span><br><span class="line">... mnist_loader.load_data_wrapper()</span><br></pre></td></tr></table></figure>
<p>在加载了MNIST数据集之后我们将会搭建一个有30个隐层神经元的网络。我们将会import之前的python程序：  </p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; import network</span><br><span class="line">&gt;&gt;&gt; net = network.Network([784, 30, 10])</span><br></pre></td></tr></table></figure>
<p>最后我们利用随机梯度下降法来对MNIST中的<code>train_data</code>进行学习，迭代轮次为30，mini-batch的大小设置为10，学习率设置为$\eta = 3.0$，  </p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; net.SGD(training_data, 30, 10, 3.0, test_data=test_data)</span><br></pre></td></tr></table></figure>
<p>当你运行这段代码的时候就会发现输出很多信息，这些输出的目的并不是为了高效的代码，相反这些输出会降低代码的时间效率，但是这些信息是为了告诉我们神经网络目前的状态。正如你所见，在仅仅一个迭代之后准确率就达到了9,129/10,000,而且正确率逐渐增长：  </p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">Epoch 0: 9129 / 10000</span><br><span class="line">Epoch 1: 9295 / 10000</span><br><span class="line">Epoch 2: 9348 / 10000</span><br><span class="line">...</span><br><span class="line">Epoch 27: 9528 / 10000</span><br><span class="line">Epoch 28: 9542 / 10000</span><br><span class="line">Epoch 29: 9534 / 10000</span><br></pre></td></tr></table></figure>
<p>最终的训练结果大概在95%~95.42%之间！这是相当令人激动的。我应该提醒你，你的代码运行结果未必和我的完全一致，因为我们的权重和偏置是随机初始化的。本章给出的结果是我运行了三次之后取的最好的一次。  </p>
<p>让我们回到实验上来，我们把隐藏层的神经元变为100，这将会花费更长的时间来执行。  </p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; net = network.Network([784, 100, 10])</span><br><span class="line">&gt;&gt;&gt; net.SGD(training_data, 30, 10, 3.0, test_data=test_data)</span><br></pre></td></tr></table></figure>
<p>可以确信的是这将会把准确率提高到96.59%。至少在这个例子中，用更多的隐藏层神经元能够帮助我们的得到更好的结果。  </p>
<p>当然，为了获得这样的准确率我已经对训练的迭代次数，mini-batch的大小和学习率$\eta$做了精心的选择。正如上面提到过的那样，这些就是我们神经网络的超参数，这和我们算法学习得到的参数(权重和偏置）有所不同。如果我们选了不好的超参数，那么我们可能得不到比较好的结果。例如，我们把学习率设置为$\eta=0.001$，  </p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; net = network.Network([784, 100, 10])</span><br><span class="line">&gt;&gt;&gt; net.SGD(training_data, 30, 10, 0.001, test_data=test_data)</span><br></pre></td></tr></table></figure>
<p>结果就不是那么理想了,  </p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">Epoch 0: 1139 / 10000</span><br><span class="line">Epoch 1: 1136 / 10000</span><br><span class="line">Epoch 2: 1135 / 10000</span><br><span class="line">...</span><br><span class="line">Epoch 27: 2101 / 10000</span><br><span class="line">Epoch 28: 2123 / 10000</span><br><span class="line">Epoch 29: 2142 / 10000</span><br></pre></td></tr></table></figure>
<p>因此，即使我们刚开始的时候选择了比较差的超参数，但是这些差的结果也能给我们一些信息来帮助我们调整神经网络的超参数。  </p>
<p>通常来讲，调试神经网络是一项挑战，尤其是当我们的初始超参数选择比较差劲的时候。例如，我们设置30个隐藏层神经元，但是我们的学习率设置为$\eta=100.0$：  </p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; net = network.Network([784, 30, 10])</span><br><span class="line">&gt;&gt;&gt; net.SGD(training_data, 30, 10, 100.0, test_data=test_data)</span><br></pre></td></tr></table></figure>
<p>这种高学习率的设置会导致以下的结果：  </p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">Epoch 0: 1009 / 10000</span><br><span class="line">Epoch 1: 1009 / 10000</span><br><span class="line">Epoch 2: 1009 / 10000</span><br><span class="line">Epoch 3: 1009 / 10000</span><br><span class="line">...</span><br><span class="line">Epoch 27: 982 / 10000</span><br><span class="line">Epoch 28: 982 / 10000</span><br><span class="line">Epoch 29: 982 / 10000</span><br></pre></td></tr></table></figure>
<p>现在想一下我们是第一次遇到这个问题。当然，我们可以从之前的实验中得知需要降低学习率。但是当我们第一次遇到这个问题时，我们没有太多的实验结果能够指导我们该如何调整。我们可能不仅仅怀疑我们的学习率，还会怀疑神经网络的方方面面。我们可能会怀疑初始的权重和偏置是不是合理，或者怀疑训练数据胡是不是不够大，或者怀疑迭代次数不够多等等。当你最初遇到这个问题时候我们可能并不能确定是哪个方面出现的问题。  </p>
<p>我们所学到的教训就是调试神经网络不是那么一件容易的事情，它可能是一门艺术。你需要学会调试以求获得好的结果。更一般地说，我们需要培养一些启发式的直觉去选择好的超参数和好的网络结构。我们将会在本书中详细地讨论这些话题。  </p>
<p>之前我跳过了MNIST数据集加载的细节。其实是非常直接的。为了完整性，下面就是加载数据集的代码。MNIST数据集的结果在注释中解释。  </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">mnist_loader</span></span><br><span class="line"><span class="string">​~~~~~~~~~~~~</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">A library to load the MNIST image data.  For details of the data</span></span><br><span class="line"><span class="string">structures that are returned, see the doc strings for ``load_data``</span></span><br><span class="line"><span class="string">and ``load_data_wrapper``.  In practice, ``load_data_wrapper`` is the</span></span><br><span class="line"><span class="string">function usually called by our neural network code.</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#### Libraries</span></span><br><span class="line"><span class="comment"># Standard library</span></span><br><span class="line"><span class="keyword">import</span> cPickle</span><br><span class="line"><span class="keyword">import</span> gzip</span><br><span class="line"></span><br><span class="line"><span class="comment"># Third-party libraries</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_data</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="string">"""Return the MNIST data as a tuple containing the training data,</span></span><br><span class="line"><span class="string">    the validation data, and the test data.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    The ``training_data`` is returned as a tuple with two entries.</span></span><br><span class="line"><span class="string">    The first entry contains the actual training images.  This is a</span></span><br><span class="line"><span class="string">    numpy ndarray with 50,000 entries.  Each entry is, in turn, a</span></span><br><span class="line"><span class="string">    numpy ndarray with 784 values, representing the 28 * 28 = 784</span></span><br><span class="line"><span class="string">    pixels in a single MNIST image.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    The second entry in the ``training_data`` tuple is a numpy ndarray</span></span><br><span class="line"><span class="string">    containing 50,000 entries.  Those entries are just the digit</span></span><br><span class="line"><span class="string">    values (0...9) for the corresponding images contained in the first</span></span><br><span class="line"><span class="string">    entry of the tuple.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    The ``validation_data`` and ``test_data`` are similar, except</span></span><br><span class="line"><span class="string">    each contains only 10,000 images.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    This is a nice data format, but for use in neural networks it's</span></span><br><span class="line"><span class="string">    helpful to modify the format of the ``training_data`` a little.</span></span><br><span class="line"><span class="string">    That's done in the wrapper function ``load_data_wrapper()``, see</span></span><br><span class="line"><span class="string">    below.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    f = gzip.open(<span class="string">'../data/mnist.pkl.gz'</span>, <span class="string">'rb'</span>)</span><br><span class="line">    training_data, validation_data, test_data = cPickle.load(f)</span><br><span class="line">    f.close()</span><br><span class="line">    <span class="keyword">return</span> (training_data, validation_data, test_data)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_data_wrapper</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="string">"""Return a tuple containing ``(training_data, validation_data,</span></span><br><span class="line"><span class="string">    test_data)``. Based on ``load_data``, but the format is more</span></span><br><span class="line"><span class="string">    convenient for use in our implementation of neural networks.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    In particular, ``training_data`` is a list containing 50,000</span></span><br><span class="line"><span class="string">    2-tuples ``(x, y)``.  ``x`` is a 784-dimensional numpy.ndarray</span></span><br><span class="line"><span class="string">    containing the input image.  ``y`` is a 10-dimensional</span></span><br><span class="line"><span class="string">    numpy.ndarray representing the unit vector corresponding to the</span></span><br><span class="line"><span class="string">    correct digit for ``x``.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    ``validation_data`` and ``test_data`` are lists containing 10,000</span></span><br><span class="line"><span class="string">    2-tuples ``(x, y)``.  In each case, ``x`` is a 784-dimensional</span></span><br><span class="line"><span class="string">    numpy.ndarry containing the input image, and ``y`` is the</span></span><br><span class="line"><span class="string">    corresponding classification, i.e., the digit values (integers)</span></span><br><span class="line"><span class="string">    corresponding to ``x``.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Obviously, this means we're using slightly different formats for</span></span><br><span class="line"><span class="string">    the training data and the validation / test data.  These formats</span></span><br><span class="line"><span class="string">    turn out to be the most convenient for use in our neural network</span></span><br><span class="line"><span class="string">    code."""</span></span><br><span class="line">    tr_d, va_d, te_d = load_data()</span><br><span class="line">    training_inputs = [np.reshape(x, (<span class="number">784</span>, <span class="number">1</span>)) <span class="keyword">for</span> x <span class="keyword">in</span> tr_d[<span class="number">0</span>]]</span><br><span class="line">    training_results = [vectorized_result(y) <span class="keyword">for</span> y <span class="keyword">in</span> tr_d[<span class="number">1</span>]]</span><br><span class="line">    training_data = zip(training_inputs, training_results)</span><br><span class="line">    validation_inputs = [np.reshape(x, (<span class="number">784</span>, <span class="number">1</span>)) <span class="keyword">for</span> x <span class="keyword">in</span> va_d[<span class="number">0</span>]]</span><br><span class="line">    validation_data = zip(validation_inputs, va_d[<span class="number">1</span>])</span><br><span class="line">    test_inputs = [np.reshape(x, (<span class="number">784</span>, <span class="number">1</span>)) <span class="keyword">for</span> x <span class="keyword">in</span> te_d[<span class="number">0</span>]]</span><br><span class="line">    test_data = zip(test_inputs, te_d[<span class="number">1</span>])</span><br><span class="line">    <span class="keyword">return</span> (training_data, validation_data, test_data)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">vectorized_result</span><span class="params">(j)</span>:</span></span><br><span class="line">    <span class="string">"""Return a 10-dimensional unit vector with a 1.0 in the jth</span></span><br><span class="line"><span class="string">    position and zeroes elsewhere.  This is used to convert a digit</span></span><br><span class="line"><span class="string">    (0...9) into a corresponding desired output from the neural</span></span><br><span class="line"><span class="string">    network."""</span></span><br><span class="line">    e = np.zeros((<span class="number">10</span>, <span class="number">1</span>))</span><br><span class="line">    e[j] = <span class="number">1.0</span></span><br><span class="line">    <span class="keyword">return</span> e</span><br></pre></td></tr></table></figure>
<p>我已经说过上面的程序能够产生比较好的结果。但是着意味着什么呢？好是和谁比较的呢？我们需要找出一个baseline来证明我们的程序能够得到比较好的结果。当然，这些baseline中最简单的就是随机猜数字。这将大概有十分之一的概率成功。我们目前达到的效果比这好多了。  </p>
<p>那么有没有更普通的baseline呢？让我们想一个极端的例子：我们将考虑一个图片有多黑。举例来说，有着数字2的图片应该会比有着1的图片更黑，这是因为2比1有着更多的黑色像素点。  </p>
<p><img src="/images/c1s6-1.png" height="10%" width="20%"></p>
<p>这意味着用训练数据计算每个数字$0, 1, 2,\ldots, 9$的平均的黑色程度。当我们遇到新的数字时计算它的黑色程度，然后猜测离它黑度值最接近的那一个就是正确的数字。这个过程很简单，也容易实现。这比随机猜要准确，结果表明在10,000个测试数据中答对了2,225个，也就是22.25%的准确率。  </p>
<p>不难发现其他的想法能达到的准确率一般在20%到25%之间。如果你更努力一点，可能达到50%的准确率。但是要想达到更高的准确率需要用机器学习的算法。让我们尝试一种广为人知的机器学习算法：支持向量机（SVM）。如果你对SVM不熟悉也不要沮丧，我们不打算深究SVM的底层细节。我们可以用一个python库：<a href="http://scikit-learn.org/stable/" target="_blank" rel="noopener">scikit-learn</a>,这里面提供了一个基于C库的SVM实现：<a href="http://www.csie.ntu.edu.tw/~cjlin/libsvm/" target="_blank" rel="noopener">LIBSVM</a>  </p>
<p>如果我们用SVM默认的设置那么最后会得到9,435/10,000的正确率。（代码在<a href="https://github.com/mnielsen/neural-networks-and-deep-learning/blob/master/src/mnist_svm.py" target="_blank" rel="noopener">这里</a>。这对于之前的简单分类方法效果好了很多。事实上，SVM的表现只比我们的神经网络差一点点。在后面的章节我们会介绍一些新的技术能够大大改善我们的神经网络。  </p>
<p>这并不是故事的结束。9435/10000是SVM默认参数所能够达到的结果。SVM有着很多可调参数。我在这里并不打算展示调参的过程，如果你感兴趣可以看<a href="http://peekaboo-vision.blogspot.ca/" target="_blank" rel="noopener">Andreas Mueller</a>的这篇<a href="http://peekaboo-vision.blogspot.de/2010/09/mnist-for-ever.html" target="_blank" rel="noopener">博客</a>。Mueller的结果证明一个精心调参的SVM能够的达到98.5%的准确率。换句话，一个精心调参的SVM能够仅仅之别错70个数据。这是个非常漂亮的结果。那么神经网络能做的更好么？  </p>
<p>事实上，能。目前，一个精心构造的神经网络性能比其他任何技术解决MNIST都要好，包括SVM。当前的技术是2013年达到的9979/10000。这是由 Li Wan, Matthew Zeiler, Sixin Zhang, Yann LeCun, and Rob Fergus共同完成的。我们将在书的稍后章节看到这些技术。这个水平已经超过人的识别水平了。而那些不能被正确识别的数字如下：  </p>
<p><img src="/images/c1s6-2.png" height="15%" width="60%"></p>
<p>我相信你肯定会承认这些数字是很难识别的！神经网络能够识别除了上述的21个数字之外的所有数字是非常了不起的一件事。通常，我们认为写一个程序解决手写数字识别需要一个复杂的算法。但神经网络其实并没有什么复杂的算法。它的困难点在于如何自动地学习。从某种意义上讲，我们的结果和那些复杂论文里面的结果的寓意都是下面的公式：  </p>
<blockquote>
<p><strong>sophisticated algorithm &lt;= simple learning algorithm + good training data</strong></p>
</blockquote>
<h2 id="迈向深度学习"><a href="#迈向深度学习" class="headerlink" title="迈向深度学习"></a>迈向深度学习</h2><p>当我们对神经网络给出的效果感到振奋时，也会感受很疑惑。权重和偏置是被自动发现的。这就意味着我们不能立即给出神经网络的工作原理。我们能不能找到一种方式来理解网络分类手写数字的原则？更进一步，如果给出这样的原则，我们能做的更好么？  </p>
<p>让我们把问题表述更为明显，假设自从神经网络主导人工智能的几十年后。我们能够理解一个智能网络的工作方式么？可能这些网络对我们来说是不透明的，我们不理解权重和偏置，因为它们是自动学习的。在早期，人工智能研究人员希望构建人工智能的工作能够帮助人们理解智能或人脑的机制。但是结果可能是我们理解它们既不是人脑机制或人工智能的工作原理。  </p>
<p>为了强调这些问题，让我们回想在这章开始所提到的人工神经元。假设我们想判断一幅图片是不是人脸：  </p>
<p><img src="/images/c1s7-1.JPG" height="30%" width="30%"><img src="/images/c1s7-2.jpg" height="30%" width="30%"><img src="/images/c1s7-3.jpg" height="30%" width="30%"></p>
<p>我们以解决手写数字识别问题的方法来解决这个问题。输入就是这些图片的像素值，输出用一个神经元来判断“是人脸”或“不是人脸”。  </p>
<p>假设我们真的如此设计，但是没有用到学习算法。相反，我们打算人工设计这个网络，选择一些合适的权重和偏置。那么我们该如何做呢？暂时先忘掉人工神经网络，我们能用的一个启发式想法就是把这个问题分解成一些子问题：图片上面的位置有眼睛么？图片中间有鼻子么？图片下面有眼睛么？等等。  </p>
<p>如果上面的问题的答案是“是”或甚至是“可能是”，那么我们就能推断这张图片有可能是人脸。相反的，如果答案是“否”，那么就不是人脸。  </p>
<p>当然这只是个粗略的启发式想法，它会有很多缺点。可能这个人是秃头，因此他们没有头发。可能我们仅仅能够看到脸的一部分，或者脸的呈现是有一定角度的，因此一些特征很难获取。这些启发式的思考也让我们想到利用神经网络解决这些子问题，然后通过把这些子问题结合起来构建一个人脸识别的神经网络。下面是可能的网络结构。注意这并不是现实中解决人脸识别的算法。它只是帮助我们建立网络工作机制的直觉。  </p>
<p><img src="/images/c1s7-4.png" height="60%" width="80%">  </p>
<p>很显然把这些子网络再分解也是合理的。假设我们考虑这样一个问题：“左上角有眼睛么？”这能够被分解为子问题：“这个地方有睫毛么？”；“这个地方有眉毛么？”；“这个地方有虹膜么？”。等等。当然这些问题在现实中应该包含位置信息。让我们把问题简化一下，“左上角有眼睛么？”能够被分解为：  </p>
<p><img src="/images/c1s7-5.png" height="60%" width="80%">  </p>
<p>这些问题同样也能被分解，一直到很多很多层。最后，我们所处理的子问题网络能够用简单的几个像素点所回答。  </p>
<p>最后把这个复杂的问题——识别人脸——分解成非常多的能够用几个像素点回答的问题。这可能需要很多层神经网络，前面的层用来回答简答的问题，而后面的层用来回答更困难的更抽象的问题。这种多层的神经网络也被称为<em>深度神经网络(deep neural networks)</em>  </p>
<p>当然我没说如何递归的分解子网络。人工设计权重和偏置显然是不实际的。相反的，我们想利用一种学习算法自动地学习权重和偏置，这样就能够训练多层神经网络了。研究人员在上个世纪80年代到90年代尝试利用<em>随机梯度下降法（stochastic gradient descent）</em>和<em>反向传播（backpropagation）</em>用来训练深度神经网络。但是很不幸，他们并没有取得很大的成功。因为这种方法的速度非常慢，在实际中并不实用。  </p>
<p>自从2006年以来，已经有大量的技术来学习深度神经网络。这些技术也利用了随机梯度下降法和反向传播，但引入了一些新的想法。这些技术让深层神经网络的学习变为可能。事实证明这些深度网络在很多问题上性能比浅层神经网络要好，因为它们能够解决抽象层次更高的问题。这有点像传统的编程语言使用模块化的设计和抽象的思想，使复杂的计算机程序的能够被创建。</p>
<h1 id="反向传播的原理"><a href="#反向传播的原理" class="headerlink" title="反向传播的原理"></a>反向传播的原理</h1><h2 id="前言-1"><a href="#前言-1" class="headerlink" title="前言"></a>前言</h2><p>在上一章节中，我们看到了神经网络能够通过梯度下降法来学习权重和偏置。但是在我们的解释中有一个缺口：我们没有讨论如何计算代价函数的梯度。这的确是一个重要的缺口！在这一章节中，我会介绍通过反向传播算法（<em>backpropagation</em>）来计算梯度。  </p>
<p>反向传播算法最初是在20世纪70年代被提出，但是它的重要性并没有认识到，直到1986年的一篇由David Rumelhart, Geoffrey Hinton和Ronald Williams三人提出的<a href="http://www.nature.com/nature/journal/v323/n6088/pdf/323533a0.pdf" target="_blank" rel="noopener">著名论文</a>，它的重要性才被别人认知。这篇文章描述了几种神经网络通过反向传播算法能够得到比其他算法好的效果，解决了之前一直不能解决的问题。今天，反向传播算法成为了神经网络的标配。  </p>
<p>这章涉及的数学可能比剩余的其它章节都要多。如果你对数学不感兴趣，那么可以跳过这章，仅仅把反向传播算法当成一个黑箱子来使用。那么为什么要花费时间研究这些细节呢？  </p>
<p>理由当然是为了更好地理解神经网络。反向传播的核心思想就是对于任意权重$w$计算关于代价函数$C$的偏导：$\partial<br>C / \partial w$（当然对于偏置$b$也是如此）。这个表达式高数我们当改变权重或者偏置时代价函数改变地有多快。这个表达式有点复杂，它也有比较漂亮的形式，每个元素都有一个自然，直观的解释。因此，反向传播算法并不仅仅是一种快速学习的算法。它其实给了我们一种直觉要如何改变权重和偏置来从整体改变神经网络的行为。这值得我们细致地研究一下。  </p>
<h2 id="基于矩阵计算神经网络输出"><a href="#基于矩阵计算神经网络输出" class="headerlink" title="基于矩阵计算神经网络输出"></a>基于矩阵计算神经网络输出</h2><p>在讨论反向传播算法之前，让我们拿一个基于矩阵的快速计算神经网络输出的算法来作为热身。我们其实在上一章节的结尾已经简单描述过这个算法，但是我只是一笔带过，所以这值得我们重温以下它的细节。特别的是，在反向传播的算法中，这还是一种舒服的符号表达。  </p>
<p>让我们开始用一种具体的方式来表示神经网络中的权重符号。我将会用$w^l_{jk}$来表示$(l-1)^{\rm th}$层的第$k^{\rm th}$个神经元到第$l^{\rm th}$层的第$j^{\rm th}$个神经元之间的权重。我们用下图来表示从第二层的第四个神经元到第三层的第二个神经元之间的权重：  </p>
<p><img src="/images/c2s1-1.png" height="50%" width="80%"></p>
<p>这些记号看起来有点麻烦，需要一点时间来消化。但是很容易就会发现这些符号其实很简单，自然。符号中比较奇怪的是关于$j$和$k$。你可能觉得用$j$表示输入，用$k$表示输出比较自然，而不是相反。我稍后会解释为什么那么表示。  </p>
<p>我们用类似的符号来表示神经网络的偏置和输出值。更具体的说，我们用$b^l_j$表示$l^{\rm th}$层第$j^{\rm th}$个神经元的偏置。用$a^l_j$表示第$l^{\rm th}$层第$j^{\rm th}$个神经元的输出。下面的图展示了符号的表示：  </p>
<p><img src="/images/c2s1-2.png" height="50%" width="50%"></p>
<p>根据上述的符号表示，我们可以把第$l^{\rm th}$层第$j^{\rm th}$个神经元的输出值表示为下面的式子：<br>
\begin{eqnarray} 
  a^{l}_j = \sigma\left( \sum_k w^{l}_{jk} a^{l-1}_k + b^l_j \right),
\tag{23}\end{eqnarray}
</p>
<p>这个式子是所有第$(l-1)^{th}$层神经元的加和。为了用矩阵的形式重写表达式，我们为每一层$l$定义权重矩阵$w^l$。权重矩阵$w^l$是的实体是所有连接到第$l^{\rm th}$层神经元的权重，那就是说，矩阵的第$j^{\rm th}$行，第$k^{\rm th}$列的表示为：$w^l_{jk}$。偏置向量$b^l$和输出向量$a^l$的表示方式都一样。  </p>
<p>最后我们需要向量化函数例如$\sigma$。我们已经看到了向量化表示的简洁性。我们向量化函数意味着将函数应用到输入向量的每一个元素。例如，我们有下面的函数：$f(x) = x^2$那么向量化之后可以表示为:  </p>

\begin{eqnarray}
  f\left(\left[ \begin{array}{c} 2 \\ 3 \end{array} \right] \right)
  = \left[ \begin{array}{c} f(2) \\ f(3) \end{array} \right]
  = \left[ \begin{array}{c} 4 \\ 9 \end{array} \right],
\tag{24}\end{eqnarray}

<p>这其实就是向量化的$f$是对输入矩阵的每个元素进行平方。带着这种想法，我们的<a href="">(23)</a>式能被重写为以下形式：  </p>

\begin{eqnarray} 
  a^{l} = \sigma(w^l a^{l-1}+b^l).
\tag{25}\end{eqnarray}

<p>这个表达式给我们一种更加全局的视角来思考一层的激活值是如何与上一层的激活值产生关联：我们仅仅将权值矩阵乘上这个激活值，然后加上偏置向量，最后在应用$\sigma$函数。这个全局的视角通常比逐个神经元的视角更加简单，更加简洁。这将帮我们远离复杂下标的苦海。这种表示方式在实战中也很有用，因为大多数的矩阵库都提供矩阵运算，向量加法和向量化的操作。  </p>
<p>当我们用（25）式计算$a^l$时，我们计算了中间量$z^l \equiv w^l a^{l-1}+b^l$。（25）式有时也写作$a^l =<br>\sigma(z^l)$。值得注意的是$z^l_j = \sum_k w^l_{jk} a^{l-1}_k+b^l_j$，这就是说$z^l_j$是第$l$层的第$j$个神经元的加权输入。  </p>
<h2 id="关于代价函数的两个假设"><a href="#关于代价函数的两个假设" class="headerlink" title="关于代价函数的两个假设"></a>关于代价函数的两个假设</h2><p>我们反向传播的目的就是计算代价函数关于权重和偏置的偏导。为了能使反向传播可用，我们需要对代价函数的形式做两个假设。在做假设之前我们先举一个代价函数的例子。我们利用上章提到过的均方差代价函数作为我们的例子：<br>
\begin{eqnarray}C = \frac{1}{2n} \sum_x \|y(x)-a^L(x)\|^2,\tag{26}\end{eqnarray} 
<br>这里$n$是总的训练样本数目；$y=y(x)$是每个训练样本$x$对应的输出;$L$代表网络的层数；$a^L=a^L(x)$是网络最后一层的输出。  </p>
<p>那么我们需要对代价函数$C$作什么样的假设才能使得反向传播好使呢？第一个关于代价函数的假设就是它能够写成每个单独训练样本的代价函数$C_x$之和的均值$C=\frac{1}{n} \sum_x C_x$.对于均方差代价函数，$ C_x=\frac{1}{2} ||y-a^L||^2 $.这个假设对我们这本书遇到的其他所有代价函数都成立。  </p>
<p>我们需要这个假设的理由是反向传播实际上是让我们对每一个训练样本计算偏导$\partial C_x/\partial w$和$\partial C_x/\partial b$。然后我们通过对训练样本求平均重新得到$\partial C/\partial w$和$\partial C/\partial b$。事实上我们一旦有了这个假设就假定我们的训练样本$x$是固定的。我们最后会把$x$放回去，但是为了简化我们先把下标去掉。  </p>
<p>第二个关于代价函数的假设是它能表示成输出的函数：  </p>
<p><img src="/images/c2s2-1.png" height="140%" width="60%"></p>
<p>举个例子，我们的均方差代价函数满足这个要求，因为它能够表示为:<br>
\begin{eqnarray} C=\frac{1}{2}\|y-a^L\|^2=\frac{1}{2}\sum_j(y_j-a^L_j)^2,\tag{27}\end{eqnarray}
</p>
<h2 id="Hadamard-product，-s-odot-t"><a href="#Hadamard-product，-s-odot-t" class="headerlink" title="Hadamard product，$s\odot t$"></a>Hadamard product，$s\odot t$</h2><p>反向传播算法是基于常见的线性代数操作。但是有一个操作并不常用。我们假设$s$和$t$是两个同维度的向量。那么我们用$s \odot t$代表对两个向量对应位置的元素逐个求积。因此$s \odot t$就是$(s \odot t)_j=s_jt_j$.举个例子，<br>
\begin{eqnarray}
\left[\begin{array}{c} 1 \\ 2 \end{array}\right] 
  \odot \left[\begin{array}{c} 3 \\ 4\end{array} \right]
= \left[ \begin{array}{c} 1 * 3 \\ 2 * 4 \end{array} \right]
= \left[ \begin{array}{c} 3 \\ 8 \end{array} \right].
\tag{28}\end{eqnarray}
<br>这种形式的乘积被称为Hadamard积。好的矩阵库会直接支持这种向量运算。</p>
<h2 id="反向传播的四个基本方程"><a href="#反向传播的四个基本方程" class="headerlink" title="反向传播的四个基本方程"></a>反向传播的四个基本方程</h2><p>反向传播就是要理解如何修改权重和偏置来该该改变代价函数。最终，这意味着我们要计算$\partial C /\partial w^l_{jk}$和$\partial C /\partial b^l_j$。但是为了计算这些我们先引入一个中间量$\delta^l_j$，我们称之为第$l$层第$j$个神经元产生的误差。反向传播给我们一种途径去计算误差，$\delta^l_j$，然后我们把$\delta^l_j$和$\partial C/\partial w^l_{jk}$，$\partial C /\partial w^l_j$关联起来。  </p>
<p>下面的图片展示了误差是如何定义的：  </p>
<p><img src="/images/c2s4-1.png" height="50%" width="80%"></p>
<p>恶魔位于第$l$层第$j$个神经元的位置。当输入到来时，它对加权的输入做了一点改动$\Delta z^l_j$，因此输出的结果不是$\delta(z^l_j)$，而是$\delta(z^l_j+\Delta z^l_j)$。这个改变会在后面层的神经网络中传播，最终会对总的误差造成的改变量为$\frac{\partial C}{\delta z^l_j} \Delta z^l_j$。  </p>
<p>现在，假设这个恶魔是一个好的恶魔，正努力帮助你减少误差，例如，它们正努力寻找$\Delta z^l_j$使得你的代价函数减小。我们现在假设$\frac{\delta C}{\delta z^l_j}$有一个较大的值（无论是正数还是负数）。那么我们能够通过取与$\frac{\delta C}{\delta z^l_j}$相反符号的$\Delta z^l_j$来大大降低代价函数。相反，当$\frac{\delta C}{\delta z^l_j}$接近0时，并不能带来多大提升。因此我们有一个启发式的直觉，可以通过$\frac{\partial C}{\partial z^l_j}$来衡量一个神经元的误差。  </p>
<p>我们定义$\delta^l_j$为<br>
\begin{eqnarray} \delta^l_j \equiv \frac {\partial C}{\delta z^l_j}. \tag{29} \end{eqnarray}
<br>我们用$\delta^l$来表示第$l$层的误差，然后将这些误差关联到$\partial C/\partial w^l_{jk}$和$\partial C/\delta b^l_j$.  </p>
<p>你可能会好奇为什么我们改变加权后的输入$z^l_j$。当然，如果我们改变输出的激活值$a^l_j$，用$\frac {\partial C}{\partial a^l_j}$可能看起来更自然。如果你那么做的话，你会发现反向传播算法会在代数形式上变得异常复杂。因此我们坚持用$\delta^l_j=\frac {\partial C}{\partial z^l_j}$作为衡量误差的标准。  </p>
<p>反向传播是基于四个基本方程式。这些方程式会给我们提供一种方式来计算误差$\delta^l$和代价函数的梯度。我将会在下面提出这四个方程。但是需要提醒一些，理解这些方程需要一些耐心和时间，期望一下子理解它们会让你比较沮丧。  </p>
<h3 id="输出层的误差，-delta-L"><a href="#输出层的误差，-delta-L" class="headerlink" title="输出层的误差，$\delta^L$"></a><strong>输出层的误差，$\delta^L$</strong></h3><p><span id="BP1"><br>
\begin{eqnarray} 
  \delta^L_j = \frac{\partial C}{\partial a^L_j} \sigma'(z^L_j).
\tag{BP1}\end{eqnarray}
<br></span><br>这是个非常自然的方程。右边的第一项，$\partial C/\partial a^L_j$衡量了当输入层的第$j^{th}$个神经元的激活值个改变时代价函数改变的速度。例如，当$C$并不依赖于具体的某个输出神经元$j$，那么$\delta^L_j$将会比较小，这正是我们期望的结果。第二项$\sigma'(z^L-j)$衡量了激活函数$\sigma$在$z^L_j$改变的速度。  </p>
<p>注意到(<strong>BP1</strong>)是非常容易计算的。当我们在神经网络的前馈过程中会计算$z^L_j$的值，那我们只需要一点额外的开销就能够计算出$\sigma’(z^L_j)$。而$\partial C / \partial a^L_j$的具体形式取决于代价函数的形式，当代价函数确定时该项也比较容易计算。例如，如果我们用均方差代价函数$C=\frac{1}{2}\Sigma_j(y_j-a_j)^2 $，那么$\partial C/\partial a^L_j = (a_j-y_j)$,这显然很容易计算。  </p>
<p>方程(<strong>BP1</strong>)是对$\delta^L$的逐个元素进行计算的。这是一个极好的表达式，但并不是我们在反向传播算法中想要的矩阵形式。当然，也很容易把这个式子写成,</p>
<p>\begin{eqnarray}<br>  \delta^L = \nabla_a C \odot \sigma’(z^L).<br>\tag{BP1a}\end{eqnarray}</p>
<p>这里，$\nabla_a C$被定义为偏导$\partial C /\partial a^L_j$组成的向量。你可以认为$\nabla_a C$是$C$对每个输出层激活值的改变率。很容易可以看出(<strong>BP1a</strong>)和(<strong>BP1</strong>)是等价的，因此我们将用(<strong>BP1</strong>)来代表这两种形式。举个例子，如果我们的代价函数为均方差代价函数，那么我们得到$\nabla_a C = (a^L-y)$，我们的矩阵形式的(<strong>BP1</strong>)变为</p>
<p>\begin{eqnarray}<br>  \delta^L = (a^L-y) \odot \sigma’(z^L).<br>\tag{30}\end{eqnarray}</p>
<p>正如你所见，这个表达式里面的每个元素都完美的表示成矩阵形式，这很容易用一些例如Numpy之类的库来运算。 </p>
<h3 id="误差-delta-l-和误差-delta-1-1-的方程"><a href="#误差-delta-l-和误差-delta-1-1-的方程" class="headerlink" title="误差$\delta^l$和误差$\delta^{1+1}$的方程"></a><strong>误差$\delta^l$和误差$\delta^{1+1}$的方程</strong></h3><p><span id="BP2" \=""><br>\begin{eqnarray}<br>  \delta^l = ((w^{l+1})^T \delta^{l+1}) \odot \sigma’(z^l),<br>\tag{BP2}\end{eqnarray}</span></p>
<p>这里$(w^{l+1})^T$是第$(l+1)^{th}$层权重$w^{l+1}$的转置。这个方程看起来很复杂，但是每个元素都有一个很好的解释。假设我们知道了第$(l+1)^{th}$层的误差$\delta^{l+1}$。当我们乘上转置矩阵$(w^{l+1})^T$时，我们能够直观的认为误差沿着网络向后传播，这给我们了一种计算第$l$层误差的方法。最后我们乘上Hadamard积$\odot \sigma’(z^l)$。  </p>
<p>通过结合(<strong>BP1</strong>)和(<strong>BP2</strong>)，我们能够计算任意层网络的误差$\delta^l$。我们首先利用(<strong>BP1</strong>)来计算$\delta^L$，然后利用(<strong>BP2</strong>)来计算$\delta^{L-1}$，然后再用(<strong>BP2</strong>)得到$\delta^{L-2}$,等等。  </p>
<h3 id="代价函数关于偏置的变化率"><a href="#代价函数关于偏置的变化率" class="headerlink" title="代价函数关于偏置的变化率"></a>代价函数关于偏置的变化率</h3><p><span id="BP3" \=""><br>\begin{eqnarray}  \frac{\partial C}{\partial b^l_j} =<br>  \delta^l_j.<br>\tag{BP3}\end{eqnarray}</span></p>
<p>这就是说，误差$\delta^l_j$恰好等于变化率$\partial C /\partial b^l_j$。这是一个重要的消息，因为(<strong>BP1</strong>)和(<strong>BP2</strong>)已经告诉了我们如何计算$\delta^l_j$。我们能够重写(<strong>BP3</strong>)成如下的形式：</p>
<p>\begin{eqnarray}<br>  \frac{\partial C}{\partial b} = \delta,<br>\tag{31}\end{eqnarray}</p>
<p>这里可以理解为$\delta$和$b$在同一个神经元被计算。</p>
<h3 id="代价函数关于权重的变化率"><a href="#代价函数关于权重的变化率" class="headerlink" title="代价函数关于权重的变化率"></a>代价函数关于权重的变化率</h3><p><span id="BP4" \=""><br>\begin{eqnarray}<br>  \frac{\partial C}{\partial w^l_{jk}} = a^{l-1}_k \delta^l_j.<br>\tag{BP4}\end{eqnarray}</span></p>
<p>这告诉我们偏导$\partial C /\partial w^l_{jk}$的计算与$\delta^l$和$a^{l-1}$有关，而这两项我们早就知道如何计算了。这个等式能够重写成向量形式<br>
\begin{eqnarray}
\frac{\partial C}{\partial w} = a_{\rm in} \delta_{\rm out},
\tag{32}\end{eqnarray}
<br>我们可以把这个描述为下图：</p>
<p><img src="/images/c2s5-1.png" \=""></p>
<p>(<strong>32</strong>)式的一个比较好的结论就是当激活值$a_{in}$很小时，$a_{\rm in} \approx 0$，梯度项$\partial C/\partial w$也将变得比较小。在这种情况下我们会说权重学习的很慢，即在梯度下降的过程中它并没有改变多少。  </p>
<p>我们能够从(<strong>BP1</strong>)-(<strong>BP4</strong>)中得到其他的一些直觉。让我们先从输出层开始。考虑(<strong>BP1</strong>)中的$\sigma’(z^L_j)$。回顾上章中的<a href="#sigmoid">sigmoid函数</a>，当$\sigma(z^L_j)$接近$0$或$1$时$\sigma$函数会变得非常平坦。这会导致$\sigma’(z^L_j) \approx 0$.因此当最后一层的激活值接近$0$或接近$1$时，我们的权重将会学习的非常缓慢，这就是我们熟称的<strong>饱和</strong>。当然同样的情况也会发生在偏置上。  </p>
<p>在其它层的神经元上也有相同的结论。我们看(<strong>BP2</strong>)中的$\sigma’(z^l)$。这意味着当神经元接近饱和时$\delta^l_j$也会变得很小。这会导致任何层的饱和神经元都会使权重学习速度变慢。  </p>
<p>上面的这些结论并没有给我们带来很大的惊喜。但是他们能够帮助我们理解神经网络在学习的过程中都发生了什么。这四个方程并不依赖于任何具体形式的激活函数（我们稍后就会看到这四个方程的证明过程并不依赖于激活函数的形式）。我们也能够利用这些方程设计我们的激活函数。举个例子，假设我们选择一个激活函数$\sigma$使得$\sigma’$始终是正的，并且永远不会接近$0$。这将会避免因神经元饱和导致学习速率下降的情况。稍后我们会看到对激活函数的修改。牢记这四个方程将会帮助我们理解为什么要修改以及他们会产生什么样的影响。  </p>
<p><img src="/images/c2s5-2.png" \=""></p>
<h2 id="四个方程的证明"><a href="#四个方程的证明" class="headerlink" title="四个方程的证明"></a>四个方程的证明</h2><p>接下来我们会证明这4个方程。所有的这四个方程的推导都利用求导中的链式法则。如果你对链式法则很熟悉，我建议你在看本节之前自己独立推导这四个方程。</p>
<h3 id="BP1的证明"><a href="#BP1的证明" class="headerlink" title="BP1的证明"></a>BP1的证明</h3><p>让我们先从(<strong>BP1</strong>)开始，这个表达式给出了输出层误差$\delta^L$。为了证明这个等式，我们先回顾一下误差的定义：<br>\begin{eqnarray}<br>  \delta^L_j = \frac{\partial C}{\partial z^L_j}.<br>\tag{36}\end{eqnarray}<br>通过利用链式法则我们能够得到如下的结果，<br>\begin{eqnarray}<br>  \delta^L_j = \sum_k \frac{\partial C}{\partial a^L_k} \frac{\partial a^L_k}{\partial z^L_j},<br>\tag{37}\end{eqnarray}<br>这里的和是所有输出层神经元$k$相加。当然输出层第$k$个神经元的激活值$a^L_k$仅仅当$k=j$时才依赖于出入权重$z^L_J$。因此当$k \neq j$时$\partial a^L_k / \partial z^L_j$将会消失。这样我们就能够把这个等式简化成如下的形式：<br>\begin{eqnarray}<br>  \delta^L_j = \frac{\partial C}{\partial a^L_j} \frac{\partial a^L_j}{\partial z^L_j}.<br>\tag{38}\end{eqnarray}<br>由于$a^L_j = \sigma(z^L_j)$，因此右边的项可以写成<br>\begin{eqnarray}<br>  \delta^L_j = \frac{\partial C}{\partial a^L_j} \sigma’(z^L_j),<br>\tag{39}\end{eqnarray}<br>这就是(<a href="#BP1">BP1</a>)的非向量形式。</p>
<h3 id="BP2的证明"><a href="#BP2的证明" class="headerlink" title="BP2的证明"></a>BP2的证明</h3><p>接来我们证明(<a href="#BP2">BP2</a>),它是用下一层的误差$\delta^{l+1}$来表示当前层的误差$\delta^l$。为了达到这个目的我们要把$\delta^l_j = \partial C / \partial<br>z^l_j$重写成项为$\delta^{l+1}_k = \partial C / \partial z^{l+1}_k$的形式。我们可以利用链式法则，<br>
\begin{eqnarray}
  \delta^l_j & = & \frac{\partial C}{\partial z^l_j} \tag{40}\\
  & = & \sum_k \frac{\partial C}{\partial z^{l+1}_k} \frac{\partial z^{l+1}_k}{\partial z^l_j} \tag{41}\\ 
  & = & \sum_k \frac{\partial z^{l+1}_k}{\partial z^l_j} \delta^{l+1}_k,
\tag{42}\end{eqnarray}
<br>在最后一行我们交换了左右两式，并且用$\delta^{l+1}_k$代替了它的定义$\frac {\partial z^{l+1}_k}{\partial z^l_j}$。为了计算第一项，我们首先看一下展开形式：<br>
\begin{eqnarray}
  z^{l+1}_k = \sum_j w^{l+1}_{kj} a^l_j +b^{l+1}_k = \sum_j w^{l+1}_{kj} \sigma(z^l_j) +b^{l+1}_k.
\tag{43}\end{eqnarray}
<br>微分之后我们得到<br>
\begin{eqnarray}
  \frac{\partial z^{l+1}_k}{\partial z^l_j} = w^{l+1}_{kj} \sigma'(z^l_j).
\tag{44}\end{eqnarray}
<br>替换回<strong>(42)</strong>式我们有<br>
\begin{eqnarray}
  \delta^l_j = \sum_k w^{l+1}_{kj}  \delta^{l+1}_k \sigma'(z^l_j).
\tag{45}\end{eqnarray}
<br>这就是<strong>(BP2)</strong>的非向量形式。</p>
<h3 id="BP3的证明"><a href="#BP3的证明" class="headerlink" title="BP3的证明"></a>BP3的证明</h3><p>通过上面的两个方程我们可以计算出每一层的误差，那么下面我们就需要利用这个计算误差函数对权重和偏置的偏导用以梯度下降。首先我们先证明误差对偏置的偏导，通过链式法则我们有<br>
\begin{eqnarray}
\nonumber\frac {\partial C}{\partial b^l_j} & = & \frac {\partial C}{\partial x^l_j} \frac {\partial z^l_j}{\partial b^l_j} \\
& = & \delta^l_j
\nonumber\end{eqnarray}
</p>
<h3 id="BP4的证明"><a href="#BP4的证明" class="headerlink" title="BP4的证明"></a>BP4的证明</h3><p>接下来我们证明代价函数对权重的偏导。<br>根据链式法则有<br>
\begin{eqnarray}
\nonumber\frac {\partial C}{\partial w^l_{jk}} & = & \frac {\partial C}{\partial z^l_j} \frac {\partial z^l_j}{\partial w^l_{jk}} \\
& = & \frac {\partial z^l_j}{\partial w^l_{jk}}{\delta^l_j}
\nonumber\end{eqnarray}
<br>我们展开$z^l_j$有<br>
\begin{eqnarray}
z^l_j = \sum_m w^l_{jm}a^{l-1}_m + b^l_j
\nonumber\end{eqnarray}
<br>只有当$m = k$时$z^l_j$才和$w^l_{jk}$有关联，因此求偏导我们可以得到<br>
\begin{eqnarray}
\frac {\partial C}{\partial w^l_{jk}} = a^{l-1}_k \delta^l_j
\nonumber\end{eqnarray}
</p>
<h2 id="反向传播算法"><a href="#反向传播算法" class="headerlink" title="反向传播算法"></a>反向传播算法</h2><p>反向传播的方程给我们提供了一种计算代价函数梯度的方法。让我们精确地描述一下该算法：</p>
<ol>
<li><strong>Input $x$:</strong>Set the corresponding activation $a^1$ for the input layer.</li>
<li><strong>Feedforward:</strong> For each $l=2,3,\ldots,L$ compute $z^{l} = w^l a^{l-1}+b^l$ and $a^{l} = \sigma(z^{l})$.</li>
<li><strong>Output error $\delta^L$:</strong> Compute the vector $\delta^{L} = \nabla_a C \odot \sigma’(z^L)$.</li>
<li><strong>Backpropagate the error:</strong>For each $l = L-1, L-2,\ldots, 2$ compute $\delta^{l} = ((w^{l+1})^T \delta^{l+1}) \odot \sigma’(z^{l})$.</li>
<li><strong>Output:</strong> The gradient of the cost function is given by $\frac{\partial C}{\partial w^l_{jk}} = a^{l-1}_k \delta^l_j$ and $\frac{\partial C}{\partial b^l_j} = \delta^l_j$  </li>
</ol>
<p>通过这个算法你就看出为什么它被称之为反向传播算法(<em>backpropagation</em>)。我们从最后一层开始向后迭代计算误差向量$\delta^l$。为了理解代价函数是如何随着前面的权重和误差改变的，我们需要反复利用利用链式法则来得到我们需要的表达式。</p>
<p>正如我所描述的那样，反向传播算法计算了单个训练样本的代价函数的梯度$C=C_x$。实际上，我们通常结合一种学习算法例如梯度下降法来计算多个训练样本的梯度。给定一个<em>mini-batch</em>大小为$m$的训练样本，我们应用基于<em>mini-batch</em>的梯度下降法：</p>
<ol>
<li><strong>Input a set of training examples</strong></li>
<li><strong>For each training example$x$:</strong> Set the corresponding input activation $a^{x,1}$,and perform the following steps:</li>
</ol>
<ul>
<li><strong>Feedforward:</strong>For each $l=2,3,\ldots,L$ compute $z^{x,l} = w^l a^{x,l-1}+b^l$ and $a^{x,l} = \sigma(z^{x,l})$.</li>
<li><strong>Output error $\delta^{x,L}$:</strong> Compute the vector $\delta^{x,L} = \nabla_a C_x \odot \sigma’(z^{x,L})$.</li>
<li><strong>Backpropagate the error:</strong>For each $l = L-1, L-2,\ldots, 2$ compute $\delta^{x,l} = ((w^{l+1})^T \delta^{x,l+1})\odot \sigma’(z^{x,l})$.</li>
</ul>
<ol>
<li><strong>Gradient descent:</strong>For each $l = L, L-1, \ldots, 2$ update the weights according to the rule $w^l \rightarrow w^l-\frac{\eta}{m} \sum_x \delta^{x,l} (a^{x,l-1})^T$ ,and the biases according to the rule $b^l \rightarrow l-\frac{\eta}{m}<br>\sum_x \delta^{x,l}$.  </li>
</ol>
<p>当然，在实际应用中，你需要循环的划分<em>mini-batch</em>大小的训练样本，我为了简洁忽略了这些东西。</p>
<h2 id="反向传播的代码"><a href="#反向传播的代码" class="headerlink" title="反向传播的代码"></a>反向传播的代码</h2><p>在抽象地理解了反向传播算法之后，我们现在能够理解上一章应用反向传播的<a href="https://github.com/mnielsen/neural-networks-and-deep-learning" target="_blank" rel="noopener">代码</a>了。这些代码是上面算法的直接翻译。特别的，<code>update_mini_batch</code>方法更新了一个<em>mini-batch</em>大小训练样本的权重和偏置：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Network</span><span class="params">(object)</span>:</span></span><br><span class="line">...</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">update_mini_batch</span><span class="params">(self, mini_batch, eta)</span>:</span></span><br><span class="line">        <span class="string">"""Update the network's weights and biases by applying</span></span><br><span class="line"><span class="string">        gradient descent using backpropagation to a single mini batch.</span></span><br><span class="line"><span class="string">        The "mini_batch" is a list of tuples "(x, y)", and "eta"</span></span><br><span class="line"><span class="string">        is the learning rate."""</span></span><br><span class="line">        nabla_b = [np.zeros(b.shape) <span class="keyword">for</span> b <span class="keyword">in</span> self.biases]</span><br><span class="line">        nabla_w = [np.zeros(w.shape) <span class="keyword">for</span> w <span class="keyword">in</span> self.weights]</span><br><span class="line">        <span class="keyword">for</span> x, y <span class="keyword">in</span> mini_batch:</span><br><span class="line">            delta_nabla_b, delta_nabla_w = self.backprop(x, y)</span><br><span class="line">            nabla_b = [nb+dnb <span class="keyword">for</span> nb, dnb <span class="keyword">in</span> zip(nabla_b, delta_nabla_b)]</span><br><span class="line">            nabla_w = [nw+dnw <span class="keyword">for</span> nw, dnw <span class="keyword">in</span> zip(nabla_w, delta_nabla_w)]</span><br><span class="line">        self.weights = [w-(eta/len(mini_batch))*nw </span><br><span class="line">                        <span class="keyword">for</span> w, nw <span class="keyword">in</span> zip(self.weights, nabla_w)]</span><br><span class="line">        self.biases = [b-(eta/len(mini_batch))*nb </span><br><span class="line">                       <span class="keyword">for</span> b, nb <span class="keyword">in</span> zip(self.biases, nabla_b)]</span><br></pre></td></tr></table></figure>
<p>大部分的工作是由<code>delta_nabla_b, delta_nabla_w = self.backprop(x, y)</code>这一行代码完成的，这行代码利用<code>backprop</code>方法计算了偏导$\partial C_x / \partial b^l_j$和偏导$\partial C_x / \partial w^l_{jk}$。<code>backprop</code>的实现如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Network</span><span class="params">(object)</span>:</span></span><br><span class="line">...</span><br><span class="line">   <span class="function"><span class="keyword">def</span> <span class="title">backprop</span><span class="params">(self, x, y)</span>:</span></span><br><span class="line">        <span class="string">"""Return a tuple "(nabla_b, nabla_w)" representing the</span></span><br><span class="line"><span class="string">        gradient for the cost function C_x.  "nabla_b" and</span></span><br><span class="line"><span class="string">        "nabla_w" are layer-by-layer lists of numpy arrays, similar</span></span><br><span class="line"><span class="string">        to "self.biases" and "self.weights"."""</span></span><br><span class="line">        nabla_b = [np.zeros(b.shape) <span class="keyword">for</span> b <span class="keyword">in</span> self.biases]</span><br><span class="line">        nabla_w = [np.zeros(w.shape) <span class="keyword">for</span> w <span class="keyword">in</span> self.weights]</span><br><span class="line">        <span class="comment"># feedforward</span></span><br><span class="line">        activation = x</span><br><span class="line">        activations = [x] <span class="comment"># list to store all the activations, layer by layer</span></span><br><span class="line">        zs = [] <span class="comment"># list to store all the z vectors, layer by layer</span></span><br><span class="line">        <span class="keyword">for</span> b, w <span class="keyword">in</span> zip(self.biases, self.weights):</span><br><span class="line">            z = np.dot(w, activation)+b</span><br><span class="line">            zs.append(z)</span><br><span class="line">            activation = sigmoid(z)</span><br><span class="line">            activations.append(activation)</span><br><span class="line">        <span class="comment"># backward pass</span></span><br><span class="line">        delta = self.cost_derivative(activations[<span class="number">-1</span>], y) * \</span><br><span class="line">            sigmoid_prime(zs[<span class="number">-1</span>])</span><br><span class="line">        nabla_b[<span class="number">-1</span>] = delta</span><br><span class="line">        nabla_w[<span class="number">-1</span>] = np.dot(delta, activations[<span class="number">-2</span>].transpose())</span><br><span class="line">        <span class="comment"># Note that the variable l in the loop below is used a little</span></span><br><span class="line">        <span class="comment"># differently to the notation in Chapter 2 of the book.  Here,</span></span><br><span class="line">        <span class="comment"># l = 1 means the last layer of neurons, l = 2 is the</span></span><br><span class="line">        <span class="comment"># second-last layer, and so on.  It's a renumbering of the</span></span><br><span class="line">        <span class="comment"># scheme in the book, used here to take advantage of the fact</span></span><br><span class="line">        <span class="comment"># that Python can use negative indices in lists.</span></span><br><span class="line">        <span class="keyword">for</span> l <span class="keyword">in</span> xrange(<span class="number">2</span>, self.num_layers):</span><br><span class="line">            z = zs[-l]</span><br><span class="line">            sp = sigmoid_prime(z)</span><br><span class="line">            delta = np.dot(self.weights[-l+<span class="number">1</span>].transpose(), delta) * sp</span><br><span class="line">            nabla_b[-l] = delta</span><br><span class="line">            nabla_w[-l] = np.dot(delta, activations[-l<span class="number">-1</span>].transpose())</span><br><span class="line">        <span class="keyword">return</span> (nabla_b, nabla_w)</span><br><span class="line"></span><br><span class="line">...</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">cost_derivative</span><span class="params">(self, output_activations, y)</span>:</span></span><br><span class="line">        <span class="string">"""Return the vector of partial derivatives \partial C_x /</span></span><br><span class="line"><span class="string">        \partial a for the output activations."""</span></span><br><span class="line">        <span class="keyword">return</span> (output_activations-y) </span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sigmoid</span><span class="params">(z)</span>:</span></span><br><span class="line">    <span class="string">"""The sigmoid function."""</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">1.0</span>/(<span class="number">1.0</span>+np.exp(-z))</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sigmoid_prime</span><span class="params">(z)</span>:</span></span><br><span class="line">    <span class="string">"""Derivative of the sigmoid function."""</span></span><br><span class="line">    <span class="keyword">return</span> sigmoid(z)*(<span class="number">1</span>-sigmoid(z))</span><br></pre></td></tr></table></figure>
<h2 id="为何说反向传播是一种快速的算法？"><a href="#为何说反向传播是一种快速的算法？" class="headerlink" title="为何说反向传播是一种快速的算法？"></a>为何说反向传播是一种快速的算法？</h2><p>为什么说反向传播是一种快速的算法呢？为了回答这个问题，我们来考虑一下另一种计算梯度的方法。想象一下这是神经网络研究的早期，大概是20世纪50年代或60年代，你是世界上第一个想出用梯度下降法学习神经网络的人！但是为了让这个想法行得通，你需要一种方法来计算代价函数的梯度。你回顾了一下你的算术的知识，决定利用链式法则来计算梯度。当你尝试了之后你会发现代数式会变得极其复杂，这会让你很沮丧。因此你努力寻找另一种方法。你决定单独考虑仅仅以$w$作为自变量时$C=C(w)$(之后我们再考虑仅仅以偏置为自变量)。首先你给权重编号$w_1, w_2, \ldots$，然后对每个权重$w_j$计算$\partial C/\partial w_j$。我们可以利用下面的近似来计算偏导</p>
<p>\begin{eqnarray}  \frac{\partial<br>    C}{\partial w_{j}} \approx \frac{C(w+\epsilon<br>    e_j)-C(w)}{\epsilon},<br>\tag{46}\end{eqnarray}</p>
<p>这里$\epsilon &gt; 0$是个很小的正数，$e_j$是在$j^{th}$方向上的单位向量。换句话说，我们可以通过计算两个不同的$w_j$来计算$C$的偏导。同样的想法也能用于计算代价函数关于偏置的偏导。  </p>
<p>这个方法看起来很诱人。它在概念上比较简单，而且很容易实现，只用几行代码就能完成。它看起来比利用链式法则计算梯度更有前景。</p>
<p>不幸的是，当你实现它之后你会发现速度极其慢。为了理解为什么，我们假设网络中有一百万个权重。对于每个不同的权重$w_j$我们都需要计算$C(w+\epsilon e_j)$来获得$\partial C / \partial w_j$。这意味着我们对于每个训练样本都要一百万次计算。</p>
<p>反向传播比较聪明的一点就是能够让我们在一次前馈和一次后馈就能一次性地计算所有的偏导$\partial C/ \partial w_j$。虽然反向传播看起来公式更复杂一些，但是它的确快了非常多！</p>
<p>这种加速在1986年被首次全面认可，它极大地扩大了神经网络所能解决的问题。这也导致了很多人都开始使用神经网络。当然，反向传播算法也不是灵丹妙药。即使在1980年的末期，人们也发现了它在解决深层网络时会出现问题。后面我们会看到现代计算机和一些聪明的想法是如何让反向传播算法训练深层网络成为可能。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2016/03/02/如何禁止C-默认成员函数/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Huaipeng Zhao">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Huaipeng Zhao">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2016/03/02/如何禁止C-默认成员函数/" itemprop="url">如何禁止C++默认成员函数</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2016-03-02T00:00:00+08:00">
                2016-03-02
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/C/" itemprop="url" rel="index">
                    <span itemprop="name">C++</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>前几天在一次笔试过程中被问到C++如何设计禁止调用默认构造函数，当时简单的想法是直接将默认构造函数声明为private即可，这样的话对象的确不能直接调用。之后查阅了《Effective C++》之后得到了比较详尽的解释。  </p>
<h1 id="了解C-的默认行为"><a href="#了解C-的默认行为" class="headerlink" title="了解C++的默认行为"></a>了解C++的默认行为</h1><p>当我们创建空类时，C++默认给我们生成了四种成员函数：  </p>
<ol>
<li>构造函数</li>
<li>析构函数</li>
<li>拷贝构造函数(<em>copy</em>)</li>
<li>重载=的拷贝函数(<em>copy assignment</em>)  </li>
</ol>
<p>因此，当你写下如下的代码：  </p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Empty</span>&#123;</span>&#125;;</span><br></pre></td></tr></table></figure>
<p>那么编译器会自动生成：  </p>
<pre><code class="C++"><span class="class"><span class="keyword">class</span> <span class="title">Empty</span>{</span>
<span class="keyword">public</span>:
    Empty(){...}                              <span class="comment">//default构造函数</span>
    Empty(<span class="keyword">const</span> Empty&amp; rhs){...}              <span class="comment">//copy构造函数</span>
    ~Empty(){...}                             <span class="comment">//析构函数</span>
    Empty&amp; <span class="keyword">operator</span>=(<span class="keyword">const</span> Empty&amp; rhs){...}   <span class="comment">//copy assignment操作符</span>
};

</code></pre>
<p>至于copy构造函数和copy assignment操作符是不是有效取决于类的成员变量，例如：如果类成员有const变量或者引用，那么是不能重新赋值的。  </p>
<h1 id="拒绝使用编译器自动生成函数"><a href="#拒绝使用编译器自动生成函数" class="headerlink" title="拒绝使用编译器自动生成函数"></a>拒绝使用编译器自动生成函数</h1><p>书中提到了一个实际的应用场景。在房子销售时，每一套房子都是独一无二的（地理位置，装修等等），那么显然我们不想让别人使用拷贝构造函数或者copy assignment操作符。但是如果我们不写，那么编译器会自动生成。如果我们写了就会可能让别人利用。那么该怎么办呢？  </p>
<ul>
<li>首先我们最自然的想法就是把这两个函数声明为私有，这样别人调用的时候可能会报错。我们的直觉是正确的。确实这样做可以行得通。于是我们如此写到：  </li>
</ul>
<pre><code class="C++"><span class="class"><span class="keyword">class</span> <span class="title">HomeForSale</span>{</span>
<span class="keyword">public</span>:
    ...
<span class="keyword">private</span>:
    HomeForSale(<span class="keyword">const</span> HomeForSale&amp; hfs){...}
    HomeForSale&amp; <span class="keyword">operator</span>=(<span class="keyword">const</span> HomeForSale&amp; hfs){...}
};
</code></pre>
<ul>
<li>当然那么做并不是万事大吉了。因为member函数和友元函数仍然能调用私有成员函数。那么你可能又想到答案：我们无需定义成员函数，只需声明即可：  </li>
</ul>
<pre><code class="C++"><span class="class"><span class="keyword">class</span> <span class="title">HomeForSale</span>{</span>
<span class="keyword">public</span>:
    ...
<span class="keyword">private</span>:
    HomeForSale(<span class="keyword">const</span> HomeForSale&amp;);
    HomeForSale&amp; <span class="keyword">operator</span>=(<span class="keyword">const</span> HomeForSale&amp;);
};
</code></pre>
<p>那么member成员函数和友元函数调用时，在编译阶段没问题，在链接阶段会报错。那么还有没有更好的方案能够让代码在编译阶段就能检测出错误呢？ 答案是肯定的。  </p>
<ul>
<li>我们为此建立一个基类：  </li>
</ul>
<pre><code class="C++"><span class="class"><span class="keyword">class</span> <span class="title">Uncopyable</span>{</span>
<span class="keyword">protected</span>:
    Uncopyable(){}                            <span class="comment">//允许derived对象的构造和解析</span>
    ~Uncopyable(){}
<span class="keyword">private</span>:
    Uncopyable(<span class="keyword">const</span> Uncopyable&amp;);            <span class="comment">//但阻止copying</span>
    Uncopyable&amp; <span class="keyword">operator</span>=(<span class="keyword">const</span> Uncopyable&amp;);
};
</code></pre>
<p>那么，为了阻止HomeForSale被拷贝，我们只需继承Uncopyable：  </p>
<pre><code class="C++"><span class="class"><span class="keyword">class</span> <span class="title">HomeForSale</span>:</span><span class="keyword">private</span> Uncopyable{
    ...
};
</code></pre>
<ul>
<li>C++11提出更为简洁的解决方案：  </li>
</ul>
<pre><code class="C++"><span class="class"><span class="keyword">class</span> <span class="title">HomeForSale</span>{</span>
<span class="keyword">public</span>:
    HomeForSale(<span class="keyword">const</span> HomeForSale&amp;) = <span class="keyword">delete</span>;
    HomeForSale&amp; <span class="keyword">operator</span>=(<span class="keyword">const</span> HomeForSale&amp;) = <span class="keyword">delete</span>;
};
</code></pre>
<h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><ul>
<li><p>为驳回编译器自动生成的成员函数，可将相应成员函数声明为private并且不予实现。或者使用像Uncopyable这样的基类。或者使用C++11的新特性。</p>
</li>
<li><p>在boost也有这样的基类:noncopyable。</p>
</li>
</ul>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
  </section>

  


          </div>
          


          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      

      <section class="site-overview-wrap sidebar-panel sidebar-panel-active">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image"
                src="/images/avatar.jpg"
                alt="Huaipeng Zhao" />
            
              <p class="site-author-name" itemprop="name">Huaipeng Zhao</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">10</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                
                  <span class="site-state-item-count">7</span>
                  <span class="site-state-item-name">分类</span>
                
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">9</span>
                  <span class="site-state-item-name">标签</span>
                </a>
              </div>
            

          </nav>

          
            <div class="feed-link motion-element">
              <a href="/atom.xml" rel="alternate">
                <i class="fa fa-rss"></i>
                RSS
              </a>
            </div>
          

          

          
          

          
          

          

        </div>
      </section>

      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2018</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Huaipeng Zhao</span>

  
</div>


  <div class="powered-by">由 <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a> 强力驱动</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">主题 &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Muse</a> v5.1.4</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  


  











  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  

  
  
    <script type="text/javascript" src="/lib/canvas-nest/canvas-nest.min.js"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  

  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  





  

  

  

  
  

  

  

  

</body>
</html>
